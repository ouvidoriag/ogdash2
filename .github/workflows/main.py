import os
import pandas as pd
import unicodedata
import re
import requests
import json
import gspread
import numpy as np
import base64 
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import warnings
from urllib.parse import quote
import hashlib
from gspread_dataframe import set_with_dataframe
import logging
from typing import List, Dict

# ----------------------------
# Logging (arquivo + console)
# ----------------------------
logging.basicConfig(
    level=logging.INFO, # Pode mudar para logging.DEBUG para mais detalhes durante a depura√ß√£o.
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler("pipeline_tratamento.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# --------------------------------------------------------
# Utilit√°rio de banner de se√ß√£o (para logs/prints)
# --------------------------------------------------------
def _BANNER(titulo):
    print("\n" + "="*18 + f" {titulo} " + "="*18)
    logging.info(titulo)

def _SUB(titulo):
    print("‚Äî " + titulo)
    logging.info(titulo)

# ========================================================
# 1) CONFIGURA√á√ÉO GOOGLE DRIVE / SHEETS (REMOVIDA LIMPEZA AGRESSIVA)
# ========================================================

_BANNER("1) CONFIGURA√á√ÉO GOOGLE DRIVE/SHEETS")

# ----------------------------
# AUTENTICA√á√ÉO √öNICA (usar apenas uma vez)
# ----------------------------
CAMINHO_CREDENCIAIS = ".github/workflows/credentials.json"
SCOPES = [
    "https://www.googleapis.com/auth/drive",
    "https://www.googleapis.com/auth/spreadsheets"
]

try:
    logging.info(f"Tentando ler string Base64 do arquivo: '{CAMINHO_CREDENCIAIS}'")
    with open(CAMINHO_CREDENCIAIS, "r", encoding="utf-8") as file:
        encoded_json_string = file.read().strip() # L√™ e remove espa√ßos/newlines

    # Decodifica de Base64 para bytes, depois para string UTF-8
    decoded_json_bytes = base64.b64decode(encoded_json_string)
    decoded_json_str = decoded_json_bytes.decode('utf-8')
    
    # --- CORRE√á√ÉO AQUI: A linha de limpeza agressiva 'clean_json_str = re.sub(...)' FOI REMOVIDA. ---
    # Agora passamos a string JSON decodificada diretamente para json.loads.
    service_account_info = json.loads(decoded_json_str)
    
    logging.info("‚úÖ Arquivo de credenciais Base64 lido e JSON decodificado com sucesso (limpeza agressiva removida).")
    
    creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)
    drive_service = build("drive", "v3", credentials=creds)
    gc = gspread.authorize(creds)
    client = gc
    logging.info("‚úÖ Autentica√ß√£o Google OK")
    print("‚úÖ Autentica√ß√£o Google OK.")
except FileNotFoundError:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google: Arquivo de credenciais n√£o encontrado em '{CAMINHO_CREDENCIAIS}'. Verifique o caminho e a cria√ß√£o do arquivo no workflow.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Arquivo de credenciais n√£o encontrado. O pipeline ser√° encerrado.")
except base64.binascii.Error as e:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google: Erro ao decodificar a string Base64. Conte√∫do inv√°lido no secret? Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Conte√∫do Base64 inv√°lido no arquivo de credenciais.")
except json.JSONDecodeError as e:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google: Erro ao decodificar JSON da string Base64 decodificada. Conte√∫do inv√°lido. Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    # Se este erro ainda ocorrer, o problema est√° na string JSON decodificada antes de qualquer limpeza.
    # O log do YML (com xxd -p) ser√° crucial para ver o que o Base64 produziu.
    raise SystemExit("Erro cr√≠tico: Conte√∫do JSON inv√°lido na string Base64 decodificada.")
except Exception as e:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google. Erro inesperado: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Falha inesperada na autentica√ß√£o Google. O pipeline ser√° encerrado.")

# ========================================================
# 2) LEITURA DA PLANILHA BRUTA (GOOGLE DRIVE - DIN√ÇMICO) - MANTIDO COM MELHORIAS DE TRY/EXCEPT
# ========================================================
_BANNER("2) LEITURA DA PLANILHA BRUTA (GOOGLE DRIVE - DIN√ÇMICO)")

# As importa√ß√µes de 'googleapiclient.discovery', 'google.oauth2.service_account',
# 'gspread', 'pandas', 'logging' j√° est√£o no topo do arquivo.
# N√£o precisam ser repetidas aqui.

# --- Fun√ß√£o helper para obter a √∫ltima planilha da pasta bruta ---
# Esta fun√ß√£o j√° estava bem definida.
def get_latest_spreadsheet_df(folder_id: str, gspread_client, drive_svc) -> (str, str, pd.DataFrame):
    try:
        res = drive_svc.files().list(
            q=f"'{folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet' and trashed=false",
            orderBy="modifiedTime desc",
            pageSize=1,
            fields="files(id, name, modifiedTime)"
        ).execute()
        files = res.get("files", [])
        if not files:
            logging.critical(f"‚ùå Nenhuma planilha bruta encontrada na pasta do Google Drive com ID: '{folder_id}'. O pipeline ser√° encerrado.", exc_info=True)
            raise SystemExit("Erro cr√≠tico: Nenhuma planilha bruta encontrada.")
        latest = files[0]
        fid, fname = latest["id"], latest["name"]
        sh = gspread_client.open_by_key(fid)
        aba = sh.sheet1
        dfb = pd.DataFrame(aba.get_all_records())
        return fid, fname, dfb
    except Exception as e:
        logging.critical(f"‚ùå Erro ao obter a √∫ltima planilha da pasta bruta '{folder_id}': {e}. O pipeline ser√° encerrado.", exc_info=True)
        raise SystemExit("Erro cr√≠tico: Falha ao carregar planilha bruta.")


# --- Uso ---
FOLDER_ID_BRUTA = "1qXj9eGauvOREKVgRPOfKjRlLSKhefXI5" # Mantenha seu ID de pasta aqui
try:
    latest_file_id, latest_file_name, df_bruta = get_latest_spreadsheet_df(FOLDER_ID_BRUTA, gc, drive_service)
    df = df_bruta.copy()

    print(f"üìÇ √öltima planilha encontrada: {latest_file_name} ({latest_file_id})")
    logging.info(f"√öltima planilha encontrada: {latest_file_name} ({latest_file_id})")
    print(f"‚úÖ Planilha bruta importada com sucesso: {df_bruta.shape}")
    logging.info(f"Planilha bruta importada com sucesso: {df_bruta.shape}")
except SystemExit: # Captura o SystemExit da fun√ß√£o helper para n√£o logar novamente
    raise
except Exception as e:
    logging.critical(f"‚ùå Erro ao processar a planilha bruta principal. Verifique o FOLDER_ID_BRUTA e permiss√µes. Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Falha no processamento da planilha bruta.")

# ========================================================
# 3) NORMALIZA√á√ÉO DE NOMES DE COLUNA - MANTIDO COM MELHORIAS DE TRY/EXCEPT
# ========================================================
_BANNER("3) NORMALIZA√á√ÉO DE NOMES DE COLUNA")

# Fun√ß√£o normalizar_nome_coluna j√° estava bem definida.
def normalizar_nome_coluna(col: str) -> str:
    if col is None:
        return ""
    col = unicodedata.normalize("NFKD", str(col)).encode("ASCII", "ignore").decode("utf-8")
    col = col.lower()
    col = re.sub(r"[^a-z0-9]+", "_", col)
    return re.sub(r"_+", "_", col).strip("_")

try:
    df.columns = [normalizar_nome_coluna(c) for c in df.columns]
    print("‚úÖ Cabe√ßalhos normalizados:", list(df.columns))
    logging.info(f"Cabe√ßalhos normalizados: {list(df.columns)}")

    # Padroniza a coluna 'protocolo' consistentemente (strip + upper)
    def normalize_protocolo_col(df_local: pd.DataFrame, col: str = "protocolo") -> pd.DataFrame:
        if col in df_local.columns:
            df_local[col] = df_local[col].astype(str).str.strip().str.upper()
        else:
            logging.warning(f"‚ö†Ô∏è Coluna '{col}' n√£o encontrada ap√≥s normaliza√ß√£o de protocolo!")
        return df_local

    df = normalize_protocolo_col(df, "protocolo")
    logging.info("Coluna 'protocolo' padronizada.")
except Exception as e:
    logging.critical(f"‚ùå Erro na normaliza√ß√£o de nomes de coluna ou padroniza√ß√£o de protocolo. Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Falha na normaliza√ß√£o de dados.")

# ========================================================
# 4) FUN√á√ïES AUXILIARES (codifica√ß√£o / datas / post em lotes) - MANTIDO
# ========================================================
_BANNER("4) AUXILIARES (codifica√ß√£o, datas, lotes)")

# Defini√ß√£o do canon_txt
def _canon_txt(v) -> str:
    """
    Fun√ß√£o de canoniza√ß√£o de texto: converte para string, remove acentos,
    converte para min√∫sculas e limpa espa√ßos.
    """
    if v is None:
        return ""
    # Normaliza para decompor caracteres acentuados (ex: '√°' -> 'a' + '¬¥')
    s = unicodedata.normalize("NFKD", str(v))
    # Remove os caracteres de combina√ß√£o (acentos)
    s = "".join(c for c in s if not unicodedata.combining(c))
    # Converte para min√∫sculas e remove espa√ßos no in√≠cio/fim
    s = s.lower().strip()
    # Substitui m√∫ltiplos espa√ßos por um √∫nico espa√ßo
    s = re.sub(r"\s+", " ", s)
    return s
    
# Exemplo para estrutura, voc√™ deve ter TODAS as suas fun√ß√µes aqui
def _canon_responsavel_series(series: pd.Series) -> pd.Series:
    base = pd.Series(series, dtype="object").apply(_canon_txt)
    patt_ouvidoria_saude = r"(?i)^ouvidoria setorial da sa(?:u|√É¬∫|\\u00fa|\?\?|[\ufffdÔøΩ])?de$"
    return base.str.strip().replace({
        patt_ouvidoria_saude: "Ouvidoria Setorial da Sa√∫de",
        r"(?i)^cidad(?:\u00e3|√£)o$": "Cidad√£o",
    }, regex=True)
    
# Padroniza data_da_criacao para o formato data DD/MM/AAAA
def _to_ddmmaa_text(series: pd.Series) -> pd.Series:
    EXCEL_BASE = pd.Timestamp("1899-12-30")
    def _one(v):
        if pd.isna(v): return None
        if isinstance(v, (pd.Timestamp, np.datetime64)):
            dt = pd.to_datetime(v, errors="coerce")
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else None ## <-- MUDAN√áA AQUI de %y para %Y
        s = str(v).strip()
        if s == "": return None
        s2 = s.replace("T", " ").replace("Z", "")
        s2 = re.sub(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$", "", s2).strip()
        if re.match(r"^\d{4}-\d{2}-\d{2}", s2):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                dt = pd.to_datetime(s2, errors="coerce", dayfirst=False)
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s ## <-- MUDAN√áA AQUI de %y para %Y
        if re.fullmatch(r"\d{5,6}(\.\d+)?", s2):
            try: return (EXCEL_BASE + pd.to_timedelta(float(s2), "D")).strftime("%d/%m/%Y") ## <-- MUDAN√áA AQUI de %y para %Y
            except: pass
        if re.fullmatch(r"\d{13}", s2):
            dt = pd.to_datetime(int(s2), unit="ms", errors="coerce")
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s ## <-- MUDAN√áA AQUI de %y para %Y
        if re.fullmatch(r"\d{10}(\.\d+)?", s2):
            dt = pd.to_datetime(float(s2), unit="s", errors="coerce")
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s ## <-- MUDAN√áA AQUI de %y para %Y
        for fmt in ["%d/%m/%Y %H:%M:%S","%d/%m/%Y %H:%M","%d/%m/%Y",
                    "%d/%m/%y %H:%M:%S","%d/%m/%y %H:%M","%d/%m/%y",

                    "%Y-%m-%d %H:%M:%S","%Y-%m-%d %H:%M","%Y-%m-%d"]:
            try: return pd.to_datetime(s2, format=fmt).strftime("%d/%m/%Y") ## <-- MUDAN√áA AQUI de %y para %Y
            except: pass
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            dt = pd.to_datetime(s2, dayfirst=True, errors="coerce")
        return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s ## <-- MUDAN√áA AQUI de %y para %Y
    return series.apply(_one).astype("object")

# Padroniza data_da_conclusao para o formato data DD/MM/AAAA
def _conclusao_strict(series: pd.Series) -> pd.Series:
    s = pd.Series(series, dtype="object").astype(str).str.strip()
    s_cf = s.str.casefold()
    invalid = {"n√£o informado","na","n/a","n\\a","nan","null","none","","-","--",
               "outro","outros","nat","sem informa√ß√£o","sem informacao"}
    out = s.copy()
    mask_invalid = s_cf.isin(invalid)
    out.loc[mask_invalid] = "N√£o conclu√≠do"
    rest_idx = out.index[~mask_invalid]
    if len(rest_idx) > 0:
        s_rest = s.loc[rest_idx]
        s_norm = s_rest.str.replace("T"," ").str.replace("Z","", regex=False)
        s_norm = s_norm.str.replace(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$","", regex=True).str.strip()
        iso_mask = s_norm.str.match(r"^\d{4}-\d{2}-\d{2}")
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            dt = pd.Series(pd.NaT, index=rest_idx, dtype="datetime64[ns]")
            iso_idx = iso_mask[iso_mask].index
            if len(iso_idx) > 0:
                dt.loc[iso_idx] = pd.to_datetime(s_rest.loc[iso_idx], errors="coerce", dayfirst=False)
            non_iso_idx = iso_mask[~iso_mask].index
            if len(non_iso_idx) > 0:
                dt.loc[non_iso_idx] = pd.to_datetime(s_rest.loc[non_iso_idx], errors="coerce", dayfirst=True)
        good_idx = dt.index[dt.notna()]
        if len(good_idx) > 0:
            out.loc[good_idx] = dt.loc[good_idx].dt.strftime("%d/%m/%Y") ## <-- MUDAN√áA AQUI de %y para %Y
    return out

def _parse_dt_cmp(series: pd.Series) -> pd.Series:
    EXCEL_BASE = pd.Timestamp("1899-12-30")
    def _one(v):
        if pd.isna(v): return pd.NaT
        s = str(v).strip()
        if s == "": return pd.NaT
        s2 = s.replace("T"," ").replace("Z","")
        s2 = re.sub(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$","",s2).strip()
        if re.fullmatch(r"\d{5,6}(\.\d+)?", s2):
            try: return EXCEL_BASE + pd.to_timedelta(float(s2), "D")
            except: return pd.NaT
        if re.fullmatch(r"\d{13}", s2):
            return pd.to_datetime(int(s2), unit="ms", errors="coerce")
        if re.fullmatch(r"\d{10}(\.\d+)?", s2):
            return pd.to_datetime(float(s2), unit="s", errors="coerce")
        for fmt in ["%d/%m/%Y %H:%M:%S","%d/%m/%Y %H:%M","%d/%m/%Y",
                    "%d/%m/%y %H:%M:%S","%d/%m/%y %H:%M","%d/%m/%y",
                    "%Y-%m-%d %H:%M:%S","%Y-%m-%d %H:%M","%Y-%m-%d"]:
            try: return pd.to_datetime(s2, format=fmt)
            except: pass
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            return pd.to_datetime(s2, dayfirst=True, errors="coerce")
    return series.apply(_one)

def _is_nao_ha_dados(v) -> bool:
    if v is None: return False
    s = str(v).strip()
    if s == "": return False
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = re.sub(r"\s+", " ", s).strip().casefold()
    return s == "nao ha dados"

def _is_concluida(v) -> bool:
    if pd.isna(v): return False
    s = str(v).strip()
    if s == "": return False
    s = "".join(ch for ch in unicodedata.normalize("NFD", s) if unicodedata.category(ch) != "Mn")
    s = re.sub(r"[^A-Za-z]+", " ", s).strip().casefold()
    return s == "concluida"

def _looks_like_demanda_concluida(v) -> bool:
    if pd.isna(v): return False
    s = str(v).strip()
    if s == "": return False
    s = "".join(ch for ch in unicodedata.normalize("NFD", s) if unicodedata.category(ch) != "Mn")
    s = s.replace("ÔøΩ", "i").replace("?", "i")
    s = re.sub(r"i{2,}", "i", s, flags=re.IGNORECASE)
    s = re.sub(r"[^A-Za-z]+", " ", s).strip().casefold()
    return s == "demanda concluida"

def _canon_prazo_restante(v):
    if pd.isna(v): return v
    if isinstance(v, (int, float)) and not pd.isna(v):
        return v
    s = _canon_txt(v)
    if s == "": return s
    s_clean = s.strip()
    if _looks_like_demanda_concluida(s_clean):
        return "Demanda Conclu√≠da"
    return s_clean

# ============================================= 
# 5) COLETA DE PROTOCOLOS EXISTENTES NA PLANILHA TRATADA - MANTIDO (com ajuste de logging)
# =============================================
_BANNER("5) COLETA DE PROTOCOLOS EXISTENTES NA PLANILHA TRATADA")

try:
    # ---------- CONSTANTES / IDs ----------
    # Defina PLANILHA_TRATADA_ID no topo do arquivo ou altere aqui diretamente:
    PLANILHA_TRATADA_ID = "1SmO5yTD5B6fN_gT-7m1wosP_sbzmtd0agTC-LNCnX9Y"  # <-- coloque aqui o ID CORRETO da planilha tratada fixa

    # ---------- ABRE A PLANILHA TRATADA (√∫nica fonte) ----------
    planilha_tratada_gs = gc.open_by_key(PLANILHA_TRATADA_ID) # Renomeado para evitar conflito com df_tratada
    aba_tratada = planilha_tratada_gs.sheet1
    logging.info(f"Planilha tratada '{PLANILHA_TRATADA_ID}' aberta.")

    df_tratada = pd.DataFrame(aba_tratada.get_all_records())
    df_tratada.columns = [normalizar_nome_coluna(c) for c in df_tratada.columns]

    df_tratada = normalize_protocolo_col(df_tratada, "protocolo")
    protocolos_existentes_set = set(df_tratada["protocolo"].astype(str).tolist())

    # ---------- L√ä A √öLTIMA PLANILHA BRUTA (reutiliza FOLDER_ID_BRUTA e helper) ----------
    # IMPORTANTE: get_latest_spreadsheet_df deve existir (Item 2)
    if 'FOLDER_ID_BRUTA' in globals():
        folder_id = FOLDER_ID_BRUTA
    else:
        folder_id = "1qXj9eGauvOREKVgRPOfKjRlLSKhefXI5"  # fallback se n√£o definido acima

    latest_file_id, latest_file_name, df_bruta = get_latest_spreadsheet_df(folder_id, gc, drive_service)

    # Normaliza colunas e protocolo da bruta
    df_bruta.columns = [normalizar_nome_coluna(c) for c in df_bruta.columns]
    df_bruta = normalize_protocolo_col(df_bruta, "protocolo")

    # Marca novos protocolos (compara√ß√£o com o conjunto da tratada)
    df_bruta["eh_novo"] = ~df_bruta["protocolo"].isin(protocolos_existentes_set)
    novos_protos = df_bruta.loc[df_bruta["eh_novo"], "protocolo"].tolist()

    print(f"üîë Protocolos j√° na planilha tratada: {len(protocolos_existentes_set)}")
    print(f"üÜï Protocolos detectados como novos: {len(novos_protos)}")
    logging.info(f"Protocolos j√° na planilha tratada: {len(protocolos_existentes_set)}")
    logging.info(f"Protocolos detectados como novos: {novos_protos[:50]}")

    # Log dos existentes que n√£o ser√£o enviados
    nao_enviados = df_bruta.loc[~df_bruta["eh_novo"], "protocolo"].tolist()
    print(f"‚ö†Ô∏è Protocolos existentes que n√£o ser√£o enviados (n√£o novos): {len(nao_enviados)}")
    logging.info(f"Protocolos existentes que n√£o ser√£o enviados: {nao_enviados[:50]}")

    # Verifica√ß√£o final
    if df_bruta.empty:
        raise Exception("A planilha bruta mais recente est√° vazia ou n√£o p√¥de ser lida.")

    # Substitui df pelo df_bruta "oficial" para manter compatibilidade posterior
    df = df_bruta.copy()

except Exception as e:
    print(f"‚ö†Ô∏è Erro ao carregar planilhas: {e}")
    logging.warning(f"Erro ao carregar planilhas: {e}")
    df_tratada = pd.DataFrame()
    protocolos_existentes_set = set()
    df = pd.DataFrame()
    df["eh_novo"] = True
    novos_protos = []
    nao_enviados = []

# ========================================================
# 5) COLETA DE PROTOCOLOS EXISTENTES ...
# ... (final do seu c√≥digo do Item 5)
# ========================================================


# ========================================================
# 5) COLETA DE PROTOCOLOS EXISTENTES ...
# ... (final do seu c√≥digo do Item 5)
# ========================================================


# ======================================================================= #
# ========= IN√çCIO DO BLOCO TEMPOR√ÅRIO E DEFINITIVO - INSIRA AQUI ========= #
# ======================================================================= #

# ===================================================================================
# 5.5) SINCRONIZA√á√ÉO COMPLETA E TEMPOR√ÅRIA DE 'servidor' (REMOVER AP√ìS 1¬™ EXECU√á√ÉO)
# ===================================================================================
_BANNER("5.5) SINCRONIZA√á√ÉO COMPLETA E TEMPOR√ÅRIA DE 'servidor'")
print(" Executando sincroniza√ß√£o COMPLETA para a coluna 'servidor'. Isso pode levar um momento...")
logging.info("INICIANDO: Sincroniza√ß√£o COMPLETA e tempor√°ria da coluna 'servidor'.")

try:
    # 1. GARANTE QUE TEMOS OS DOIS DATAFRAMES COMPLETOS
    # Recarrega a base bruta e a tratada para garantir que temos os dados mais recentes e completos
    _SUB("Carregando e preparando dados para sincroniza√ß√£o...")
    
    # Carrega a base bruta (df_bruta) e normaliza
    latest_file_id_sync, latest_file_name_sync, df_bruta_sync = get_latest_spreadsheet_df(FOLDER_ID_BRUTA, gc, drive_service)
    df_bruta_sync.columns = [normalizar_nome_coluna(c) for c in df_bruta_sync.columns]
    print(f" Base bruta para sincroniza√ß√£o carregada: {df_bruta_sync.shape[0]} linhas.")

    # Carrega a base tratada (df_tratada) e normaliza
    df_tratada_sync = pd.DataFrame(aba_tratada.get_all_records())
    df_tratada_sync.columns = [normalizar_nome_coluna(c) for c in df_tratada_sync.columns]
    print(f" Base tratada para sincroniza√ß√£o carregada: {df_tratada_sync.shape[0]} linhas.")
    
    if 'protocolo' not in df_bruta_sync.columns or 'servidor' not in df_bruta_sync.columns or 'protocolo' not in df_tratada_sync.columns:
        raise ValueError("Colunas 'protocolo' ou 'servidor' n√£o encontradas. Verifique os nomes das colunas nas planilhas.")

    # 2. CRIA UM MAPA DE "VERDADE" A PARTIR DA BASE BRUTA
    # Este mapa ter√°: {protocolo: servidor_correto}
    _SUB("Criando mapa de servidores corretos a partir da base bruta...")
    # Remove duplicatas da base bruta para garantir um mapa limpo
    df_bruta_sync.drop_duplicates(subset=['protocolo'], keep='first', inplace=True)
    mapa_servidor_correto = df_bruta_sync.set_index('protocolo')['servidor'].to_dict()
    print(f" Mapa de servidores criado com {len(mapa_servidor_correto)} protocolos √∫nicos da base bruta.")

    # 3. IDENTIFICA AS DIFEREN√áAS NA BASE TRATADA
    _SUB("Identificando registros que precisam de corre√ß√£o na base tratada...")
    # Aplica o mapa √† base tratada para encontrar o valor que 'servidor' deveria ter
    df_tratada_sync['servidor_correto'] = df_tratada_sync['protocolo'].map(mapa_servidor_correto)
    
    # Compara o valor atual com o valor correto, ignorando os que n√£o est√£o na base bruta (NaN)
    df_para_atualizar = df_tratada_sync[
        (df_tratada_sync['servidor_correto'].notna()) & 
        (df_tratada_sync['servidor'] != df_tratada_sync['servidor_correto'])
    ]
    
    if df_para_atualizar.empty:
        print("‚úÖ Nenhuma diverg√™ncia encontrada. A coluna 'servidor' j√° est√° sincronizada.")
        logging.info("Nenhuma diverg√™ncia encontrada na sincroniza√ß√£o de 'servidor'.")
    else:
        print(f" Encontradas {len(df_para_atualizar)} linhas para corrigir na coluna 'servidor'.")
        logging.info(f"Encontradas {len(df_para_atualizar)} linhas para corrigir em 'servidor'.")

        # 4. PREPARA E EXECUTA A ATUALIZA√á√ÉO EM LOTE
        _SUB("Preparando e enviando a atualiza√ß√£o para o Google Sheets...")
        TARGET_COL_INDEX = df_tratada_sync.columns.get_loc('servidor') + 1 # Encontra o √≠ndice da coluna dinamicamente
        
        protocolos_na_sheet = aba_tratada.col_values(1)
        protocolo_para_linha = {proto: i + 1 for i, proto in enumerate(protocolos_na_sheet)}
        
        cells_to_update = []
        for _, row in df_para_atualizar.iterrows():
            protocolo = row['protocolo']
            servidor_correto = row['servidor_correto']
            if protocolo in protocolo_para_linha:
                linha_idx = protocolo_para_linha[protocolo]
                cells_to_update.append(gspread.Cell(row=linha_idx, col=TARGET_COL_INDEX, value=str(servidor_correto)))

        if cells_to_update:
            aba_tratada.update_cells(cells_to_update, value_input_option='USER_ENTERED')
            print(f"‚úÖ Sincroniza√ß√£o COMPLETA da coluna 'servidor' conclu√≠da. {len(cells_to_update)} c√©lulas foram atualizadas.")
            print(" AVISO: Lembre-se de remover todo o bloco 'Item 5.5' do script ap√≥s esta execu√ß√£o.")
            logging.info(f"CONCLU√çDO: Sincroniza√ß√£o completa. {len(cells_to_update)} c√©lulas de 'servidor' atualizadas.")
        else:
            print(" Nenhuma c√©lula p√¥de ser mapeada para atualiza√ß√£o.")

except Exception as e:
    print(f"‚ùå Erro cr√≠tico durante a sincroniza√ß√£o completa: {e}")
    logging.error(f"Erro cr√≠tico durante a sincroniza√ß√£o completa do servidor: {e}", exc_info=True)


# ======================================================================= #
# =================== FIM DO BLOCO TEMPOR√ÅRIO DEFINITIVO ================== #
# ======================================================================= #


# ========================================================
# 6) LIMPEZA B√ÅSICA + RECORTE PARA NOVOS POR PROTOCOLO
# ========================================================
print("üßπ Limpando e identificando novos protocolos...")
df_tratada_protocolos = df_tratada["protocolo"].astype(str).str.strip().tolist()
df["protocolo"] = df["protocolo"].astype(str).str.strip()

df["eh_novo"] = ~df["protocolo"].isin(df_tratada_protocolos)
novos = df[df["eh_novo"] == True]
existentes = df[df["eh_novo"] == False]

print(f"üÜï Novos protocolos: {len(novos)}")
print(f"üîÑ Protocolos existentes: {len(existentes)}")
logging.info(f"Novos protocolos: {len(novos)}, Existentes: {len(existentes)}")

# ========================================================
# 7) TRATAMENTOS E ATUALIZA√á√ÉO DE DADOS (somente NOVOS)
# ========================================================
_BANNER("7) TRATAMENTOS (somente NOVOS)")

# Seleciona apenas os protocolos novos identificados no Item 5
df_novos = df[df["eh_novo"] == True].copy()

if df_novos.empty:
    logging.info("Nenhum protocolo novo para tratamento.")
else:
    logging.info(f"Aplicando tratamentos em {len(df_novos)} protocolos novos. Shape inicial: {df_novos.shape}")

def _tratar_full(df_in: pd.DataFrame) -> pd.DataFrame:
    df_loc = df_in.copy()
    logging.debug(f"Iniciando _tratar_full com DataFrame de shape: {df_loc.shape}")

    # 7.1 Tema/Assunto ‚Äî mant√©m 'n√£o se aplica' ‚Üí 'Ass√©dio'
    try:
        if "tema" in df_loc.columns and "assunto" in df_loc.columns:
            tema_tmp = df_loc["tema"].astype(str).str.strip().str.casefold()
            assunto_tmp = df_loc["assunto"].astype(str).str.strip().str.casefold()
            valores_assunto = ["outro", "outros", "na", "n/a", "n\\a", ""]
            cond_42 = (tema_tmp == "n√£o se aplica") & (assunto_tmp.isin(valores_assunto))
            if int(cond_42.sum()):
                df_loc.loc[cond_42, "assunto"] = "Ass√©dio"
                logging.info(f"Tratamento 7.1 (Assunto) aplicado a {int(cond_42.sum())} linhas.")
            cond_41 = (tema_tmp == "n√£o se aplica")
            if int(cond_41.sum()):
                df_loc.loc[cond_41, "tema"] = "Ass√©dio"
                logging.info(f"Tratamento 7.1 (Tema) aplicado a {int(cond_41.sum())} linhas.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.1 (Tema/Assunto): {e}", exc_info=True)

    # 7.2 Data da conclus√£o ‚Üí texto "DD/MM/AA" ou "N√£o conclu√≠do"
    try:
        if "data_da_conclusao" in df_loc.columns:
            df_loc["data_da_conclusao"] = _conclusao_strict(df_loc["data_da_conclusao"])
            df_loc["data_da_conclusao"] = df_loc["data_da_conclusao"].apply(
                lambda x: x if pd.notna(x) and str(x).strip().lower() not in ["na", "nan", "n/a", ""] else "N√£o conclu√≠do"
            )
            logging.info("Tratamento 7.2 (Data da Conclus√£o) aplicado.")
            # QA: Verifica se a coluna tem valores inv√°lidos ap√≥s o tratamento
            invalid_dates = df_loc["data_da_conclusao"].apply(
                lambda x: pd.isna(x) or (str(x).strip().lower() not in ["n√£o conclu√≠do"] and not re.match(r"\d{2}/\d{2}/\d{2}", str(x)))
            )
            if invalid_dates.any():
                logging.warning(f"QA 7.2: Coluna 'data_da_conclusao' cont√©m valores inv√°lidos/inesperados ap√≥s tratamento em {invalid_dates.sum()} linhas. Exemplos: {df_loc.loc[invalid_dates, 'data_da_conclusao'].unique()[:5].tolist()}")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.2 (Data da Conclus√£o): {e}", exc_info=True)


    # 7.3 Unidades de sa√∫de (capitaliza e trata ‚Äúsem informa√ß√£o‚Äù)
    try:
        for col in df_loc.columns:
            if "unidade" in col and "saude" in col:
                linhas_alteradas = (df_loc[col].astype(str).str.strip().str.lower() == "sem informa√ß√£o").sum()
                df_loc[col] = (
                    df_loc[col].astype(str).str.strip().str.lower()
                    .replace("sem informa√ß√£o", "N√£o √© uma Unidade de Sa√∫de")
                    .str.capitalize()
                )
                if linhas_alteradas > 0:
                    logging.info(f"Tratamento 7.3 (Unidades de Sa√∫de) aplicado na coluna '{col}' para {linhas_alteradas} linhas.")
                # QA para 'unidade'/'saude'
                if df_loc[col].astype(str).str.contains(r'(?i)(sim|nao|true|false|\?{2,})').any():
                    logging.warning(f"QA 7.3: Coluna '{col}' ainda cont√©m valores inesperados (Sim/N√£o/True/False/??) ap√≥s tratamento. Exemplos: {df_loc.loc[df_loc[col].astype(str).str.contains(r'(?i)(sim|nao|true|false|\?{2,})'), col].unique()[:5].tolist()}")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.3 (Unidades de Sa√∫de): {e}", exc_info=True)


    # 7.4 √ìrg√£os por tema ‚Äî MATCH EXATO, fallback apenas se TEMA vazio
    # Reafirma√ß√£o das fun√ß√µes auxiliares para garantir auto-sufici√™ncia deste bloco
    import unicodedata as _ud, re as _re

    def _norm(s):
        if pd.isna(s): return ""
        s = str(s).strip().lower()
        s = _ud.normalize("NFD", s)
        s = "".join(ch for ch in s if _ud.category(ch) != "Mn")
        return _re.sub(r"\s+", " ", s)

    def _div_temas(v, seps=(",", ";", "|", "/")):
        if pd.isna(v): return []
        t = str(v)
        for s in seps: t = t.replace(s, ",")
        partes = [p.strip() for p in t.split(",") if p.strip()]
        return partes if partes else [str(v).strip()]

    map_tema_para_orgao = {
        "Administra√ß√£o P√∫blica":"Secretaria de Administra√ß√£o","Agricultura":"Secretaria de Obras e Agricultura",
        "Assist√™ncia Social e Direitos Humanos":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Assuntos Jur√≠dicos":"Procuradoria Geral","Comunica√ß√£o Social":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas",
        "Controle Governamental":"Secretaria de Controle Interno","Crian√ßa, Adolescente e Idoso":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Cultura e Turismo":"Secretaria de Cultura e Turismo","Defesa Civil":"Secretaria de Defesa Civil",
        "Direitos √† Pessoa com Defici√™ncia":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Direitos e Vantagens do Servidor":"Secretaria de Administra√ß√£o","Educa√ß√£o":"Secretaria de Educa√ß√£o",
        "Empresas e Legaliza√ß√µes":"Secretaria de Fazenda","Esporte e Lazer":"Secretaria de Esporte e Lazer",
        "Fiscaliza√ß√£o e tributos":"Secretaria de Fazenda","Fiscaliza√ß√£o Urbana, Regulariza√ß√£o e Registro de Im√≥veis":"Secretaria de Urbanismo e Habita√ß√£o",
        "FUNDEC":"FUNDEC","Governan√ßa":"Secretaria de Governo","Governo Municipal e Enterro Gratuito":"Secretaria de Governo",
        "Habita√ß√£o":"Secretaria de Urbanismo e Habita√ß√£o","Inclus√£o e Acessibilidade":"Secretaria de Gest√£o, Inclus√£o e Mulher",
        "Meio Ambiente":"Secretaria de Meio Ambiente",
        "Meio Ambiente (Polui√ß√£o Sonora, √Årvores, Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.)":"Secretaria de Meio Ambiente",
        "Ass√©dio":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas","Obras P√∫blicas":"Secretaria de Obras e Agricultura",
        "Obras, Limpeza Urbana e Bra√ßo de Luz":"Secretaria de Obras e Agricultura","Prote√ß√£o Animal":"Secretaria de Prote√ß√£o Animal",
        "Sa√∫de":"Secretaria de Sa√∫de","Seguran√ßa P√∫blica":"Secretaria de Seguran√ßa P√∫blica",
        "Seguran√ßa, Sinaliza√ß√£o e Multas":"Secretaria de Seguran√ßa P√∫blica",
        "Trabalho, Emprego e Renda":"Secretaria de Trabalho, Emprego e Renda",
        "Transportes e Servi√ßos P√∫blicos":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "Transportes, Servi√ßos P√∫blicos e Troca de L√¢mpadas":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "Urbanismo":"Secretaria de Urbanismo e Habita√ß√£o",
        "Vetores e Zoonoses (Combate √† Dengue, Controle de Pragas, Cria√ß√£o Irregular de Animais e etc.)":"Secretaria de Sa√∫de",
        "Vigil√¢ncia Sanit√°ria":"Secretaria de Sa√∫de",
        "Obras":"Secretaria de Obras e Agricultura","Trabalho":"Secretaria de Trabalho, Emprego e Renda",
        "Seguran√ßa":"Secretaria de Seguran√ßa P√∫blica","Servi√ßos P√∫blicos e Troca de L√¢mpadas":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "√Årvores":"Secretaria de Meio Ambiente","Controle de Pragas":"Secretaria de Sa√∫de","Cria√ß√£o Irregular de Animais":"Secretaria de Sa√∫de",
        "Adolescente e Idoso":"Secretaria de Assist√™ncia Social e Direitos Humanos","Crian√ßa":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Emprego e Renda":"Secretaria de Trabalho, Emprego e Renda","Fiscaliza√ß√£o Urbana":"Secretaria de Urbanismo e Habita√ß√£o",
        "Regulariza√ß√£o e Registro de Im√≥veis":"Secretaria de Urbanismo e Habita√ß√£o","Limpeza Urbana e Bra√ßo de Luz":"Secretaria de Obras e Agricultura",
        "Meio Ambiente (Polui√ß√£o Sonora)":"Secretaria de Meio Ambiente","Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.":"Secretaria de Meio Ambiente",
        "Vetores e Zoonoses (Combate √† Dengue)":"Secretaria de Sa√∫de",
        "Cria√ß√£o Irregular de Animais e etc.)":"Secretaria de Sa√∫de","Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.)":"Secretaria de Meio Ambiente",
        "Meio Ambiente (Polui√ß√£o Sonora":"Secretaria de Meio Ambiente","N√£o se aplica":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas",
        "Sinaliza√ß√£o e Multas":"Secretaria de Seguran√ßa P√∫blica","Transportes":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "Vetores e Zoonoses (Combate √† Dengue":"Secretaria de Sa√∫de",
    }
    map_tema_para_orgao = {k: _canon_txt(v) for k, v in map_tema_para_orgao.items()}
    map_exact = { _norm(k): v for k, v in map_tema_para_orgao.items() }

    def mapear_orgao_exato(celula_tema):
        orgs = []
        # Garante que celula_tema √© string antes de passar para _div_temas
        tema_as_str = str(celula_tema) if pd.notna(celula_tema) else ""
        for t in _div_temas(tema_as_str):
            t_norm = _norm(t)
            if not t_norm:
                continue
            if t_norm in map_exact:
                orgs.append(map_exact[t_norm])
        # Garante que sempre retorna uma string ou None, nunca uma lista vazia ou algo booleano
        return " | ".join(dict.fromkeys(o.strip() for o in orgs if o and str(o).strip())) or None

    try:
        if "tema" in df_loc.columns:
            # Explicitamente converte 'tema' para string ANTES de aplicar a l√≥gica,
            # para evitar que booleanos ou outros tipos sejam passados para as fun√ß√µes de mapeamento.
            df_loc["tema"] = df_loc["tema"].astype(str)
            logging.debug("Coluna 'tema' convertida para string.")

            def atribuir_orgao_para_linha(row):
                tema_val = row.get("tema") # 'tema_val' ser√° agora uma string
                orgao = mapear_orgao_exato(tema_val)
                if not orgao or str(orgao).strip() == "":
                    # Fallback, garanta que √© uma string, n√£o None ou booleano
                    return "Secretaria Municipal de Comunica√ß√£o e Rela√ß√µes P√∫blicas"
                return orgao # <-- CORRETAMENTE INDENTADO!

            # Aplica atribui√ß√£o de √≥rg√£os para TODAS as linhas, garantindo novos protocolos
            # Cria a coluna 'orgaos' se n√£o existir, ou a preenche se existir
            df_loc["orgaos"] = df_loc.apply(lambda row: atribuir_orgao_para_linha(row), axis=1)
            logging.info("Tratamento 7.4 (√ìrg√£os por tema) aplicado.")
        else:
            # Se 'tema' n√£o existe, garante que 'orgaos' √© criada ou preenchida com um valor padr√£o
            if "orgaos" not in df_loc.columns:
                df_loc["orgaos"] = "Secretaria Municipal de Comunica√ß√£o e Rela√ß√µes P√∫blicas"
                logging.warning("Coluna 'tema' ausente. 'orgaos' criada com valor padr√£o.")
            else:
                df_loc["orgaos"].fillna("Secretaria Municipal de Comunica√ß√£o e Rela√ß√µes P√∫blicas", inplace=True)
                df_loc["orgaos"] = df_loc["orgaos"].astype(str) # Garante que a coluna √© string
                logging.warning("Coluna 'tema' ausente. 'orgaos' preenchida com valor padr√£o e convertida para string.")

        # Padroniza√ß√£o final de √≥rg√£os
        def _canon_orgaos(cell):
            if cell is None or str(cell).strip() == "":
                return ""
            partes = [p.strip() for p in str(cell).split("|")]
            partes = [_canon_txt(p) for p in partes if p]
            return " | ".join(dict.fromkeys(partes))

        # Aplica canoniza√ß√£o final e garante tipo string
        df_loc["orgaos"] = df_loc["orgaos"].apply(_canon_orgaos).astype(str)
        logging.info("Tratamento 7.4 (Padroniza√ß√£o final de √≥rg√£os) aplicado.")

        # Fallback adicional para c√©lulas com TEMA vazio e ORG√ÉOS ainda vazios
        if "tema" in df_loc.columns: # Condi√ß√£o para evitar erro se 'tema' n√£o existir
            mask_tema_vazio = df_loc["tema"].isna() | (df_loc["tema"].astype(str).str.strip() == "")
            mask_org_vazio  = df_loc["orgaos"].isna() | (df_loc["orgaos"].astype(str).str.strip() == "")
            if (mask_tema_vazio & mask_org_vazio).any():
                count_fallback = (mask_tema_vazio & mask_org_vazio).sum()
                df_loc.loc[mask_tema_vazio & mask_org_vazio, "orgaos"] = "Secretaria Municipal de Comunica√ß√£o e Rela√ß√µes P√∫blicas"
                logging.warning(f"QA 7.4: {count_fallback} linhas tiveram 'orgaos' preenchido por fallback final (tema e orgaos vazios).")
        else: # Se 'tema' n√£o existe, preenche 'orgaos' onde estiver vazio
            count_fillna = df_loc["orgaos"].isna().sum()
            if count_fillna > 0:
                df_loc["orgaos"].fillna("Secretaria Municipal de Comunica√ß√£o e Rela√ß√µes P√∫blicas", inplace=True)
                logging.warning(f"QA 7.4: 'orgaos' preenchido por fallback final para {count_fillna} linhas (tema ausente).")

        # QA Final para 'orgaos': verifica valores inesperados (Sim/N√£o/True/False, etc.)
        unexpected_orgaos = df_loc["orgaos"].astype(str).str.contains(r'(?i)^(sim|nao|true|false|cidad√£o|\?{2,}|nan)$')
        if unexpected_orgaos.any():
            logging.error(f"QA 7.4: Coluna 'orgaos' ainda cont√©m valores inesperados em {unexpected_orgaos.sum()} linhas. Exemplos: {df_loc.loc[unexpected_orgaos, 'orgaos'].unique()[:5].tolist()}",
                          extra={'data': df_loc.loc[unexpected_orgaos, ['protocolo', 'tema', 'orgaos']].to_dict(orient='records')[:5]})
            # Considere levantar uma exce√ß√£o ou tomar uma a√ß√£o mais dr√°stica aqui se esses valores forem cr√≠ticos.

        logging.info(f"QA 7.4: value_counts da coluna 'orgaos' ap√≥s tratamento: \n{df_loc['orgaos'].value_counts(dropna=False).to_string()}")

    except Exception as e:
        logging.error(f"Erro no tratamento 7.4 (√ìrg√£os por tema): {e}", exc_info=True)


    # 7.5 Padroniza√ß√£o 'servidor' (dicion√°rio completo)
    try:
        dicionario_servidor = {
            "Camila do Lago Marins": "Camila Marins", "Camila Marins": "Camila Marins",
            "Dhayane Cristina Pinho de Almeida": "Dhayane Cristina Pinho de Almeida", "Dhayane Pinho": "Dhayane Cristina Pinho de Almeida",
            "Joana Darc Salles Ferreira": "Joana Darc Salles Ferreira", "Joana Salles": "Joana Darc Salles Ferreira",
            "Lucia Helena Tinoco Pacehco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
            "Lucia  Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "L√∫cia  Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
            "L√∫cia Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helenba Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
            "Rafaella Marques Gomes Santos": "Rafaella Marques Gomes Santos",
            "Roilene Pereira da Silva": "Rosilene Pereira da Silva", "Rosilene Pereira da Silva": "Rosilene Pereira da Silva",
            "Stephanie dos Santos Silva": "Stephanie dos Santos Silva", "Stephanie Santos": "Stephanie dos Santos Silva",
            "St√©phanie Santos": "Stephanie dos Santos Silva", "St√©phaniesantos": "Stephanie dos Santos Silva",
            "Stpehanie Santos": "Stephanie dos Santos Silva",
            "Anne Beatriz da Silva": "Anne Beatriz da Silva Rodrigues", "Bruna Maria ( Coordenadora)": "Cidad√£o",
            "Isabel": "Cidad√£o", "Gabriela da Silva Rozi": "Cidad√£o", "Lana Carolina Mesquita de Andrade": "Cidad√£o",
            "L√≠via Cavalcante": "L√≠via Kathleen Cavalcante Patriota Leite", "L√≠via Kathleen Cavalcante Patriota Leite": "L√≠via Kathleen Cavalcante Patriota Leite",
            "Lucia Helena": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helena Tinoco": "L√∫cia Helena Tinoco Pacheco Varella",
            "Lucia Helena Tinoco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helen Tinoco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
            "Lucia Helan Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helena  Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
            "Lucia Helena Tinoco Pachewco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
            "Mery": "Cidad√£o", "Ouvidoria Geral (Adm)": "Cidad√£o", "Rafaella Marques": "Rafaella Marques Gomes Santos",
            "Ronaldo de Oliveira Brand√£o": "Cidad√£o", "S√©phanie Santos": "Stephanie dos Santos Silva",
            "Shirley Santana": "Cidad√£o", "St√©panie Santos": "Stephanie dos Santos Silva",
            "St√©phanie  Santos": "Stephanie dos Santos Silva", "St√©phanie Santos": "Stephanie dos Santos Silva",
            "Stephanie dos Santos": "Stephanie dos Santos Silva", "St√©phanie Santoa": "Stephanie dos Santos Silva",
            "Stephanie Santos": "Stephanie dos Santos Silva", "Stephanie dos Santos": "Stephanie dos Santos Silva",
            "Stephanie Santos": "Stephanie dos Santos Silva", "Thamires Manh√£es": "Cidad√£o"
        }
        if "servidor" in df_loc.columns:
            _orig = df_loc["servidor"].astype(str).str.strip()
            df_loc["servidor"] = _orig.map(dicionario_servidor).fillna(_orig)
            logging.info("Tratamento 7.5 (Padroniza√ß√£o 'servidor') aplicado.")
            logging.debug(f"QA 7.5: value_counts da coluna 'servidor' ap√≥s tratamento: \n{df_loc['servidor'].value_counts(dropna=False).to_string(max_rows=10)}")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.5 (Padroniza√ß√£o 'servidor'): {e}", exc_info=True)


    # 7.6 Respons√°vel (normaliza√ß√£o)
    try:
        if "responsavel" in df_loc.columns:
            df_loc["responsavel"] = _canon_responsavel_series(df_loc["responsavel"])
            df_loc["responsavel"] = df_loc["responsavel"].astype(str).replace(
                {"Sim": "Cidad√£o", "N√£o": "N√£o Informado", "True": "Cidad√£o", "False": "N√£o Informado"}, regex=False
            )
            df_loc.loc[df_loc["responsavel"].str.strip() == "", "responsavel"] = "N√£o Informado"
            logging.info("Tratamento 7.6 (Respons√°vel) aplicado.")
            # QA para 'responsavel'
            unexpected_responsavel = df_loc["responsavel"].astype(str).str.contains(r'(?i)^(sim|nao|true|false|\?{2,}|nan)$')
            if unexpected_responsavel.any():
                logging.warning(f"QA 7.6: Coluna 'responsavel' ainda cont√©m valores inesperados em {unexpected_responsavel.sum()} linhas. Exemplos: {df_loc.loc[unexpected_responsavel, 'responsavel'].unique()[:5].tolist()}")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.6 (Respons√°vel): {e}", exc_info=True)


    # 7.7 Datas e tipos
    try:
        if "data_da_criacao" in df_loc.columns:
            df_loc["data_da_criacao"] = _to_ddmmaa_text(df_loc["data_da_criacao"]).astype(str)
            logging.info("Tratamento 7.7 (data_da_criacao) aplicado.")
        if "status_demanda" in df_loc.columns:
            df_loc["status_demanda"] = df_loc["status_demanda"].astype(str)
            logging.info("Tratamento 7.7 (status_demanda) tipo aplicado.")
        if "data_da_conclusao" in df_loc.columns:
            df_loc["data_da_conclusao"] = _conclusao_strict(df_loc["data_da_conclusao"])
            logging.info("Tratamento 7.7 (data_da_conclusao) strict aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.7 (Datas e Tipos): {e}", exc_info=True)


    # 7.8 Regra de ouro: se CONCLU√çDA => 'prazo_restante' = 'Demanda Conclu√≠da'
    try:
        if "status_demanda" in df_loc.columns and "prazo_restante" in df_loc.columns:
            mask_conc = df_loc["status_demanda"].map(_is_concluida)
            if mask_conc.any():
                df_loc.loc[mask_conc, "prazo_restante"] = "Demanda Conclu√≠da"
                logging.info(f"Tratamento 7.8 (prazo_restante p/ conclu√≠da) aplicado para {mask_conc.sum()} linhas.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.8 (Prazo Restante): {e}", exc_info=True)

    # --- NOVO TRATAMENTO 7.9: Padroniza√ß√£o adicional para 'responsavel' (Ouvidorias) ---
    try:
        if "responsavel" in df_loc.columns:
            # Garante que a coluna √© do tipo string para usar m√©todos .str
            df_loc["responsavel"] = df_loc["responsavel"].astype(str)

            # Padroniza "Ouvidoria Geral" (case-insensitive, ignora espa√ßos extras)
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*ouvidoria\s+geral\s*$", "Ouvidoria Geral", regex=True, case=False
            )

            # Padroniza "Ouvidoria Setorial de Obras" (case-insensitive, ignora espa√ßos extras)
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*ouvidoria\s+setorial\s+de\s+obras\s*$", "Ouvidoria Setorial de Obras", regex=True, case=False
            )
            logging.info("Tratamento 7.9 (Padroniza√ß√£o de Ouvidorias em 'responsavel') aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.9 (Padroniza√ß√£o de Ouvidorias): {e}", exc_info=True)

    # --- NOVO TRATAMENTO 7.10: Padroniza√ß√£o da coluna 'canal' ---
    try:
        if "canal" in df_loc.columns:
            df_loc["canal"] = df_loc["canal"].astype(str)
            # Combina 'Colab Gov' e 'Portal Cidad√£o' em 'Aplicativo Colab' (case-insensitive)
            df_loc["canal"] = df_loc["canal"].str.replace(
                r"^\s*(Colab Gov|Portal Cidad√£o)\s*$", "Aplicativo Colab", regex=True, case=False
            )
            logging.info("Tratamento 7.10 (Padroniza√ß√£o de 'canal') aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.10 (Padroniza√ß√£o de 'canal'): {e}", exc_info=True)


    logging.debug(f"Finalizando _tratar_full. Shape final: {df_loc.shape}")
    return df_loc

# Aplica o tratamento aos novos protocolos
try:
    if not df_novos.empty:
        df_novos = _tratar_full(df_novos)
        logging.info(f"Tratamento full aplicado a {len(df_novos)} protocolos novos.")
    else:
        logging.info("df_novos est√° vazio, pulando _tratar_full.")
except Exception as e:
    logging.critical(f"Erro CR√çTICO ao aplicar _tratar_full em df_novos: {e}", exc_info=True)
    # Dependendo da severidade, voc√™ pode querer levantar a exce√ß√£o ou parar o pipeline.
    raise

# QA p√≥s _tratar_full global para df_novos
if not df_novos.empty:
    for col in ['orgaos', 'responsavel', 'status_demanda', 'data_da_conclusao']:
        if col in df_novos.columns:
            empty_count = df_novos[col].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']).sum()
            if empty_count > 0:
                logging.warning(f"QA P√≥s-Tratamento (df_novos): Coluna '{col}' cont√©m {empty_count} valores vazios/inv√°lidos/n√£o informados. Exemplos: {df_novos.loc[df_novos[col].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']), col].unique()[:5].tolist()}")
            
            # Verifica√ß√£o de tipos para garantir que s√£o strings
            if not pd.api.types.is_string_dtype(df_novos[col]):
                logging.error(f"QA P√≥s-Tratamento (df_novos): Coluna '{col}' n√£o √© do tipo string ap√≥s tratamento. Tipo atual: {df_novos[col].dtype}. Convertendo para string.")
                df_novos[col] = df_novos[col].astype(str)
                
# ========================================================
# 8) ATUALIZA√á√ÉO NA PLANILHA TRATADA ‚Äî APENAS NOVOS
# ========================================================
_BANNER("8) ATUALIZA√á√ÉO NA PLANILHA TRATADA ‚Äî APENAS NOVOS")

# ----------------------------------------------------------
# GARANTE QUE df_bruta EXISTE E TEM A COLUNA 'protocolo'
# ----------------------------------------------------------
try:
    if 'df_bruta' not in globals() or df_bruta.empty:
        raise SystemExit("‚ùå df_bruta n√£o est√° definido ou est√° vazio. Carregue a base bruta antes do Item 8.")
    logging.info(f"df_bruta presente e com shape: {df_bruta.shape}")

    df_bruta.columns = [normalizar_nome_coluna(c) for c in df_bruta.columns] # Garante que est√° normalizado
    df_bruta = normalize_protocolo_col(df_bruta, "protocolo") # Garante que protocolo est√° padronizado
    logging.debug("Colunas e protocolos de df_bruta normalizados.")
except Exception as e:
    logging.critical(f"Erro na checagem inicial de df_bruta no Item 8: {e}", exc_info=True)
    raise

# ----------------------------------------------------------
# CARREGA PLANILHA TRATADA E OBT√âM PROTOCOLOS EXISTENTES
# ----------------------------------------------------------

try:
    if 'client' not in globals() or client is None:
        raise SystemExit("‚ùå Cliente gspread n√£o autenticado. Verifique Item 1.")

    PLANILHA_TRATADA_ID = "1SmO5yTD5B6fN_gT-7m1wosP_sbzmtd0agTC-LNCnX9Y"
    planilha_tratada_gs = client.open_by_key(PLANILHA_TRATADA_ID)
    aba_tratada = planilha_tratada_gs.sheet1
    logging.info(f"Planilha tratada '{PLANILHA_TRATADA_ID}' aberta.")
except Exception as e:
    logging.critical(f"Erro ao abrir a planilha tratada ou autenticar no Item 8: {e}", exc_info=True)
    raise

try:
    df_tratada_existente = pd.DataFrame(aba_tratada.get_all_records())
    df_tratada_existente.columns = [normalizar_nome_coluna(c) for c in df_tratada_existente.columns]
    logging.info(f"df_tratada_existente carregado com shape: {df_tratada_existente.shape}")

    protocolos_existentes_set_final = set()
    if "protocolo" in df_tratada_existente.columns:
        df_tratada_existente["protocolo"] = df_tratada_existente["protocolo"].astype(str).str.strip().str.upper()
        protocolos_existentes_set_final = set(df_tratada_existente["protocolo"])
        logging.debug(f"Set de protocolos existentes criado com {len(protocolos_existentes_set_final)} itens.")
    else:
        logging.warning("Coluna 'protocolo' n√£o encontrada em df_tratada_existente. N√£o ser√° poss√≠vel identificar protocolos existentes.")

    cols_alvo_tratada = list(df_tratada_existente.columns)
    if df_tratada_existente.empty:
        if 'df_novos' in globals() and not df_novos.empty:
            cols_alvo_tratada = list(df_novos.columns)
            logging.info("Planilha tratada vazia, usando colunas de df_novos como refer√™ncia para o schema.")
        elif not df_bruta.empty:
            cols_alvo_tratada = list(df_bruta.columns)
            logging.info("Planilha tratada vazia e df_novos vazio, usando colunas de df_bruta como refer√™ncia para o schema.")
        else:
            logging.error("N√£o foi poss√≠vel determinar o schema da planilha tratada. df_tratada_existente, df_novos e df_bruta est√£o vazios.")
            raise ValueError("N√£o foi poss√≠vel determinar o schema da planilha tratada.")
    logging.debug(f"Colunas alvo da planilha tratada: {cols_alvo_tratada}")

except Exception as e:
    logging.critical(f"Erro ao processar df_tratada_existente ou definir schema alvo no Item 8: {e}", exc_info=True)
    raise

# ----------------------------------------------------------
# IDENTIFICA E PREPARA NOVOS PROTOCOLOS PARA ENVIO
# ----------------------------------------------------------

try:
    novos_protocolos_a_enviar = set(df_bruta["protocolo"]) - protocolos_existentes_set_final
    df_send_bruto = df_bruta[df_bruta["protocolo"].isin(novos_protocolos_a_enviar)].copy()

    if df_send_bruto.empty:
        logging.info("Nenhum protocolo novo detectado para envio. df_send ser√° um DataFrame vazio.")
        print("üßπ Nenhum protocolo novo para enviar.")
        df_send = pd.DataFrame(columns=cols_alvo_tratada) # Define df_send vazio com colunas corretas
    else:
        logging.info(f"Detectados {len(df_send_bruto)} protocolos novos para processar e enviar. Shape inicial: {df_send_bruto.shape}")
        print(f"üßπ Novos protocolos a enviar: {len(df_send_bruto)}")

        # APLICA TODOS OS TRATAMENTOS DE _tratar_full AQUI!
        df_send = _tratar_full(df_send_bruto.copy())
        logging.info(f"Fun√ß√£o _tratar_full aplicada a df_send_bruto. Shape ap√≥s tratamento: {df_send.shape}")

        # Remove colunas auxiliares que n√£o devem ser escritas no Google Sheets
        cols_to_drop = []
        if "eh_novo" in df_send.columns:
            cols_to_drop.append("eh_novo")
        # Adicione aqui outras colunas auxiliares
        # if "alguma_coluna_temp" in df_send.columns: cols_to_drop.append("alguma_coluna_temp")

        if cols_to_drop:
            df_send = df_send.drop(columns=cols_to_drop)
            logging.info(f"Colunas auxiliares removidas de df_send: {cols_to_drop}. Novo shape: {df_send.shape}")

        # Garante que o df_send tem as colunas corretas e na ordem certa
        df_send_final = df_send.reindex(columns=cols_alvo_tratada, fill_value="")
        logging.info(f"df_send reindexado para alinhar com colunas alvo. Shape final: {df_send_final.shape}")

        # QA: Verifica se alguma coluna do df_send_final cont√©m valores inesperados antes do envio
        for col_qa in ['orgaos', 'responsavel', 'status_demanda', 'data_da_conclusao']:
            if col_qa in df_send_final.columns:
                unexpected_values = df_send_final[col_qa].astype(str).str.contains(r'(?i)^(sim|nao|true|false|\?{2,}|nan|none)$')
                if unexpected_values.any():
                    logging.error(f"QA Pr√©-Envio (df_send): Coluna '{col_qa}' cont√©m valores inesperados em {unexpected_values.sum()} linhas. Exemplos: {df_send_final.loc[unexpected_values, col_qa].unique()[:5].tolist()}",
                                  extra={'data': df_send_final.loc[unexpected_values, ['protocolo', col_qa]].to_dict(orient='records')[:5]})
                    # Considerar um raise SystemExit aqui se a qualidade do dado for cr√≠tica

        # Garante que todas as colunas sejam strings para evitar problemas de tipo no GSpread
        for col in df_send_final.columns:
            df_send_final[col] = df_send_final[col].astype(str)
        logging.info("Todas as colunas de df_send_final convertidas para string.")

        df_send = df_send_final.copy() # Atribui o DataFrame final preparado para df_send

except Exception as e:
    logging.critical(f"Erro na prepara√ß√£o final de df_send no Item 8: {e}", exc_info=True)
    raise

# ----------------------------------------------------------
# TRATAMENTO CR√çTICO ‚Äî DATA DA CONCLUS√ÉO (AP√ìS _tratar_full)
# e PADRONIZA OUTRAS DATAS
#
# Com a aplica√ß√£o de _tratar_full acima, estas fun√ß√µes devem ser menos necess√°rias.
# Elas s√£o mantidas como um √∫ltimo ajuste de formato para DD/MM/YYYY se _tratar_full
# retornar DD/MM/YY e o GSheet esperar o ano com 4 d√≠gitos.
# ----------------------------------------------------------
def tratar_data_conclusao_item8(x):
    if pd.isna(x) or str(x).strip().lower() in ["", "nan", "na", "n/a", "none", "n√£o conclu√≠do"]:
        return "N√£o conclu√≠do"
    try:
        dt = pd.to_datetime(x, errors="coerce", dayfirst=True)
        if pd.notna(dt):
            return dt.strftime("%d/%m/%Y")
        else:
            return "N√£o conclu√≠do"
    except Exception:
        return "N√£o conclu√≠do"

if not df_send.empty and "data_da_conclusao" in df_send.columns:
    df_send["data_da_conclusao"] = df_send["data_da_conclusao"].apply(tratar_data_conclusao_item8)
    logging.debug("Re-aplicado tratamento de 'data_da_conclusao' para garantir formato DD/MM/YYYY.")

def tratar_data_generica_item8(x):
    try:
        if pd.isna(x) or str(x).strip().lower() in ["", "nan", "na", "n/a"]:
            return ""
        dt = pd.to_datetime(x, errors="coerce", dayfirst=True)
        if pd.notna(dt):
            return dt.strftime("%d/%m/%Y")
        else:
            return ""
    except Exception:
        return ""

for col in ["data_da_criacao"]:
    if not df_send.empty and col in df_send.columns:
        df_send[col] = df_send[col].apply(tratar_data_generica_item8)
        logging.debug(f"Re-aplicado tratamento de '{col}' para garantir formato DD/MM/YYYY.")

# ----------------------------------------------------------
# CHECAGEM DE SANIDADE ‚Äî UNIDADE_CADASTRO (em df_send j√° tratado)
# ----------------------------------------------------------
if not df_send.empty and "unidade_cadastro" in df_send.columns:
    nulos_uc = int(df_send["unidade_cadastro"].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']).sum())
    print(f"üß™ Checagem (NOVOS - PRONTOS PARA ENVIO): unidade_cadastro presente | vazios={nulos_uc}")
    logging.info(f"Checagem (NOVOS - PRONTOS PARA ENVIO): unidade_cadastro presente | vazios={nulos_uc}")
    if nulos_uc > 0:
        logging.warning(f"QA Pr√©-Envio: 'unidade_cadastro' cont√©m {nulos_uc} valores vazios/inv√°lidos em df_send. Exemplos: {df_send.loc[df_send['unidade_cadastro'].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']), 'unidade_cadastro'].unique()[:5].tolist()}")
else:
    print("‚ö†Ô∏è Aviso: unidade_cadastro n√£o est√° em df_send ou df_send est√° vazio.")
    logging.warning("unidade_cadastro n√£o est√° em df_send ou df_send est√° vazio. Verifique a consist√™ncia do schema.")

# ----------------------------------------------------------
# ENVIO EM LOTES
# ----------------------------------------------------------
if df_send.empty:
    logging.info("Nenhum protocolo para enviar, pulando envio em lotes.")
    print("üì¶ Nenhum protocolo para enviar.")
else:
    lote = 500
    total_lotes = (len(df_send) + lote - 1) // lote
    print(f"üì¶ Envio ‚Äî APENAS NOVOS (FINAL): {len(df_send)} linhas | {total_lotes} lotes")
    logging.info(f"Envio ‚Äî APENAS NOVOS (FINAL): {len(df_send)} linhas | {total_lotes} lotes")

    existing_values = aba_tratada.get_all_values()
    sheet_is_empty = len(existing_values) == 0

    for i in range(0, len(df_send), lote):
        chunk = df_send.iloc[i:i+lote].copy()
        rows = chunk.values.tolist()
        first_idx = i + 1
        last_idx = min(i + lote, len(df_send))
        protos_preview = list(chunk.get("protocolo", []))[:3]
        logging.debug(f"Processando lote {first_idx}-{last_idx}. Pr√©via protocolos: {protos_preview}")
        print(f"   ‚Ä¢ Enviando {first_idx}-{last_idx} (pr√©via protocolos: {protos_preview})")

        try:
            if sheet_is_empty:
                header = chunk.columns.tolist()
                aba_tratada.append_rows([header] + rows)
                logging.info(f"Lote {first_idx}-{last_idx} enviado com cabe√ßalho.")
                sheet_is_empty = False
            else:
                aba_tratada.append_rows(rows)
                logging.info(f"Lote {first_idx}-{last_idx} enviado (sem cabe√ßalho).")
        except Exception as e:
            logging.exception(f"Erro CR√çTICO ao enviar lote {first_idx}-{last_idx}: {e}")
            print(f"‚ùå Erro ao enviar lote {first_idx}-{last_idx}: {e}")
            failed = chunk[["protocolo"]].copy()
            timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            failed.to_csv(f"failed_append_{first_idx}_{last_idx}_{timestamp}.csv", index=False, encoding="utf-8-sig")
            # Dependendo da severidade, voc√™ pode querer parar o pipeline aqui.

print("‚úÖ Atualiza√ß√£o da planilha tratada conclu√≠da com sucesso.")
logging.info("‚úÖ Atualiza√ß√£o da planilha tratada conclu√≠da com sucesso.")

# ========================================================
# 9) PATCH / ATUALIZA√á√ÉO DE STATUS E DELTA HIST√ìRICO (CORRIGIDO)
# ========================================================
_BANNER("9) PATCH / ATUALIZA√á√ÉO DE STATUS E DELTA HIST√ìRICO (CORRIGIDO)")

import time

# --------------------------------------------------------
# üîß Helper principal de padroniza√ß√£o de status e datas
# --------------------------------------------------------
def _prepare_status(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    # 1Ô∏è‚É£ Padroniza status_demanda
    if "status_demanda" in df.columns:
        df["status_demanda"] = df["status_demanda"].apply(
            lambda v: "CONCLU√çDA" if _is_concluida(v)
            else "EM ANDAMENTO" if (v and str(v).strip() != "")
            else v
        )

    # 2Ô∏è‚É£ Padroniza prazo_restante
    if "prazo_restante" in df.columns:
        df["prazo_restante"] = df["prazo_restante"].apply(_canon_prazo_restante)

    # 3Ô∏è‚É£ Padroniza data_da_conclusao ‚Äî tratamento definitivo
    if "data_da_conclusao" in df.columns:
        def _tratar_data_conclusao(x):
            """Converte datas v√°lidas e substitui inv√°lidas/vazias por 'N√£o conclu√≠do'"""
            if pd.isna(x) or str(x).strip().lower() in ["", "nan", "na", "n/a", "none"]:
                return "N√£o conclu√≠do"
            try:
                dt = pd.to_datetime(x, errors="coerce")
                if pd.isna(dt):
                    return "N√£o conclu√≠do"
                return dt.strftime("%d/%m/%Y")
            except Exception:
                return "N√£o conclu√≠do"

        df["data_da_conclusao"] = df["data_da_conclusao"].apply(_tratar_data_conclusao)

    # 4Ô∏è‚É£ Limpeza de "N√£o h√° dados"
    for col in ["tempo_de_resolucao_em_dias"]:
        if col in df.columns:
            df[col] = df[col].replace("N√£o h√° dados", "")

    return df


# --------------------------------------------------------
# Fallback de envio de lotes (log)
# --------------------------------------------------------
def _post_lotes(df: pd.DataFrame, msg: str, cols: list):
    logging.warning(f"Fallback acionado ({msg}) para {len(df)} linhas. Colunas: {cols}")

# Antes de criar/usar os deltas, aplique o prepare em todo df (ou pelo menos nos deltas)
df = _prepare_status(df)  # garante que coluna principal esteja padronizada

# Em seguida crie os deltas com base no df j√° padronizado:
delta_status = df[df["status_demanda"] != df.get("status_demanda_OLD", df["status_demanda"])]
delta_conc   = df[df["data_da_conclusao"] != df.get("data_da_conclusao_OLD", df["data_da_conclusao"])]
delta_tempo  = df[df["tempo_de_resolucao_em_dias"] != df.get("tempo_de_resolucao_em_dias_OLD", df["tempo_de_resolucao_em_dias"])]

# --------------------------------------------------------
# PATCH principal ‚Äî atualiza√ß√£o de c√©lulas no Google Sheets
# --------------------------------------------------------
def _patch_grouped_force(df: pd.DataFrame, key_col: str, value_col: str, sheet=None):
    if df.empty:
        logging.info("Delta vazio. Nada a atualizar.")
        return

    batch_size = 50

    # Aplica padroniza√ß√µes completas
    df = _prepare_status(df)
    df[key_col] = df[key_col].astype(str).str.strip()
    df[value_col] = df[value_col].astype(str).str.strip()

    # üîí Corrige 'data_da_conclusao' p√≥s-stringifica√ß√£o
    if value_col == "data_da_conclusao":
        df[value_col] = df[value_col].apply(
            lambda x: "N√£o conclu√≠do" if str(x).strip().lower() in ["", "nan", "na", "n/a", "none", "nat"] else x
        )

    if sheet is None:
        _post_lotes(df, f"{value_col} (fallback)", [key_col, value_col])
        return

    # Tenta localizar colunas no Sheets
    try:
        key_list = sheet.col_values(1)
    except Exception as e:
        logging.error(f"Erro ao obter protocolos da planilha: {e}")
        _post_lotes(df, f"{value_col} (fallback)", [key_col, value_col])
        return

    try:
        col_idx = sheet.find(value_col).col
    except Exception:
        logging.warning(f"Coluna '{value_col}' n√£o encontrada. Usando coluna 2 como fallback.")
        col_idx = 2

    # --- PREPARA LISTA DE ATUALIZA√á√ÉO ---
    to_update = []
    for _, row in df.iterrows():
        key = row[key_col]
        value = row.get(f"{value_col}_trat", row[value_col])
        if key in key_list:
            row_idx = key_list.index(key) + 1
            to_update.append((row_idx, col_idx, value))
        else:
            logging.warning(f"Protocolo '{key}' n√£o encontrado na planilha.")

    # --- ATUALIZA√á√ÉO EM BATCH ---
    total_updated = 0
    for i in range(0, len(to_update), batch_size):
        batch = to_update[i:i + batch_size]
        if not batch:
            continue

        cleaned_batch = []
        for r in batch:
            value = r[2]
            if pd.isna(value) or str(value).strip() == "":
                value = "N√£o conclu√≠do" if value_col == "data_da_conclusao" else ""
            value = str(value).strip()
            cleaned_batch.append((r[0], r[1], value))

        range_rows = [r[0] for r in cleaned_batch]
        values = [[r[2]] for r in cleaned_batch]

        try:
            import gspread.utils
            start_row = min(range_rows)
            end_row = max(range_rows)
            range_str = (
                f"{gspread.utils.rowcol_to_a1(start_row, col_idx)}:"
                f"{gspread.utils.rowcol_to_a1(end_row, col_idx)}"
            )
            sheet.update(range_str, values)
            total_updated += len(cleaned_batch)
        except Exception as e:
            logging.error(f"Erro no batch update: {e}")
            _post_lotes(
                pd.DataFrame([(r[0], r[2]) for r in cleaned_batch], columns=[key_col, value_col]),
                f"{value_col} (fallback)",
                [key_col, value_col],
            )

    logging.info(f"‚úÖ Status atualizado em batch: {total_updated}/{len(df)} linhas.")
    if total_updated == 0:
        logging.info("Nenhuma linha atualizada diretamente. Enviando via _post_lotes como fallback.")
        _post_lotes(df, f"{value_col} (fallback)", [key_col, value_col])


# ========================================================
# 9.1) Delta TEMPO_RESOLUCAO local (0 ‚Üí 1)
# ========================================================
if "tempo_de_resolucao_em_dias" in df.columns:
    s_local = df["tempo_de_resolucao_em_dias"].astype("object")

    def _fix_zero_keep_text(v):
        if pd.isna(v):
            return v
        sv = str(v).strip()
        try:
            num = pd.to_numeric(sv, errors="coerce")
            if pd.notna(num) and float(num) == 0.0:
                return "1"
        except Exception:
            pass
        return sv

    df["tempo_de_resolucao_em_dias"] = s_local.map(_fix_zero_keep_text)


# ========================================================
# 9.2) Corre√ß√£o pontual servidor (Raphael)
# ========================================================
if "protocolo" in df.columns and "servidor" in df.columns:
    protos_raphael = (
        df.loc[df["servidor"].astype(str).str.strip() == "Raphael Pereira de Mello", "protocolo"]
          .astype(str).str.strip().unique().tolist()
    )
    if protos_raphael:
        df_fix = pd.DataFrame({
            "protocolo": protos_raphael,
            "servidor": "Raphael Pereira de Mello"
        })
        _patch_grouped_force(df_fix, "protocolo", "servidor", aba_tratada)
    else:
        logging.info("Nenhum protocolo local com 'Raphael Pereira de Mello' encontrado.")

# ========================================================
# 10) DELTAS HIST√ìRICOS (status_demanda, data_da_conclusao, tempo_de_resolucao_em_dias)
# ========================================================

_BANNER("10) DELTAS HIST√ìRICOS (ajustado para novos protocolos)")

# --- Alinha colunas do df_send ao schema da tratada ---
try:
    cols_tratada = list(df_tratada.columns)
    # df_send pode n√£o existir (caso nenhum novo); garante vari√°vel
    if 'df_send' not in globals():
        df_send = pd.DataFrame(columns=cols_tratada)
    df_send_aligned = df_send.reindex(columns=cols_tratada, fill_value="")
    df_full = pd.concat([df_tratada, df_send_aligned], ignore_index=True, sort=False)
except Exception as e:
    logging.warning(f"Falha ao concatenar bases tratada + novos: {e}")
    # fallback simples: tenta usar df_send como fonte
    df_full = df_send.copy()

# --- Garante exist√™ncia das colunas *_OLD para compara√ß√µes de hist√≥rico ---
for col in ["status_demanda", "data_da_conclusao", "tempo_de_resolucao_em_dias"]:
    old_col = f"{col}_OLD"
    if old_col not in df_full.columns:
        # copia o valor atual para coluna OLD (se n√£o existir), normalizando nulos
        df_full[old_col] = df_full.get(col, "").fillna("")

# --- Fun√ß√£o delta robusta (com fillna e casting a str) ---
def _delta_df(df_full_local: pd.DataFrame, col: str) -> pd.DataFrame:
    old_col = f"{col}_OLD"
    if old_col in df_full_local.columns:
        left = df_full_local.get(col, "").fillna("").astype(str)
        right = df_full_local.get(old_col, "").fillna("").astype(str)
        return df_full_local[left != right].copy()
    else:
        # se n√£o h√° coluna OLD, considera apenas os rec√©m marcados como 'eh_novo'
        return df_full_local[df_full_local.get("eh_novo", False) == True].copy()

# --- Calcula deltas espec√≠ficos (apenas uma vez e sem sobrescritas) ---
delta_status = _delta_df(df_full, "status_demanda")
delta_conc   = _delta_df(df_full, "data_da_conclusao")
delta_tempo  = _delta_df(df_full, "tempo_de_resolucao_em_dias")

# --- Logs e verifica√ß√µes ---
logging.info(f"Delta STATUS: {len(delta_status)} linhas alteradas/novas")
logging.info(f"Delta DATA_CONCLUSAO: {len(delta_conc)} linhas alteradas/novas")
logging.info(f"Delta TEMPO_DE_RESOLUCAO: {len(delta_tempo)} linhas alteradas/novas")

print(f"üìä Deltas calculados com sucesso:")
print(f"   ‚Ä¢ STATUS: {len(delta_status)}")
print(f"   ‚Ä¢ DATA_CONCLUSAO: {len(delta_conc)}")
print(f"   ‚Ä¢ TEMPO_DE_RESOLUCAO: {len(delta_tempo)}")

# ========================================================
# 11) QA & SUM√ÅRIO FINAL
# ========================================================
_BANNER("11) QA & SUM√ÅRIO FINAL")

# --- Protocolos novos / existentes ---
if "eh_novo" in df.columns:
    novos_protos = df.loc[df["eh_novo"] == True, "protocolo"].astype(str).str.strip()
    print(f"üîπ Protocolos novos: {len(novos_protos)}")
    logging.info(f"Protocolos novos: {len(novos_protos)}")
else:
    novos_protos = pd.Series([], dtype=str)
    logging.info("Coluna 'eh_novo' ausente ‚Äî nenhum protocolo novo identificado.")

# --- QA de colunas cr√≠ticas ---
qa_cols = ["status_demanda", "data_da_conclusao", "tempo_de_resolucao_em_dias"]
for col in qa_cols:
    if col in df.columns:
        nulos = df[col].isna().sum()
        print(f"üîπ {col} vazio: {nulos} linhas")
        logging.info(f"QA: {col} vazio: {nulos} linhas")
    else:
        logging.info(f"Coluna '{col}' ausente.")

# --- Sum√°rio final ---
print("‚úÖ QA conclu√≠do. Sum√°rio final:")
logging.info("QA conclu√≠do. Sum√°rio final:")
print(f"  ‚Ä¢ Protocolos totais na planilha: {len(df)}")
print(f"  ‚Ä¢ Protocolos novos identificados: {len(novos_protos)}")
for col in qa_cols:
    nulos = df[col].isna().sum() if col in df.columns else 0
    print(f"  ‚Ä¢ {col} vazio: {nulos}")

# ========================================================
# 12) FINALIZA√á√ÉO
# ========================================================

# --- Sanity checks finais (colocar antes do _BANNER("12) PIPELINE FINALIZADO")) ---
try:
    bruta_rows = len(df_bruta) if 'df_bruta' in globals() and isinstance(df_bruta, pd.DataFrame) else 0
    bruta_cols = list(df_bruta.columns)[:20] if 'df_bruta' in globals() and isinstance(df_bruta, pd.DataFrame) and not df_bruta.empty else []
except Exception:
    bruta_rows, bruta_cols = 0, []

try:
    tratada_rows = len(df_tratada) if 'df_tratada' in globals() and isinstance(df_tratada, pd.DataFrame) else 0
    tratada_cols = list(df_tratada.columns)[:20] if 'df_tratada' in globals() and isinstance(df_tratada, pd.DataFrame) and not df_tratada.empty else []
except Exception:
    tratada_rows, tratada_cols = 0, []

novos_cnt = int(df_bruta['eh_novo'].sum()) if 'df_bruta' in globals() and 'eh_novo' in df_bruta.columns else 0
df_send_cnt = len(df_send) if 'df_send' in globals() and isinstance(df_send, pd.DataFrame) else 0

logging.info(f"Sanity: df_bruta rows={bruta_rows} cols={bruta_cols}")
logging.info(f"Sanity: df_tratada rows={tratada_rows} cols={tratada_cols}")
logging.info(f"Sanity: novos detectados={novos_cnt} | df_send (preparados para envio)={df_send_cnt}")
print(f"Sanity checks ‚Äî bruta:{bruta_rows} rows, tratada:{tratada_rows} rows, novos:{novos_cnt}, to_send:{df_send_cnt}")

# opcional: numero real de linhas na sheet (pode ser custoso em tempo)
try:
    if 'aba_tratada' in globals():
        total_sheet_rows = len(aba_tratada.get_all_values())
        logging.info(f"Sanity: aba_tratada (sheet) rows={total_sheet_rows}")
except Exception as e:
    logging.warning(f"N√£o foi poss√≠vel obter aba_tratada.get_all_values(): {e}")

_BANNER("12) PIPELINE FINALIZADO")

print("üéØ Pipeline executado com sucesso!")
logging.info("Pipeline executado com sucesso")

print("Fluxo: GoogleSheets (bruto) ‚Üí Python (tratamento) ‚Üí GoogleSheets (tratado) ‚Üí LookerStudio")

# --- Resumo de atualiza√ß√µes completas ---
for col in qa_cols:
    if col in df.columns:
        atualizadas = len(df[df[col].notna()])
        print(f"  ‚Ä¢ {col} atualizadas: {atualizadas} linhas")
        logging.info(f"{col} atualizadas: {atualizadas} linhas")
