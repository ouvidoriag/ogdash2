# ----------------------------
# Logging (adicionado, n√£o remove prints)
# ----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler("pipeline_tratamento.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# --------------------------------------------------------
# Utilit√°rio de banner de se√ß√£o (para logs/prints)
# --------------------------------------------------------
def _BANNER(titulo):
    print("\n" + "="*18 + f" {titulo} " + "="*18)
    logging.info(titulo)

def _SUB(titulo):
    print("‚Äî " + titulo)
    logging.info(titulo)

# ========================================================
# 1) CONFIGURA√á√ÉO GOOGLE DRIVE / SHEETS
# ========================================================
_BANNER("1) CONFIGURA√á√ÉO GOOGLE DRIVE/SHEETS")

CAMINHO_CREDENCIAIS = "/home/niltonjunio/meuprojeto/ouvidoria-tratamento-dados.json"

# Pasta da planilha bruta (entrada)
PASTA_BRUTA_ID = "1qXj9eGauvOREKVgRPOfKjRlLSKhefXI5"
# Pasta da planilha tratada (sa√≠da)
PASTA_TRATADA_ID = "10mW1LPrjsGRPYSWLMAF7tKgQbufLgDie"
# Nome da planilha tratada (mesmo nome da bruta)
NOME_PLANILHA_TRATADA = "Dashboard_Duque_de_Caxias_Ouvidoria_Duque_de_Caxias_Tabela"

SCOPES = [
    "https://www.googleapis.com/auth/drive",
    "https://www.googleapis.com/auth/spreadsheets"
]

# Autentica√ß√£o - tentarei falhar claramente se n√£o encontrar credenciais
try:
    creds = Credentials.from_service_account_file(CAMINHO_CREDENCIAIS, scopes=SCOPES)
    drive_service = build("drive", "v3", credentials=creds)
    gc = gspread.authorize(creds)
    print("‚úÖ Autentica√ß√£o Google OK.")
    logging.info("Autentica√ß√£o Google OK")
except Exception as e:
    logging.exception("Falha na autentica√ß√£o Google. Verifique CAMINHO_CREDENCIAIS.")
    raise

# ========================================================
# 2) LEITURA ‚Äî √öLTIMA PLANILHA BRUTA DO DRIVE
# ========================================================
_BANNER("2) LEITURA DA PLANILHA BRUTA (GOOGLE DRIVE)")

query = f"'{PASTA_BRUTA_ID}' in parents and mimeType='application/vnd.google-apps.spreadsheet'"

try:
    arquivos = drive_service.files().list(
        q=query, spaces="drive",
        fields="files(id, name, createdTime)",
        orderBy="createdTime desc", pageSize=1
    ).execute().get("files", [])
except Exception as e:
    logging.exception("Erro ao listar arquivos no Drive.")
    raise

if not arquivos:
    raise FileNotFoundError(f"Nenhuma planilha na pasta {PASTA_BRUTA_ID}.")
sheet_id = arquivos[0]["id"]
print(f"üìÇ √öltima planilha encontrada: {arquivos[0]['name']}")
logging.info(f"√öltima planilha encontrada: {arquivos[0]['name']} ({sheet_id})")

# Abrir com gspread (mant√©m seu fluxo original)
try:
    sh = gc.open_by_key(sheet_id)
    # usa sheet1 como no seu script original
    df = pd.DataFrame(sh.sheet1.get_all_records())
    print(f"‚úÖ Planilha bruta importada com sucesso: {df.shape}")
    logging.info(f"Planilha bruta importada com sucesso: {df.shape}")
except Exception as e:
    logging.exception("Erro ao abrir planilha por key com gspread.")
    raise

# ========================================================
# 3) NORMALIZA√á√ÉO DE NOMES DE COLUNA
# ========================================================
_BANNER("3) NORMALIZA√á√ÉO DE NOMES DE COLUNA")

def normalizar_nome_coluna(col: str) -> str:
    # fun√ß√£o original preservada, com levemente mais segura contra None
    if col is None:
        return ""
    col = unicodedata.normalize("NFKD", str(col)).encode("ASCII", "ignore").decode("utf-8")
    col = col.lower()
    col = re.sub(r"[^a-z0-9]+", "_", col)
    return re.sub(r"_+", "_", col).strip("_")

# aplica conforme original
df.columns = [normalizar_nome_coluna(c) for c in df.columns]
print("‚úÖ Cabe√ßalhos normalizados:", list(df.columns))
logging.info(f"Cabe√ßalhos normalizados: {list(df.columns)}")

# ========================================================
# 4) FUN√á√ïES AUXILIARES (codifica√ß√£o / datas / post em lotes)
# ========================================================
_BANNER("5) AUXILIARES (codifica√ß√£o, datas, lotes)")

# --- _canon_txt (mantido integralmente) ---
def _canon_txt(x):
    if x is None:
        return ""
    s = str(x)
    if s == "":
        return s
    s = s.replace("\u00A0", " ").replace("&nbsp;", " ")
    s = re.sub(r"[\u2000-\u200A\u202F\u205F\u3000]", " ", s)
    s = re.sub(r"[\u200B-\u200D\u2060\uFEFF]", "", s)

    def _try_fix(t, enc):
        try:    return t.encode(enc).decode("utf-8")
        except: return t

    if ("√É" in s) or ("√Ç" in s) or ("ÔøΩ" in s):
        cand = max([s, _try_fix(s, "latin-1"), _try_fix(s, "cp1252")],
                   key=lambda txt: (-(txt.count("√É")+txt.count("√Ç")+txt.count("ÔøΩ")),
                                    sum(ch in "√°√©√≠√≥√∫√¢√™√¥√£√µ√†√ß√Å√â√ç√ì√ö√Ç√ä√î√É√ï√Ä√á" for ch in txt)))
        s = cand

    s = re.sub(r"Sa\?\?de", "Sa√∫de", s, flags=re.IGNORECASE)
    s = re.sub(r"Sa[\ufffdÔøΩ]de", "Sa√∫de", s, flags=re.IGNORECASE)

    s = unicodedata.normalize("NFC", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# --- _canon_responsavel_series (mantido) ---
def _canon_responsavel_series(series: pd.Series) -> pd.Series:
    base = pd.Series(series, dtype="object").apply(_canon_txt)
    patt_ouvidoria_saude = r"(?i)^ouvidoria setorial da sa(?:u|√É¬∫|\\u00fa|\?\?|[\ufffdÔøΩ])?de$"
    return base.str.strip().replace({
        patt_ouvidoria_saude: "Ouvidoria Setorial da Sa√∫de",
        r"(?i)^cidad(?:\u00e3|√£)o$": "Cidad√£o",
    }, regex=True)

# --- _to_ddmmaa_text (mantido) ---
def _to_ddmmaa_text(series: pd.Series) -> pd.Series:
    EXCEL_BASE = pd.Timestamp("1899-12-30")
    def _one(v):
        if pd.isna(v):
            return None
        if isinstance(v, (pd.Timestamp, np.datetime64)):
            dt = pd.to_datetime(v, errors="coerce")
            return dt.strftime("%d/%m/%y") if pd.notna(dt) else None
        s = str(v).strip()
        if s == "":
            return None
        # normaliza separadores e sufixos
        s2 = s.replace("T", " ").replace("Z", "")
        s2 = re.sub(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$", "", s2).strip()

        # 1) ISO (YYYY-MM-DD...) -> for√ßa dayfirst=False (cobre milissegundos)
        if re.match(r"^\d{4}-\d{2}-\d{2}", s2):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                dt = pd.to_datetime(s2, errors="coerce", dayfirst=False)
            return dt.strftime("%d/%m/%y") if pd.notna(dt) else s

        # 2) Excel serial
        if re.fullmatch(r"\d{5,6}(\.\d+)?", s2):
            try:
                return (EXCEL_BASE + pd.to_timedelta(float(s2), "D")).strftime("%d/%m/%y")
            except:
                pass

        # 3) epoch ms
        if re.fullmatch(r"\d{13}", s2):
            dt = pd.to_datetime(int(s2), unit="ms", errors="coerce")
            return dt.strftime("%d/%m/%y") if pd.notna(dt) else s

        # 4) epoch s
        if re.fullmatch(r"\d{10}(\.\d+)?", s2):
            dt = pd.to_datetime(float(s2), unit="s", errors="coerce")
            return dt.strftime("%d/%m/%y") if pd.notna(dt) else s

        # 5) formatos expl√≠citos (mantidos)
        for fmt in [
            "%d/%m/%Y %H:%M:%S", "%d/%m/%Y %H:%M", "%d/%m/%Y",
            "%d/%m/%y %H:%M:%S", "%d/%m/%y %H:%M", "%d/%m/%y",
            "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d"
        ]:
            try:
                return pd.to_datetime(s2, format=fmt).strftime("%d/%m/%y")
            except:
                pass

        # 6) fallback BR (dia primeiro)
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            dt = pd.to_datetime(s2, dayfirst=True, errors="coerce")
        return dt.strftime("%d/%m/%y") if pd.notna(dt) else s
    return series.apply(_one).astype("object")

# --- _conclusao_strict (mantido) ---
def _conclusao_strict(series: pd.Series) -> pd.Series:
    s = pd.Series(series, dtype="object").astype(str).str.strip()
    s_cf = s.str.casefold()
    invalid = {
        "n√£o informado","na","n/a","n\\a","nan","null","none","","-","--",
        "outro","outros","nat","sem informa√ß√£o","sem informacao"
    }
    out = s.copy()
    mask_invalid = s_cf.isin(invalid)
    out.loc[mask_invalid] = "N√£o conclu√≠do"

    rest_idx = out.index[~mask_invalid]
    if len(rest_idx) > 0:
        s_rest = s.loc[rest_idx]
        # normaliza separadores/sufixos para checar prefixo ISO
        s_norm = s_rest.str.replace("T", " ").str.replace("Z", "", regex=False)
        s_norm = s_norm.str.replace(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$", "", regex=True).str.strip()
        iso_mask = s_norm.str.match(r"^\d{4}-\d{2}-\d{2}")

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            dt = pd.Series(pd.NaT, index=rest_idx, dtype="datetime64[ns]")
            iso_idx = iso_mask[iso_mask].index
            if len(iso_idx) > 0:
                dt.loc[iso_idx] = pd.to_datetime(s_rest.loc[iso_idx], errors="coerce", dayfirst=False)
            non_iso_idx = iso_mask[~iso_mask].index
            if len(non_iso_idx) > 0:
                dt.loc[non_iso_idx] = pd.to_datetime(s_rest.loc[non_iso_idx], errors="coerce", dayfirst=True)

        good_idx = dt.index[dt.notna()]
        if len(good_idx) > 0:
            out.loc[good_idx] = dt.loc[good_idx].dt.strftime("%d/%m/%y")

    return out

# --- _parse_dt_cmp (mantido) ---
def _parse_dt_cmp(series: pd.Series) -> pd.Series:
    EXCEL_BASE = pd.Timestamp("1899-12-30")
    def _one(v):
        if pd.isna(v): return pd.NaT
        s = str(v).strip()
        if s == "": return pd.NaT
        s2 = s.replace("T"," ").replace("Z","")
        s2 = re.sub(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$","",s2).strip()
        if re.fullmatch(r"\d{5,6}(\.\d+)?", s2):
            try: return EXCEL_BASE + pd.to_timedelta(float(s2), "D")
            except: return pd.NaT
        if re.fullmatch(r"\d{13}", s2):
            return pd.to_datetime(int(s2), unit="ms", errors="coerce")
        if re.fullmatch(r"\d{10}(\.\d+)?", s2):
            return pd.to_datetime(float(s2), unit="s", errors="coerce")
        for fmt in ["%d/%m/%Y %H:%M:%S","%d/%m/%Y %H:%M","%d/%m/%Y",
                    "%d/%m/%y %H:%M:%S","%d/%m/%y %H:%M","%d/%m/%y",
                    "%Y-%m-%d %H:%M:%S","%Y-%m-%d %H:%M","%Y-%m-%d"]:
            try: return pd.to_datetime(s2, format=fmt)
            except: pass
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            return pd.to_datetime(s2, dayfirst=True, errors="coerce")
    return series.apply(_one)

# <<< NOVO HELPER: detectar "N√£o h√° dados" >>>
def _is_nao_ha_dados(v) -> bool:
    if v is None:
        return False
    s = str(v).strip()
    if s == "":
        return False
    # remove acentos e normaliza
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = re.sub(r"\s+", " ", s).strip().casefold()
    return s == "nao ha dados"
# >>> FIM NOVO HELPER

# <<< PATCH: helper p/ detectar 'Conclu√≠da'
def _is_concluida(v) -> bool:
    if pd.isna(v):
        return False
    s = str(v).strip()
    if s == "":
        return False
    s = "".join(ch for ch in unicodedata.normalize("NFD", s) if unicodedata.category(ch) != "Mn")
    s = re.sub(r"[^A-Za-z]+", " ", s).strip().casefold()
    return s == "concluida"
# >>> PATCH

# Helpers p/ 'Demanda Conclu√≠da'
def _looks_like_demanda_concluida(v) -> bool:
    if pd.isna(v):
        return False
    s = str(v).strip()
    if s == "":
        return False
    s = "".join(ch for ch in unicodedata.normalize("NFD", s) if unicodedata.category(ch) != "Mn")
    s = s.replace("ÔøΩ", "i").replace("?", "i")
    s = re.sub(r"i{2,}", "i", s, flags=re.IGNORECASE)
    s = re.sub(r"[^A-Za-z]+", " ", s).strip().casefold()
    return s == "demanda concluida"

def _canon_prazo_restante(v):
    if pd.isna(v):
        return v
    if isinstance(v, (int, float)) and not pd.isna(v):
        return v
    s = _canon_txt(v)
    if s == "":
        return s
    s_clean = s.strip()
    if _looks_like_demanda_concluida(s_clean):
        return "Demanda Conclu√≠da"
    return s_clean

def _post_lotes(df_send: pd.DataFrame, titulo: str, cols_allowed, prefer: str = "return=minimal", lote: int = 500):
    if df_send.empty:
        print(f"üì¶ {titulo}: 0 linhas (skip).")
        logging.info(f"{titulo}: 0 linhas (skip).")
        return
    df_send = df_send[[c for c in df_send.columns if c in cols_allowed]].copy()
    if "protocolo" in df_send.columns:
        df_send["protocolo"] = df_send["protocolo"].astype(str).str.strip()
    total_lotes = (len(df_send) + lote - 1) // lote
    print(f"üì¶ {titulo}: {len(df_send)} linhas | {total_lotes} lotes")
    logging.info(f"{titulo}: {len(df_send)} linhas | {total_lotes} lotes")
    for i in range(0, len(df_send), lote):
        chunk = df_send.iloc[i:i+lote].replace({np.nan: None})
        payload = chunk.to_dict(orient="records")
        data_utf8 = json.dumps(payload, ensure_ascii=False)
        first_idx = i + 1
        last_idx  = min(i + lote, len(df_send))
        protos_preview = list(chunk.get("protocolo", []))[:3]
        print(f"   ‚Ä¢ Enviando {first_idx}-{last_idx} (pr√©via protocolos: {protos_preview})")
        logging.info(f"Enviando {first_idx}-{last_idx} (pr√©via protocolos: {protos_preview})")
        # requests POST com timeout e tratamento de exce√ß√£o
        try:
            r = requests.post(url_upsert, headers={**headers, "Prefer": prefer}, data=data_utf8.encode("utf-8"), timeout=30)
        except requests.RequestException as e:
            logging.exception(f"Erro ao enviar lote {first_idx}-{last_idx}")
            print(f"       Resposta: Erro na requisi√ß√£o: {e}")
            continue
        print(f"     ‚Üí Lote {i//lote + 1}/{total_lotes} | status {r.status_code}")
        logging.info(f"Lote {i//lote + 1}/{total_lotes} status {r.status_code}")
        if r.status_code not in (200, 201):
            print("       Resposta:", (r.text or "")[:400])
            logging.warning(f"Resposta n√£o-200/201 no post_lotes: {r.status_code} - {r.text[:400]}")

# Payload permitido
colunas_existentes = [
    "protocolo","data_da_criacao","status_demanda","prazo_restante",
    "data_da_conclusao","tempo_de_resolucao_em_dias","prioridade",
    "tipo_de_manifestacao","tema","assunto","canal","endereco",
    "unidade_cadastro","unidade_saude","status","servidor","responsavel",
    "orgaos","verificado"
]

# ========================================================
# 5) COLETA DE CHAVES EXISTENTES NO SERVIDOR (protocolo)
# ========================================================
_BANNER("6) COLETA DE PROTOCOLOS EXISTENTES NO SERVIDOR")

def _fetch_all_protocols(step: int = 1000) -> set:
    protos, offset = set(), 0
    while True:
        hdrs = headers.copy()
        hdrs.update({"Range-Unit": "items", "Range": f"{offset}-{offset+step-1}"})
        try:
            r = requests.get(url_get, headers=hdrs, params={"select": "protocolo"}, timeout=30)
        except requests.RequestException as e:
            logging.exception("Falha na requisi√ß√£o de protocolos ao servidor")
            raise SystemExit(f"‚ùå Falha ao paginar protocolos (erro de requisi√ß√£o): {e}")
        if r.status_code not in (200, 206):
            raise SystemExit(f"‚ùå Falha ao paginar protocolos ({r.status_code}): {r.text[:200]}")
        chunk = r.json() or []
        if not chunk:
            break
        for row in chunk:
            p = str(row.get("protocolo") or "").strip()
            if p:
                protos.add(p)
        offset += len(chunk)
    return protos

# fetch protocolos (mantido)
protocolos_server = _fetch_all_protocols()
print(f"üîë Protocolos j√° no servidor: {len(protocolos_server)}")
logging.info(f"Protocolos j√° no servidor: {len(protocolos_server)}")

# ========================================================
# 6) LIMPEZA B√ÅSICA (texto) + RECORTE PARA NOVOS por PROTOCOLO
# ========================================================
_BANNER("6) LIMPEZA B√ÅSICA + RECORTE POR PROTOCOLO")

if "protocolo" not in df.columns:
    raise KeyError("Falta a coluna obrigat√≥ria: protocolo")

# padroniza protocolo e filtra
df["protocolo"] = df["protocolo"].astype(str).str.strip()
df = df[df["protocolo"].ne("")].copy()

mask_novos = ~df["protocolo"].isin(protocolos_server)
df_novos = df[mask_novos].copy()

print(f"üÜï Linhas novas (protocolo ainda n√£o existente no servidor): {len(df_novos)}")
logging.info(f"Linhas novas: {len(df_novos)}")

# ========================================================
# 7) TRATAMENTOS ‚Äî APLICADOS SOMENTE AOS NOVOS
# ========================================================
_BANNER("8) TRATAMENTOS (somente NOVOS)")

def _tratar_full(df_in: pd.DataFrame) -> pd.DataFrame:
    df_loc = df_in.copy()

    # 7.1 Tema/Assunto ‚Äî mant√©m 'n√£o se aplica' ‚Üí 'Ass√©dio'
    if "tema" in df_loc.columns and "assunto" in df_loc.columns:
        tema_tmp = df_loc["tema"].astype(str).str.strip().str.casefold()
        assunto_tmp = df_loc["assunto"].astype(str).str.strip().str.casefold()
        valores_assunto = ["outro", "outros", "na", "n/a", "n\\a", ""]
        cond_42 = (tema_tmp == "n√£o se aplica") & (assunto_tmp.isin(valores_assunto))
        if int(cond_42.sum()):
            df_loc.loc[cond_42, "assunto"] = "Ass√©dio"
        cond_41 = (tema_tmp == "n√£o se aplica")
        if int(cond_41.sum()):
            df_loc.loc[cond_41, "tema"] = "Ass√©dio"

    # 7.2 Data da conclus√£o ‚Üí texto "DD/MM/AA" ou "N√£o conclu√≠do"
    if "data_da_conclusao" in df_loc.columns:
        df_loc["data_da_conclusao"] = _conclusao_strict(df_loc["data_da_conclusao"])

    # 7.3 Unidades de sa√∫de (capitaliza e trata ‚Äúsem informa√ß√£o‚Äù)
    for col in df_loc.columns:
        if "unidade" in col and "saude" in col:
            df_loc[col] = (
                df_loc[col].astype(str).str.strip().str.lower()
                .replace("sem informa√ß√£o", "N√£o √© uma Unidade de Sa√∫de")
                .str.capitalize()
            )

    # 7.4 √ìrg√£os por tema ‚Äî MATCH EXATO, fallback apenas se TEMA vazio
    import unicodedata as _ud, re as _re
    def _norm(s):
        if pd.isna(s): return ""
        s = str(s).strip().lower()
        s = _ud.normalize("NFD", s)
        s = "".join(ch for ch in s if _ud.category(ch) != "Mn")
        return _re.sub(r"\s+", " ", s)

    def _div_temas(v, seps=(",", ";", "|", "/")):
        if pd.isna(v): return []
        t = str(v)
        for s in seps: t = t.replace(s, ",")
        partes = [p.strip() for p in t.split(",") if p.strip()]
        return partes if partes else [str(v).strip()]

    map_tema_para_orgao = {
        "Administra√ß√£o P√∫blica":"Secretaria de Administra√ß√£o","Agricultura":"Secretaria de Obras e Agricultura",
        "Assist√™ncia Social e Direitos Humanos":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Assuntos Jur√≠dicos":"Procuradoria Geral","Comunica√ß√£o Social":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas",
        "Controle Governamental":"Secretaria de Controle Interno","Crian√ßa, Adolescente e Idoso":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Cultura e Turismo":"Secretaria de Cultura e Turismo","Defesa Civil":"Secretaria de Defesa Civil",
        "Direitos √† Pessoa com Defici√™ncia":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Direitos e Vantagens do Servidor":"Secretaria de Administra√ß√£o","Educa√ß√£o":"Secretaria de Educa√ß√£o",
        "Empresas e Legaliza√ß√µes":"Secretaria de Fazenda","Esporte e Lazer":"Secretaria de Esporte e Lazer",
        "Fiscaliza√ß√£o e tributos":"Secretaria de Fazenda","Fiscaliza√ß√£o Urbana, Regulariza√ß√£o e Registro de Im√≥veis":"Secretaria de Urbanismo e Habita√ß√£o",
        "FUNDEC":"FUNDEC","Governan√ßa":"Secretaria de Governo","Governo Municipal e Enterro Gratuito":"Secretaria de Governo",
        "Habita√ß√£o":"Secretaria de Urbanismo e Habita√ß√£o","Inclus√£o e Acessibilidade":"Secretaria de Gest√£o, Inclus√£o e Mulher",
        "Meio Ambiente":"Secretaria de Meio Ambiente",
        "Meio Ambiente (Polui√ß√£o Sonora, √Årvores, Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.)":"Secretaria de Meio Ambiente",
        "Ass√©dio":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas","Obras P√∫blicas":"Secretaria de Obras e Agricultura",
        "Obras, Limpeza Urbana e Bra√ßo de Luz":"Secretaria de Obras e Agricultura","Prote√ß√£o Animal":"Secretaria de Prote√ß√£o Animal",
        "Sa√∫de":"Secretaria de Sa√∫de","Seguran√ßa P√∫blica":"Secretaria de Seguran√ßa P√∫blica",
        "Seguran√ßa, Sinaliza√ß√£o e Multas":"Secretaria de Seguran√ßa P√∫blica",
        "Trabalho, Emprego e Renda":"Secretaria de Trabalho, Emprego e Renda",
        "Transportes e Servi√ßos P√∫blicos":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "Transportes, Servi√ßos P√∫blicos e Troca de L√¢mpadas":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "Urbanismo":"Secretaria de Urbanismo e Habita√ß√£o",
        "Vetores e Zoonoses (Combate √† Dengue, Controle de Pragas, Cria√ß√£o Irregular de Animais e etc.)":"Secretaria de Sa√∫de",
        "Vigil√¢ncia Sanit√°ria":"Secretaria de Sa√∫de",
        "Obras":"Secretaria de Obras e Agricultura","Trabalho":"Secretaria de Trabalho, Emprego e Renda",
        "Seguran√ßa":"Secretaria de Seguran√ßa P√∫blica","Servi√ßos P√∫blicos e Troca de L√¢mpadas":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "√Årvores":"Secretaria de Meio Ambiente","Controle de Pragas":"Secretaria de Sa√∫de","Cria√ß√£o Irregular de Animais":"Secretaria de Sa√∫de",
        "Adolescente e Idoso":"Secretaria de Assist√™ncia Social e Direitos Humanos","Crian√ßa":"Secretaria de Assist√™ncia Social e Direitos Humanos",
        "Emprego e Renda":"Secretaria de Trabalho, Emprego e Renda","Fiscaliza√ß√£o Urbana":"Secretaria de Urbanismo e Habita√ß√£o",
        "Regulariza√ß√£o e Registro de Im√≥veis":"Secretaria de Urbanismo e Habita√ß√£o","Limpeza Urbana e Bra√ßo de Luz":"Secretaria de Obras e Agricultura",
        "Meio Ambiente (Polui√ß√£o Sonora)":"Secretaria de Meio Ambiente","Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.":"Secretaria de Meio Ambiente",
        "Vetores e Zoonoses (Combate √† Dengue)":"Secretaria de Sa√∫de",
        "Cria√ß√£o Irregular de Animais e etc.)":"Secretaria de Sa√∫de","Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.)":"Secretaria de Meio Ambiente",
        "Meio Ambiente (Polui√ß√£o Sonora":"Secretaria de Meio Ambiente","N√£o se aplica":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas",
        "Sinaliza√ß√£o e Multas":"Secretaria de Seguran√ßa P√∫blica","Transportes":"Secretaria de Transportes e Servi√ßos P√∫blicos",
        "Vetores e Zoonoses (Combate √† Dengue":"Secretaria de Sa√∫de",
    }
    # aplica canoniza√ß√£o dos valores de retorno (mant√©m l√≥gica original)
    map_tema_para_orgao = {k: _canon_txt(v) for k, v in map_tema_para_orgao.items()}
    map_exact = { _norm(k): v for k, v in map_tema_para_orgao.items() }

    def mapear_orgao_exato(celula_tema):
        orgs = []
        for t in _div_temas(celula_tema):
            t_norm = _norm(t)
            if not t_norm:
                continue
            if t_norm in map_exact:
                orgs.append(map_exact[t_norm])
        return " | ".join(dict.fromkeys(o.strip() for o in orgs if o and str(o).strip())) or None

    if "tema" in df_loc.columns:
        df_loc["orgaos"] = df_loc["tema"].apply(mapear_orgao_exato)

        def _canon_orgaos(cell):
            if cell is None or str(cell).strip() == "":
                return cell
            partes = [p.strip() for p in str(cell).split("|")]
            partes = [_canon_txt(p) for p in partes if p]
            return " | ".join(dict.fromkeys(partes))

        df_loc["orgaos"] = df_loc["orgaos"].apply(_canon_orgaos)

        mask_tema_vazio = df_loc["tema"].isna() | (df_loc["tema"].astype(str).str.strip() == "")
        mask_org_vazio  = df_loc["orgaos"].isna() | (df_loc["orgaos"].astype(str).str.strip() == "")
        df_loc.loc[mask_tema_vazio & mask_org_vazio, "orgaos"] = "Secretaria Municipal de Comunica√ß√£o e Rela√ß√µes P√∫blicas"

    # 7.5 Padroniza√ß√£o 'servidor' (dicion√°rio mantido integralmente)
    dicionario_servidor = {
        "Camila do Lago Marins": "Camila Marins",
        "Camila Marins": "Camila Marins",
        "Dhayane Cristina Pinho de Almeida": "Dhayane Cristina Pinho de Almeida",
        "Dhayane Pinho": "Dhayane Cristina Pinho de Almeida",
        "Joana Darc Salles Ferreira": "Joana Darc Salles Ferreira",
        "Joana Salles": "Joana Darc Salles Ferreira",
        "Lucia Helena Tinoco Pacehco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "Lucia Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "Lucia  Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "L√∫cia  Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "L√∫cia Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "Lucia Helenba Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "Rafaella Marques Gomes Santos": "Rafaella Marques Gomes Santos",
        "Roilene Pereira da Silva": "Rosilene Pereira da Silva",
        "Rosilene Pereira da Silva": "Rosilene Pereira da Silva",
        "Stephanie dos Santos Silva": "Stephanie dos Santos Silva",
        "Stephanie Santos": "Stephanie dos Santos Silva",
        "St√©phanie Santos": "Stephanie dos Santos Silva",
        "St√©phaniesantos": "Stephanie dos Santos Silva",
        "Stpehanie Santos": "Stephanie dos Santos Silva",
        "Anne Beatriz da Silva": "Anne Beatriz da Silva Rodrigues",
        "Bruna Maria ( Coordenadora)": "Cidad√£o",
        "Isabel": "Cidad√£o",
        "Gabriela da Silva Rozi": "Cidad√£o",
        "Lana Carolina Mesquita de Andrade": "Cidad√£o",
        "L√≠via Cavalcante": "L√≠via Kathleen Cavalcante Patriota Leite",
        "L√≠via Kathleen Cavalcante Patriota Leite": "L√≠via Kathleen Cavalcante Patriota Leite",
        "Lucia Helena": "L√∫cia Helena Tinoco Pacheco Varella",
        "Lucia Helena Tinoco": "L√∫cia Helena Tinoco Pacheco Varella",
        "Lucia Helena Tinoco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "Lucia Helen Tinoco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "Lucia Helan Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
        "Mery": "Cidad√£o",
        "Ouvidoria Geral (Adm)": "Cidad√£o",
        "Rafaella Marques": "Rafaella Marques Gomes Santos",
        "Ronaldo de Oliveira Brand√£o": "Cidad√£o",
        "S√©phanie Santos": "Stephanie dos Santos Silva",
        "Shirley Santana": "Cidad√£o",
        "St√©panie Santos": "Stephanie dos Santos Silva",
        "St√©phanie  Santos": "Stephanie dos Santos Silva",
        "St√©phanie Santos": "Stephanie dos Santos Silva",
        "Stephanie dos Santos": "Stephanie dos Santos Silva",
        "St√©phanie Santoa": "Stephanie dos Santos Silva",
        "Stephanie Santos": "Stephanie dos Santos Silva",
        "Stephanie dos Santos": "Stephanie dos Santos Silva",
        "Stephanie Santos": "Stephanie dos Santos Silva",
        "Thamires Manh√£es": "Cidad√£o"
    }
    if "servidor" in df_loc.columns:
        _orig = df_loc["servidor"].astype(str).str.strip()
        df_loc["servidor"] = _orig.map(dicionario_servidor).fillna(_orig)

    # 7.6 Respons√°vel (normaliza√ß√£o) - mantido
    if "responsavel" in df_loc.columns:
        df_loc["responsavel"] = _canon_responsavel_series(df_loc["responsavel"])

    # 7.7 Datas e tipos - mantidos
    if "data_da_criacao" in df_loc.columns:
        df_loc["data_da_criacao"] = _to_ddmmaa_text(df_loc["data_da_criacao"]).astype(str)
    if "status_demanda" in df_loc.columns:
        df_loc["status_demanda"] = df_loc["status_demanda"].astype(str)
    if "data_da_conclusao" in df_loc.columns:
        df_loc["data_da_conclusao"] = _conclusao_strict(df_loc["data_da_conclusao"])

    # 7.8 Regra de ouro: se CONCLU√çDA => 'prazo_restante' = 'Demanda Conclu√≠da'
    if "status_demanda" in df_loc.columns and "prazo_restante" in df_loc.columns:
        mask_conc = df_loc["status_demanda"].map(_is_concluida)
        df_loc.loc[mask_conc, "prazo_restante"] = "Demanda Conclu√≠da"
    elif "prazo_restante" in df_loc.columns:
        mask_dc = df_loc["prazo_restante"].map(_looks_like_demanda_concluida)
        df_loc.loc[mask_dc, "prazo_restante"] = "Demanda Conclu√≠da"

    # 7.9 tempo_de_resolucao_em_dias: "N√£o h√° dados" -> "" (vazio)
    if "tempo_de_resolucao_em_dias" in df_loc.columns:
        s = df_loc["tempo_de_resolucao_em_dias"].astype("object")
        def _blank_nao_ha_dados(v):
            if _is_nao_ha_dados(v):
                return ""   # linha vazia (texto vazio)
            return v
        df_loc["tempo_de_resolucao_em_dias"] = s.map(_blank_nao_ha_dados)

    # 7.9 (n√£o altera mais nada aqui ‚Äî sincroniza√ß√£o geral ficar√° no item 10.3)
    return df_loc[[c for c in colunas_existentes if c in df_loc.columns]].copy()

# cria SEMPRE o df_upsert (mesmo vazio) e faz sanity pass
df_upsert = _tratar_full(df_novos)

if "data_da_criacao" in df_upsert.columns:
    df_upsert["data_da_criacao"] = _to_ddmmaa_text(df_upsert["data_da_criacao"]).astype(str).str.strip()
if "data_da_conclusao" in df_upsert.columns:
    df_upsert["data_da_conclusao"] = _conclusao_strict(df_upsert["data_da_conclusao"]).astype(str).str.strip()

print(f"üîé Pr√©-envio (novos) ‚Äî colunas: {list(df_upsert.columns)} | linhas: {len(df_upsert)}")
logging.info(f"Pr√©-envio (novos) ‚Äî colunas: {list(df_upsert.columns)} | linhas: {len(df_upsert)}")

# ========================================================
# 8) UPSERT ‚Äî APENAS NOVOS (por PROTOCOLO)
# ========================================================
_BANNER("9) UPSERT ‚Äî APENAS NOVOS")

if 'df_upsert' not in globals():
    raise SystemExit("‚ùå df_upsert n√£o encontrado. Gere-o com _tratar_full(df_novos) antes do item 9.")

df_novos_filtrado = df_upsert[~df_upsert["protocolo"].astype(str).str.strip().isin(protocolos_server)].copy()
print(f"üÜï Linhas realmente novas ap√≥s filtro: {len(df_novos_filtrado)}")
logging.info(f"Linhas realmente novas ap√≥s filtro: {len(df_novos_filtrado)}")

if df_novos_filtrado.empty:
    print("‚úÖ Nenhum registro novo para enviar.")
    logging.info("Nenhum registro novo para enviar.")
else:
    post_url = f"{url_upsert}?on_conflict=protocolo"
    # Prefer 'return-minimal' (sem merge autom√°tico): s√≥ insere novos; conflito vira erro (esperado n√£o ocorrer pelo filtro).
    post_headers = {**headers, "Prefer": "return=minimal"}

    cols_ok = [c for c in df_novos_filtrado.columns if c in colunas_existentes]
    df_send = df_novos_filtrado[cols_ok].copy()

    # LOG de sanidade para unidade_cadastro (somente novos)
    if "unidade_cadastro" in df_send.columns:
        nulos_uc = int(df_send["unidade_cadastro"].isna().sum())
        print(f"üß™ Checagem (NOVOS): unidade_cadastro presente | nulos={nulos_uc}")
        logging.info(f"Checagem (NOVOS): unidade_cadastro presente | nulos={nulos_uc}")
    else:
        print("‚ö†Ô∏è Aviso: unidade_cadastro n√£o est√° em df_send (verifique colunas_existentes).")
        logging.warning("unidade_cadastro n√£o est√° em df_send (verifique colunas_existentes).")

    lote = 500
    total_lotes = (len(df_send) + lote - 1) // lote
    print(f"üì¶ UPSERT ‚Äî APENAS NOVOS: {len(df_send)} linhas | {total_lotes} lotes")
    logging.info(f"UPSERT ‚Äî APENAS NOVOS: {len(df_send)} linhas | {total_lotes} lotes")

    for i in range(0, len(df_send), lote):
        chunk = df_send.iloc[i:i+lote].replace({np.nan: None})
        payload = json.dumps(chunk.to_dict(orient="records"), ensure_ascii=False).encode("utf-8")

        first_idx = i + 1
        last_idx = min(i + lote, len(df_send))
        protos_preview = list(chunk.get("protocolo", []))[:3]

        print(f"   ‚Ä¢ Enviando {first_idx}-{last_idx} (pr√©via protocolos: {protos_preview})")
        logging.info(f"Enviando {first_idx}-{last_idx} (pr√©via protocolos: {protos_preview})")

        try:
            r = requests.post(post_url, headers=post_headers, data=payload, timeout=30)
        except requests.RequestException as e:
            logging.exception(f"Erro ao enviar lote {first_idx}-{last_idx} (UPSERT)")
            print(f"       ‚ùå Erro na requisi√ß√£o: {e}")
            continue

        print(f"     ‚Üí Lote {i//lote + 1}/{total_lotes} | status {r.status_code}")
        logging.info(f"Lote {i//lote + 1}/{total_lotes} | status {r.status_code}")

        if r.status_code not in (200, 201, 204):
            print("       ‚ùå Erro:", (r.text or "")[:400])
            logging.warning(f"Erro no upsert lote {i//lote + 1}: {r.status_code} - {r.text[:400]}")
            protos_errados = list(chunk.get("protocolo", []))
            print(f"       Protocolos neste lote com erro: {protos_errados}")
            logging.info(f"Protocolos com erro: {protos_errados}")

# ========================================================
# 9) DELTAS HIST√ìRICOS ‚Äî status_demanda e data_da_conclusao
# ========================================================
_BANNER("10) DELTAS HIST√ìRICOS (status_demanda & data_da_conclusao)")

# ==== BLOQUEIO: nunca PATCH em unidade_cadastro ====
_COLUNAS_PATCH_PROIBIDAS = {"unidade_cadastro"}
# ===================================================

def _buscar_cols_por_protocolos(protos_series: pd.Series, cols, tam_lote: int = 300) -> dict:
    protos = protos_series.dropna().astype(str).str.strip().unique().tolist()
    resultado = {}
    if not protos: return resultado
    sel = "protocolo," + ",".join(cols)
    for i in range(0, len(protos), tam_lote):
        lote = protos[i:i+tam_lote]
        in_list = ",".join([f'"{p.replace("\"", "\'")}"' for p in lote])
        params = {"select": sel, "protocolo": f"in.({in_list})"}
        try:
            r = requests.get(url_get, headers=headers, params=params, timeout=30)
        except requests.RequestException:
            logging.exception("Falha ao buscar cols por protocolos")
            continue
        if r.status_code in (200, 206):
            for row in r.json():
                p = str(row.get("protocolo")).strip()
                resultado[p] = {c: row.get(c) for c in cols}
        else:
            print(f"‚ö†Ô∏è Falha ao buscar {cols} -> {r.status_code}: {r.text[:200]}")
            logging.warning(f"Falha ao buscar {cols} -> {r.status_code}: {r.text[:200]}")
    return resultado

def _patch_grouped_force(df_send: pd.DataFrame, key_col: str, value_col: str, batch_keys: int = 400):
    # BLOQUEIO expl√≠cito para evitar tocar unidade_cadastro
    if value_col in _COLUNAS_PATCH_PROIBIDAS:
        print(f"‚õî PATCH bloqueado para coluna: {value_col} (pol√≠tica: n√£o alterar unidade_cadastro)")
        logging.warning(f"PATCH bloqueado para coluna: {value_col}")
        return

    if df_send.empty:
        print(f"üì¶ DELTA(force) {value_col}: 0 linhas (skip).")
        logging.info(f"DELTA(force) {value_col}: 0 linhas (skip).")
        return
    df_send = df_send[[key_col, value_col]].dropna(subset=[key_col, value_col]).copy()
    total = len(df_send)
    grupos = df_send.groupby(value_col)
    print(f"üì¶ DELTA(force) {value_col}: {total} linhas | {len(grupos)} grupos")
    logging.info(f"DELTA(force) {value_col}: {total} linhas | {len(grupos)} grupos")

    for val, g in grupos:
        protos = g[key_col].astype(str).str.strip().unique().tolist()
        payload = json.dumps({value_col: val}, ensure_ascii=False).encode("utf-8")
        for i in range(0, len(protos), batch_keys):
            bloco = protos[i:i+batch_keys]
            quoted_list = ",".join([f'"{p.replace("\"","\'")}"' for p in bloco])
            qs = f'{quote(key_col)}=in.({quoted_list})'
            print(f"   ‚Ä¢ PATCH {value_col}='{val}' | protos {i+1}-{i+len(bloco)} de {len(protos)}")
            logging.info(f"PATCH {value_col}='{val}' | protos {i+1}-{i+len(bloco)} de {len(protos)}")
            try:
                r = requests.patch(f"{url_upsert}?{qs}", headers=headers, data=payload, timeout=30)
            except requests.RequestException:
                logging.exception("Erro no PATCH agrupado")
                print("       Resposta: Erro na requisi√ß√£o PATCH")
                continue
            print(f"     ‚Üí status {r.status_code} (itens {len(bloco)})")
            logging.info(f"Status PATCH: {r.status_code} (itens {len(bloco)})")
            if r.status_code not in (200, 204):
                print("       Resposta:", r.text[:300])
                logging.warning(f"PATCH retornou {r.status_code}: {r.text[:300]}")

server_map = _buscar_cols_por_protocolos(df["protocolo"], ["status_demanda","data_da_conclusao"], 300)

# Delta STATUS
loc_status = df[["protocolo","status_demanda"]].copy()
loc_status["local_norm"]  = pd.Series(loc_status["status_demanda"], dtype="object").astype(str).str.strip()
loc_status["server_norm"] = loc_status["protocolo"].map(lambda p: _canon_txt((server_map.get(p, {}) or {}).get("status_demanda", "")))
mask_st_changed = loc_status["local_norm"].fillna("") != loc_status["server_norm"].fillna("")
df_status_delta = loc_status.loc[mask_st_changed, ["protocolo"]].copy()
df_status_delta["status_demanda"] = loc_status.loc[mask_st_changed, "local_norm"].values
print(f"üîÅ Delta STATUS: {len(df_status_delta)} linhas para atualizar.")
logging.info(f"Delta STATUS: {len(df_status_delta)} linhas para atualizar.")

# Delta DATA DA CONCLUS√ÉO
loc_dc = df[["protocolo","data_da_conclusao"]].copy()
loc_dc["local_norm"]  = _conclusao_strict(loc_dc["data_da_conclusao"]).astype(str).str.strip()
srv_raw_dc = pd.Series([ (server_map.get(p, {}) or {}).get("data_da_conclusao", None) for p in df["protocolo"] ])
srv_norm_dc = _conclusao_strict(srv_raw_dc).astype(str).str.strip()
loc_dc["server_norm"] = srv_norm_dc.values
mask_dc_changed = loc_dc["local_norm"].fillna("") != loc_dc["server_norm"].fillna("")
df_conc_delta = loc_dc.loc[mask_dc_changed, ["protocolo"]].copy()
df_conc_delta["data_da_conclusao"] = loc_dc.loc[mask_dc_changed, "local_norm"].values
print(f"üîÅ Delta DATA_CONCLUSAO: {len(df_conc_delta)} linhas para atualizar.")
logging.info(f"Delta DATA_CONCLUSAO: {len(df_conc_delta)} linhas para atualizar.")

if not df_conc_delta.empty:
    df_conc_delta["data_da_conclusao"] = (
        _conclusao_strict(df_conc_delta["data_da_conclusao"])
        .astype(str).str.strip()
    )

_patch_grouped_force(df_status_delta, "protocolo", "status_demanda")
_patch_grouped_force(df_conc_delta,   "protocolo", "data_da_conclusao")

# ========================================================
# 9.1) DELTA HIST√ìRICO ‚Äî prazo_restante (respeita base bruta)
# ========================================================
_BANNER("9.1) DELTA HIST√ìRICO (prazo_restante ‚Äî respeitar base bruta)")

server_prz_map = _buscar_cols_por_protocolos(df["protocolo"], ["prazo_restante"], 300)

loc_prz = df[["protocolo","prazo_restante"]].copy()
loc_prz["local_norm"] = loc_prz["prazo_restante"].map(_canon_prazo_restante)

if "status_demanda" in df.columns:
    mask_conc_local = df["status_demanda"].map(_is_concluida)
    loc_prz.loc[mask_conc_local, "local_norm"] = "Demanda Conclu√≠da"

loc_prz["local_is_dc"] = loc_prz["local_norm"].map(_looks_like_demanda_concluida)
loc_prz["server_raw"] = loc_prz["protocolo"].map(lambda p: (server_prz_map.get(str(p).strip(), {}) or {}).get("prazo_restante", ""))
loc_prz["server_is_dc"] = loc_prz["server_raw"].map(_looks_like_demanda_concluida)

df_delta_fix_grafia = loc_prz.loc[
    loc_prz["server_is_dc"] & (loc_prz["server_raw"].astype(str).str.strip() != "Demanda Conclu√≠da"),
    ["protocolo"]
].copy()
df_delta_fix_grafia["prazo_restante"] = "Demanda Conclu√≠da"

df_delta_from_base = loc_prz.loc[
    loc_prz["local_is_dc"] & (~loc_prz["server_is_dc"]),
    ["protocolo"]
].copy()
df_delta_from_base["prazo_restante"] = "Demanda Conclu√≠da"

df_prazo_delta = pd.concat([df_delta_fix_grafia, df_delta_from_base], ignore_index=True).drop_duplicates("protocolo")
print(f"üîÅ Delta PRAZO_RESTANTE (hist√≥rico): {len(df_prazo_delta)} linhas para atualizar.")
logging.info(f"Delta PRAZO_RESTANTE: {len(df_prazo_delta)} linhas para atualizar.")
if not df_prazo_delta.empty:
    _patch_grouped_force(df_prazo_delta, "protocolo", "prazo_restante")

# ========================================================
# 9.3) DELTA HIST√ìRICO ‚Äî tempo_de_resolucao_em_dias (sincroniza com a planilha; 0 -> 1)
# ========================================================
_BANNER("9.3) DELTA HIST√ìRICO (tempo_de_resolucao_em_dias ‚Äî sincroniza local; 0‚Üí1)")

if "tempo_de_resolucao_em_dias" in df.columns:
    server_tmp_map = _buscar_cols_por_protocolos(df["protocolo"], ["tempo_de_resolucao_em_dias"], 300)

    s_local = df["tempo_de_resolucao_em_dias"].astype("object")

    # <<< SUBSTITU√çDA pela vers√£o pedida >>>
    def _fix_zero_keep_text(v):
        if pd.isna(v):
            return v
        sv = str(v).strip()

        # 1) "N√£o h√° dados" -> vazio
        if _is_nao_ha_dados(sv):
            return ""   # texto vazio, evita drop por NA no patch

        # 2) 0 -> 1 (mant√©m sua regra)
        try:
            num = pd.to_numeric(sv, errors="coerce")
            if pd.notna(num) and float(num) == 0.0:
                return "1"
        except:
            pass

        # 3) mant√©m como est√° em qualquer outro caso
        return sv
    # >>> FIM SUBSTITUI√á√ÉO

    s_local_fixed = s_local.map(_fix_zero_keep_text)

    s_server = df["protocolo"].astype(str).str.strip().map(
        lambda p: _canon_txt((server_tmp_map.get(p, {}) or {}).get("tempo_de_resolucao_em_dias", ""))
    ).astype("object").str.strip()

    mask_change = (s_local_fixed.fillna("") != s_server.fillna(""))

    if mask_change.any():
        df_tmp_delta = pd.DataFrame({
            "protocolo": df.loc[mask_change, "protocolo"].astype(str).str.strip(),
            "tempo_de_resolucao_em_dias": s_local_fixed.loc[mask_change].values
        })
        print(f"üîÅ Delta TEMPO_RESOLUCAO (sincroniza): {len(df_tmp_delta)} linhas para atualizar.")
        logging.info(f"Delta TEMPO_RESOLUCAO: {len(df_tmp_delta)} linhas para atualizar.")
        _patch_grouped_force(df_tmp_delta, "protocolo", "tempo_de_resolucao_em_dias")
    else:
        print("üîÅ Delta TEMPO_RESOLUCAO: nada a atualizar (j√° igual ao local).")
        logging.info("Delta TEMPO_RESOLUCAO: nada a atualizar (j√° igual ao local).")
else:
    print("üîÅ Delta TEMPO_RESOLUCAO: coluna ausente na planilha local.")
    logging.info("Delta TEMPO_RESOLUCAO: coluna ausente na planilha local.")

# ========================================================
# 9.3B) LIMPEZA NO SERVIDOR ‚Äî "N√£o h√° dados" -> "" (vazio)
# ========================================================
_BANNER("9.3B) LIMPEZA NO SERVIDOR (tempo_de_resolucao_em_dias)")

def _cleanup_nao_ha_dados_server():
    alvo = "N√£o h√° dados"

    # 1) Contagem r√°pida do que ainda existe no servidor
    hdrs = headers.copy()
    hdrs.update({"Range-Unit": "items", "Range": "0-0", "Prefer": "count=exact"})
    params_count = {"select": "protocolo", "tempo_de_resolucao_em_dias": f"eq.{alvo}"}
    try:
        r_count = requests.get(url_get, headers=hdrs, params=params_count, timeout=30)
    except requests.RequestException:
        logging.exception("Falha na contagem de 'N√£o h√° dados' no servidor.")
        print("‚ö†Ô∏è Falha ao contar 'N√£o h√° dados' no servidor (erro de requisi√ß√£o).")
        return

    if r_count.status_code not in (200, 206):
        print(f"‚ö†Ô∏è Falha ao contar 'N√£o h√° dados' no servidor: {r_count.status_code} {r_count.text[:200]}")
        logging.warning(f"Falha ao contar 'N√£o h√° dados' no servidor: {r_count.status_code}")
        return

    try:
        total = int(r_count.headers.get("content-range", "0/0").split("/")[-1])
    except:
        total = 0

    if total == 0:
        print("‚ÑπÔ∏è Nada para limpar no servidor (0 ocorr√™ncias de 'N√£o h√° dados').")
        logging.info("Nada para limpar no servidor (0 ocorr√™ncias de 'N√£o h√° dados').")
        return

    print(f"üßπ Encontradas {total} ocorr√™ncias no servidor com 'N√£o h√° dados'. Aplicando PATCH...")
    logging.info(f"Encontradas {total} ocorr√™ncias no servidor com 'N√£o h√° dados'.")

    # 2) PATCH em lote no servidor: troca para texto vazio
    payload = json.dumps({"tempo_de_resolucao_em_dias": ""}, ensure_ascii=False).encode("utf-8")
    params_patch = {"tempo_de_resolucao_em_dias": f"eq.{alvo}"}
    try:
        r_patch = requests.patch(url_upsert, headers=headers, params=params_patch, data=payload, timeout=30)
    except requests.RequestException:
        logging.exception("Erro ao aplicar PATCH de limpeza 'N√£o h√° dados' no servidor.")
        print("‚ùå Erro ao aplicar PATCH (requisi√ß√£o).")
        return

    if r_patch.status_code in (200, 204):
        print("‚úÖ PATCH aplicado no servidor: 'N√£o h√° dados' -> '' (vazio).")
        logging.info("PATCH aplicado no servidor: 'N√£o h√° dados' -> '' (vazio).")
    else:
        print(f"‚ùå Erro ao aplicar PATCH: {r_patch.status_code} {r_patch.text[:300]}")
        logging.warning(f"Erro ao aplicar PATCH: {r_patch.status_code} {r_patch.text[:300]}")

_cleanup_nao_ha_dados_server()

# ========================================================
# 9.X) CORRE√á√ÉO PONTUAL ‚Äî servidor (Raphael)
# ========================================================
_BANNER("10.X) CORRE√á√ÉO PONTUAL ‚Äî servidor (Raphael)")

try:
    if "protocolo" in df.columns and "servidor" in df.columns:
        # Protocolos que na planilha local est√£o com o nome correto
        protos_raphael = (
            df.loc[df["servidor"].astype(str).str.strip().eq("Raphael Pereira de Mello"), "protocolo"]
              .astype(str).str.strip().unique().tolist()
        )

        if protos_raphael:
            df_fix = pd.DataFrame({
                "protocolo": protos_raphael,
                "servidor": "Raphael Pereira de Mello"
            })
            _patch_grouped_force(df_fix, "protocolo", "servidor")
        else:
            print("‚ÑπÔ∏è Nenhum protocolo local com 'Raphael Pereira de Mello' encontrado para corrigir.")
            logging.info("Nenhum protocolo local com 'Raphael Pereira de Mello' encontrado para corrigir.")
    else:
        print("‚ÑπÔ∏è Colunas 'protocolo' e/ou 'servidor' ausentes no DF local.")
        logging.info("Colunas 'protocolo' e/ou 'servidor' ausentes no DF local.")
except Exception as e:
    # mantido pass silencioso anterior, mas agora logamos
    logging.exception("Erro durante corre√ß√£o pontual (Raphael).")

# ========================================================
# 10) EXPORTA√á√ÉO INCREMENTAL ‚Äî GOOGLE DRIVE (PLANILHA TRATADA)
# ========================================================
_BANNER("10) EXPORTA√á√ÉO INCREMENTAL PARA GOOGLE DRIVE (PLANILHA TRATADA)")

# --- Cria refer√™ncia da base tratada final (necess√°rio para integra√ß√£o incremental)
df_tratado = df.copy()

query_tratada = f"'{PASTA_TRATADA_ID}' in parents and name='{NOME_PLANILHA_TRATADA}' and mimeType='application/vnd.google-apps.spreadsheet'"
arquivos_tratados = drive_service.files().list(
    q=query_tratada, spaces="drive",
    fields="files(id, name, createdTime)"
).execute().get("files", [])

if arquivos_tratados:
    sheet_tratada_id = arquivos_tratados[0]["id"]
    print(f"üìÑ Planilha tratada existente encontrada: {NOME_PLANILHA_TRATADA} (ID: {sheet_tratada_id})")
    sh_tratada = gc.open_by_key(sheet_tratada_id)
    try:
        df_existente = pd.DataFrame(sh_tratada.sheet1.get_all_records())
        df_existente.columns = [normalizar_nome_coluna(c) for c in df_existente.columns]
        print(f"üìä Base tratada existente carregada: {df_existente.shape}")
    except Exception as e:
        print("‚ö†Ô∏è Erro ao ler planilha tratada existente, criando DataFrame vazio:", e)
        df_existente = pd.DataFrame(columns=df_tratado.columns)
else:
    print(f"üÜï Nenhuma planilha tratada encontrada; uma nova ser√° criada.")
    df_existente = pd.DataFrame(columns=df_tratado.columns)
    sheet_tratada_id = None

# ========================================================
# 10.1) DETEC√á√ÉO DE NOVOS REGISTROS
# ========================================================
if "protocolo" not in df_tratado.columns:
    raise KeyError("A coluna 'protocolo' √© obrigat√≥ria para sincroniza√ß√£o incremental.")

df_tratado["protocolo"] = df_tratado["protocolo"].astype(str).str.strip()
df_existente["protocolo"] = df_existente.get("protocolo", pd.Series(dtype="object")).astype(str).str.strip()

protocolos_existentes = set(df_existente["protocolo"].dropna().tolist())
df_novos = df_tratado[~df_tratado["protocolo"].isin(protocolos_existentes)].copy()
print(f"‚ûï Novos protocolos detectados: {len(df_novos)}")

# ========================================================
# 10.2) MERGE (HIST√ìRICO + NOVOS)
# ========================================================
if not df_existente.empty:
    df_combinado = pd.concat([df_existente, df_novos], ignore_index=True)
    df_combinado.drop_duplicates(subset=["protocolo"], keep="last", inplace=True)
else:
    df_combinado = df_tratado.copy()

print(f"üìà Total final de registros ap√≥s merge: {len(df_combinado)}")

# ========================================================
# 10.3) EXPORTA√á√ÉO FINAL PARA GOOGLE SHEETS
# ========================================================
if sheet_tratada_id:
    sh_tratada = gc.open_by_key(sheet_tratada_id)
    ws = sh_tratada.sheet1
    ws.clear()
else:
    file_metadata = {
        "name": NOME_PLANILHA_TRATADA,
        "mimeType": "application/vnd.google-apps.spreadsheet",
        "parents": [PASTA_TRATADA_ID]
    }
    nova_planilha = drive_service.files().create(body=file_metadata, fields="id").execute()
    sheet_tratada_id = nova_planilha["id"]
    sh_tratada = gc.open_by_key(sheet_tratada_id)
    ws = sh_tratada.sheet1

set_with_dataframe(ws, df_combinado)
print("‚úÖ Base tratada atualizada de forma incremental com sucesso!")
print(f"üìé Link direto: https://docs.google.com/spreadsheets/d/{sheet_tratada_id}/edit")

_BANNER("11) PIPELINE FINALIZADO")
print("üéØ Fluxo conclu√≠do:")
print("GoogleDrive (Bruto) ‚Üí PythonAnywhere (Tratamento) ‚Üí GoogleDrive (Tratado Incremental) ‚Üí Looker Studio")

# ========================================================
# 11) QA & SUM√ÅRIO FINAL
# ========================================================

_BANNER("11) QA & SUM√ÅRIO FINAL")

def _count_where_chunked(protos: list, chunk_size: int = 300) -> int:
    total_null = 0
    for i in range(0, len(protos), chunk_size):
        lote = protos[i:i+chunk_size]
        quoted_list = ",".join([f'"{p.replace("\"","\'")}"' for p in lote])

        hdrs = headers.copy()
        hdrs.update({"Range-Unit":"items", "Range":"0-0", "Prefer":"count=exact"})
        params = {"select":"protocolo", "protocolo": f"in.({quoted_list})", "orgaos": "is.null"}

        try:
            r = requests.get(url_upsert, headers=hdrs, params=params, timeout=30)
        except requests.RequestException:
            logging.exception("Falha no QA chunk (requisi√ß√£o)")
            print(f"‚ö†Ô∏è QA chunk {i//chunk_size+1}: erro na requisi√ß√£o")
            continue

        if r.status_code in (200, 206):
            try:
                total_null += int(r.headers.get("content-range","0/0").split("/")[-1])
            except:
                pass
        else:
            print(f"‚ö†Ô∏è QA chunk {i//chunk_size+1}: {r.status_code} {r.text[:150]}")
            logging.warning(f"QA chunk {i//chunk_size+1}: {r.status_code} {r.text[:150]}")
    return total_null

if not df_upsert.empty:
    novos_protos = df_upsert["protocolo"].astype(str).str.strip().unique().tolist()
    vazios_novos = _count_where_chunked(novos_protos, chunk_size=300)
    print(f"üîé QA (NOVOS): 'orgaos' NULL = {vazios_novos}")
    logging.info(f"QA (NOVOS): 'orgaos' NULL = {vazios_novos}")
else:
    print("‚ÑπÔ∏è QA (NOVOS): n√£o havia linhas novas para inserir.")
    logging.info("QA (NOVOS): n√£o havia linhas novas para inserir.")

# ========================================================
# 12) FINALIZA√á√ÉO
# ========================================================
_BANNER("6) PIPELINE FINALIZADO")

print("üéØ Pipeline executado com sucesso!")
logging.info("Pipeline executado com sucesso")
print("Fluxo: GoogleDrive (bruto) ‚Üí PythonAnywhere (tratamento) ‚Üí GoogleDrive (tratado) ‚Üí LookerStudio")