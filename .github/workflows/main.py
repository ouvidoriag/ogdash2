import os
import pandas as pd
import unicodedata
import re
import requests
import json
import gspread
import numpy as np
import base64 
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import warnings
from urllib.parse import quote
import hashlib
from gspread_dataframe import set_with_dataframe
import logging
from typing import List, Dict

# ----------------------------
# Logging (arquivo + console)
# ----------------------------
logging.basicConfig(
    level=logging.INFO, # Pode mudar para logging.DEBUG para mais detalhes durante a depura√ß√£o.
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler("pipeline_tratamento.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# --------------------------------------------------------
# Utilit√°rio de banner de se√ß√£o (para logs/prints)
# --------------------------------------------------------
def _BANNER(titulo):
    print("\n" + "="*18 + f" {titulo} " + "="*18)
    logging.info(titulo)

def _SUB(titulo):
    print("‚Äî " + titulo)
    logging.info(titulo)

# ========================================================
# 1) CONFIGURA√á√ÉO GOOGLE DRIVE / SHEETS (REMOVIDA LIMPEZA AGRESSIVA)
# ========================================================

_BANNER("1) CONFIGURA√á√ÉO GOOGLE DRIVE/SHEETS")

# ----------------------------
# AUTENTICA√á√ÉO √öNICA (usar apenas uma vez)
# ----------------------------
CAMINHO_CREDENCIAIS = ".github/workflows/credentials.json"
SCOPES = [
    "https://www.googleapis.com/auth/drive",
    "https://www.googleapis.com/auth/spreadsheets"
]

try:
    logging.info(f"Tentando ler string Base64 do arquivo: '{CAMINHO_CREDENCIAIS}'")
    with open(CAMINHO_CREDENCIAIS, "r", encoding="utf-8") as file:
        encoded_json_string = file.read().strip() # L√™ e remove espa√ßos/newlines

    # Decodifica de Base64 para bytes, depois para string UTF-8
    decoded_json_bytes = base64.b64decode(encoded_json_string)
    decoded_json_str = decoded_json_bytes.decode('utf-8')
    
    # --- CORRE√á√ÉO AQUI: A linha de limpeza agressiva 'clean_json_str = re.sub(...)' FOI REMOVIDA. ---
    # Agora passamos a string JSON decodificada diretamente para json.loads.
    service_account_info = json.loads(decoded_json_str)
    
    logging.info("‚úÖ Arquivo de credenciais Base64 lido e JSON decodificado com sucesso (limpeza agressiva removida).")
    
    creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)
    drive_service = build("drive", "v3", credentials=creds)
    gc = gspread.authorize(creds)
    client = gc
    logging.info("‚úÖ Autentica√ß√£o Google OK")
    print("‚úÖ Autentica√ß√£o Google OK.")
except FileNotFoundError:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google: Arquivo de credenciais n√£o encontrado em '{CAMINHO_CREDENCIAIS}'. Verifique o caminho e a cria√ß√£o do arquivo no workflow.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Arquivo de credenciais n√£o encontrado. O pipeline ser√° encerrado.")
except base64.binascii.Error as e:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google: Erro ao decodificar a string Base64. Conte√∫do inv√°lido no secret? Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Conte√∫do Base64 inv√°lido no arquivo de credenciais.")
except json.JSONDecodeError as e:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google: Erro ao decodificar JSON da string Base64 decodificada. Conte√∫do inv√°lido. Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    # Se este erro ainda ocorrer, o problema est√° na string JSON decodificada antes de qualquer limpeza.
    # O log do YML (com xxd -p) ser√° crucial para ver o que o Base64 produziu.
    raise SystemExit("Erro cr√≠tico: Conte√∫do JSON inv√°lido na string Base64 decodificada.")
except Exception as e:
    logging.critical(f"‚ùå Falha na autentica√ß√£o Google. Erro inesperado: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Falha inesperada na autentica√ß√£o Google. O pipeline ser√° encerrado.")

# ========================================================
# 2) LEITURA DA PLANILHA BRUTA (GOOGLE DRIVE - DIN√ÇMICO) - MANTIDO COM MELHORIAS DE TRY/EXCEPT
# ========================================================
_BANNER("2) LEITURA DA PLANILHA BRUTA (GOOGLE DRIVE - DIN√ÇMICO)")

# As importa√ß√µes de 'googleapiclient.discovery', 'google.oauth2.service_account',
# 'gspread', 'pandas', 'logging' j√° est√£o no topo do arquivo.
# N√£o precisam ser repetidas aqui.

# --- Fun√ß√£o helper para obter a √∫ltima planilha da pasta bruta ---
# Esta fun√ß√£o j√° estava bem definida.
def get_latest_spreadsheet_df(folder_id: str, gspread_client, drive_svc) -> (str, str, pd.DataFrame):
    try:
        res = drive_svc.files().list(
            q=f"'{folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet' and trashed=false",
            orderBy="modifiedTime desc",
            pageSize=1,
            fields="files(id, name, modifiedTime)"
        ).execute()
        files = res.get("files", [])
        if not files:
            logging.critical(f"‚ùå Nenhuma planilha bruta encontrada na pasta do Google Drive com ID: '{folder_id}'. O pipeline ser√° encerrado.", exc_info=True)
            raise SystemExit("Erro cr√≠tico: Nenhuma planilha bruta encontrada.")
        latest = files[0]
        fid, fname = latest["id"], latest["name"]
        sh = gspread_client.open_by_key(fid)
        aba = sh.sheet1
        dfb = pd.DataFrame(aba.get_all_records())
        return fid, fname, dfb
    except Exception as e:
        logging.critical(f"‚ùå Erro ao obter a √∫ltima planilha da pasta bruta '{folder_id}': {e}. O pipeline ser√° encerrado.", exc_info=True)
        raise SystemExit("Erro cr√≠tico: Falha ao carregar planilha bruta.")


# --- Uso ---
FOLDER_ID_BRUTA = "1qXj9eGauvOREKVgRPOfKjRlLSKhefXI5" # Mantenha seu ID de pasta aqui
try:
    latest_file_id, latest_file_name, df_bruta = get_latest_spreadsheet_df(FOLDER_ID_BRUTA, gc, drive_service)
    df = df_bruta.copy()

    print(f"üìÇ √öltima planilha encontrada: {latest_file_name} ({latest_file_id})")
    logging.info(f"√öltima planilha encontrada: {latest_file_name} ({latest_file_id})")
    print(f"‚úÖ Planilha bruta importada com sucesso: {df_bruta.shape}")
    logging.info(f"Planilha bruta importada com sucesso: {df_bruta.shape}")
except SystemExit: # Captura o SystemExit da fun√ß√£o helper para n√£o logar novamente
    raise
except Exception as e:
    logging.critical(f"‚ùå Erro ao processar a planilha bruta principal. Verifique o FOLDER_ID_BRUTA e permiss√µes. Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Falha no processamento da planilha bruta.")

# ========================================================
# 3) NORMALIZA√á√ÉO DE NOMES DE COLUNA - MANTIDO COM MELHORIAS DE TRY/EXCEPT
# ========================================================
_BANNER("3) NORMALIZA√á√ÉO DE NOMES DE COLUNA")

# Fun√ß√£o normalizar_nome_coluna j√° estava bem definida.
def normalizar_nome_coluna(col: str) -> str:
    if col is None:
        return ""
    col = unicodedata.normalize("NFKD", str(col)).encode("ASCII", "ignore").decode("utf-8")
    col = col.lower()
    col = re.sub(r"[^a-z0-9]+", "_", col)
    return re.sub(r"_+", "_", col).strip("_")

try:
    df.columns = [normalizar_nome_coluna(c) for c in df.columns]
    print("‚úÖ Cabe√ßalhos normalizados:", list(df.columns))
    logging.info(f"Cabe√ßalhos normalizados: {list(df.columns)}")

    # Padroniza a coluna 'protocolo' consistentemente (strip + upper)
    def normalize_protocolo_col(df_local: pd.DataFrame, col: str = "protocolo") -> pd.DataFrame:
        if col in df_local.columns:
            df_local[col] = df_local[col].astype(str).str.strip().str.upper()
        else:
            logging.warning(f"‚ö†Ô∏è Coluna '{col}' n√£o encontrada ap√≥s normaliza√ß√£o de protocolo!")
        return df_local

    df = normalize_protocolo_col(df, "protocolo")
    logging.info("Coluna 'protocolo' padronizada.")
except Exception as e:
    logging.critical(f"‚ùå Erro na normaliza√ß√£o de nomes de coluna ou padroniza√ß√£o de protocolo. Erro: {e}. O pipeline ser√° encerrado.", exc_info=True)
    raise SystemExit("Erro cr√≠tico: Falha na normaliza√ß√£o de dados.")

# ========================================================
# 4) FUN√á√ïES AUXILIARES (codifica√ß√£o / datas / post em lotes) - MANTIDO
# ========================================================
_BANNER("4) AUXILIARES (codifica√ß√£o, datas, lotes)")

def _clean_whitespace(v) -> str:
    """NOVA FUN√á√ÉO: Limpa apenas espa√ßos extras (in√≠cio, fim e m√∫ltiplos)
       mas PRESERVA acentua√ß√£o e capitaliza√ß√£o."""
    if v is None:
        return ""
    s = str(v).strip()
    s = re.sub(r"\s+", " ", s)
    return s

def _canon_txt(v) -> str:
    """
    Fun√ß√£o de canoniza√ß√£o de texto: converte para string, remove acentos,
    converte para min√∫sculas e limpa espa√ßos.
    """
    if v is None:
        return ""
    s = unicodedata.normalize("NFKD", str(v))
    s = "".join(c for c in s if not unicodedata.combining(c))
    s = s.lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s

def _canon_txt_preserve_case(v) -> str:
    """
    NOVA VERS√ÉO: Canoniza texto (remove acentos, limpa espa√ßos),
    mas PRESERVA a capitaliza√ß√£o original.
    """
    if v is None:
        return ""
    s = unicodedata.normalize("NFKD", str(v))
    s = "".join(c for c in s if not unicodedata.combining(c))
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s

def _to_proper_case_pt(text: str) -> str:
    """
    Converte uma string para o formato 'Title Case' apropriado para o portugu√™s.
    """
    if not isinstance(text, str) or not text.strip():
        return text
    conectivos = ['de', 'da', 'do', 'dos', 'das', 'e', 'a', 'o', 'em']
    palavras = text.lower().split()
    palavras_capitalizadas = []
    for i, palavra in enumerate(palavras):
        if i == 0 or palavra not in conectivos:
            palavras_capitalizadas.append(palavra.capitalize())
        else:
            palavras_capitalizadas.append(palavra)
    return ' '.join(palavras_capitalizadas)

def _to_ddmmaa_text(series: pd.Series) -> pd.Series:
    EXCEL_BASE = pd.Timestamp("1899-12-30")
    def _one(v):
        if pd.isna(v): return None
        if isinstance(v, (pd.Timestamp, np.datetime64)):
            dt = pd.to_datetime(v, errors="coerce")
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else None
        s = str(v).strip()
        if s == "": return None
        s2 = s.replace("T", " ").replace("Z", "")
        s2 = re.sub(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$", "", s2).strip()
        if re.match(r"^\d{4}-\d{2}-\d{2}", s2):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                dt = pd.to_datetime(s2, errors="coerce", dayfirst=False)
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s
        if re.fullmatch(r"\d{5,6}(\.\d+)?", s2):
            try: return (EXCEL_BASE + pd.to_timedelta(float(s2), "D")).strftime("%d/%m/%Y")
            except: pass
        if re.fullmatch(r"\d{13}", s2):
            dt = pd.to_datetime(int(s2), unit="ms", errors="coerce")
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s
        if re.fullmatch(r"\d{10}(\.\d+)?", s2):
            dt = pd.to_datetime(float(s2), unit="s", errors="coerce")
            return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s
        for fmt in ["%d/%m/%Y %H:%M:%S","%d/%m/%Y %H:%M","%d/%m/%Y", "%d/%m/%y %H:%M:%S","%d/%m/%y %H:%M","%d/%m/%y", "%Y-%m-%d %H:%M:%S","%Y-%m-%d %H:%M","%Y-%m-%d"]:
            try: return pd.to_datetime(s2, format=fmt).strftime("%d/%m/%Y")
            except: pass
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            dt = pd.to_datetime(s2, dayfirst=True, errors="coerce")
        return dt.strftime("%d/%m/%Y") if pd.notna(dt) else s
    return series.apply(_one).astype("object")

def _conclusao_strict(series: pd.Series) -> pd.Series:
    s = pd.Series(series, dtype="object").astype(str).str.strip()
    s_cf = s.str.casefold()
    invalid = {"n√£o informado","na","n/a","n\\a","nan","null","none","","-","--", "outro","outros","nat","sem informa√ß√£o","sem informacao"}
    out = s.copy()
    mask_invalid = s_cf.isin(invalid)
    out.loc[mask_invalid] = "N√£o conclu√≠do"
    rest_idx = out.index[~mask_invalid]
    if len(rest_idx) > 0:
        s_rest = s.loc[rest_idx]
        s_norm = s_rest.str.replace("T"," ").str.replace("Z","", regex=False)
        s_norm = s_norm.str.replace(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$","", regex=True).str.strip()
        iso_mask = s_norm.str.match(r"^\d{4}-\d{2}-\d{2}")
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            dt = pd.Series(pd.NaT, index=rest_idx, dtype="datetime64[ns]")
            iso_idx = iso_mask[iso_mask].index
            if len(iso_idx) > 0:
                dt.loc[iso_idx] = pd.to_datetime(s_rest.loc[iso_idx], errors="coerce", dayfirst=False)
            non_iso_idx = iso_mask[~iso_mask].index
            if len(non_iso_idx) > 0:
                dt.loc[non_iso_idx] = pd.to_datetime(s_rest.loc[non_iso_idx], errors="coerce", dayfirst=True)
        good_idx = dt.index[dt.notna()]
        if len(good_idx) > 0:
            out.loc[good_idx] = dt.loc[good_idx].dt.strftime("%d/%m/%Y")
    return out

def _parse_dt_cmp(series: pd.Series) -> pd.Series:
    EXCEL_BASE = pd.Timestamp("1899-12-30")
    def _one(v):
        if pd.isna(v): return pd.NaT
        s = str(v).strip()
        if s == "": return pd.NaT
        s2 = s.replace("T"," ").replace("Z","")
        s2 = re.sub(r"([+-]\d{2}:?\d{2}|[+-]\d{2}| UTC)$","",s2).strip()
        if re.fullmatch(r"\d{5,6}(\.\d+)?", s2):
            try: return EXCEL_BASE + pd.to_timedelta(float(s2), "D")
            except: return pd.NaT
        if re.fullmatch(r"\d{13}", s2):
            return pd.to_datetime(int(s2), unit="ms", errors="coerce")
        if re.fullmatch(r"\d{10}(\.\d+)?", s2):
            return pd.to_datetime(float(s2), unit="s", errors="coerce")
        for fmt in ["%d/%m/%Y %H:%M:%S","%d/%m/%Y %H:%M","%d/%m/%Y", "%d/%m/%y %H:%M:%S","%d/%m/%y %H:%M","%d/%m/%y", "%Y-%m-%d %H:%M:%S","%Y-%m-%d %H:%M","%Y-%m-%d"]:
            try: return pd.to_datetime(s2, format=fmt)
            except: pass
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            return pd.to_datetime(s2, dayfirst=True, errors="coerce")
    return series.apply(_one)

def _is_nao_ha_dados(v) -> bool:
    if v is None: return False
    s = str(v).strip()
    if s == "": return False
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = re.sub(r"\s+", " ", s).strip().casefold()
    return s == "nao ha dados"

def _is_concluida(v) -> bool:
    if pd.isna(v): return False
    s = str(v).strip()
    if s == "": return False
    s = "".join(ch for ch in unicodedata.normalize("NFD", s) if unicodedata.category(ch) != "Mn")
    s = re.sub(r"[^A-Za-z]+", " ", s).strip().casefold()
    return s == "concluida"

def _looks_like_demanda_concluida(v) -> bool:
    if pd.isna(v): return False
    s = str(v).strip()
    if s == "": return False
    s = unicodedata.normalize("NFD", s)
    s = s.replace("ÔøΩ", "i").replace("?", "i")
    s = re.sub(r"i{2,}", "i", s, flags=re.IGNORECASE)
    s = re.sub(r"[^A-Za-z]+", " ", s).strip().casefold()
    return s == "demanda concluida"

def _canon_prazo_restante(v):
    if pd.isna(v): return v
    if isinstance(v, (int, float)) and not pd.isna(v):
        return v
    s = _canon_txt(v)
    if s == "": return s
    s_clean = s.strip()
    if _looks_like_demanda_concluida(s_clean):
        return "Demanda Conclu√≠da"
    return s_clean

# --- L√ìGICA DE MAPEAMENTO DE √ìRG√ÉOS MOVIDA PARA C√Å (ESCOPO GLOBAL) ---

def _norm_tema(s):
    if pd.isna(s): return ""
    s = str(s).strip().lower()
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    return re.sub(r"\s+", " ", s)

def _div_temas(v, seps=(",", ";", "|", "/")):
    if pd.isna(v): return []
    t = str(v)
    for s in seps: t = t.replace(s, ",")
    partes = [p.strip() for p in t.split(",") if p.strip()]
    return partes if partes else [str(v).strip()]

MAP_TEMA_PARA_ORGAO = {
    "Administra√ß√£o P√∫blica":"Secretaria de Administra√ß√£o","Agricultura":"Secretaria de Obras e Agricultura",
    "Assist√™ncia Social e Direitos Humanos":"Secretaria de Assist√™ncia Social e Direitos Humanos",
    "Assuntos Jur√≠dicos":"Procuradoria Geral","Comunica√ß√£o Social":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas",
    "Controle Governamental":"Secretaria de Controle Interno","Crian√ßa, Adolescente e Idoso":"Secretaria de Assist√™ncia Social e Direitos Humanos",
    "Cultura e Turismo":"Secretaria de Cultura e Turismo","Defesa Civil":"Secretaria de Defesa Civil",
    "Direitos √† Pessoa com Defici√™ncia":"Secretaria de Assist√™ncia Social e Direitos Humanos",
    "Direitos e Vantagens do Servidor":"Secretaria de Administra√ß√£o","Educa√ß√£o":"Secretaria de Educa√ß√£o",
    "Empresas e Legaliza√ß√µes":"Secretaria de Fazenda","Esporte e Lazer":"Secretaria de Esporte e Lazer",
    "Fiscaliza√ß√£o e tributos":"Secretaria de Fazenda","Fiscaliza√ß√£o Urbana, Regulariza√ß√£o e Registro de Im√≥veis":"Secretaria de Urbanismo e Habita√ß√£o",
    "FUNDEC":"FUNDEC","Governan√ßa":"Secretaria de Governo","Governo Municipal e Enterro Gratuito":"Secretaria de Governo",
    "Habita√ß√£o":"Secretaria de Urbanismo e Habita√ß√£o","Inclus√£o e Acessibilidade":"Secretaria de Gest√£o, Inclus√£o e Mulher",
    "Meio Ambiente":"Secretaria de Meio Ambiente",
    "Meio Ambiente (Polui√ß√£o Sonora, √Årvores, Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.)":"Secretaria de Meio Ambiente",
    "Ass√©dio":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas","Obras P√∫blicas":"Secretaria de Obras e Agricultura",
    "Obras, Limpeza Urbana e Bra√ßo de Luz":"Secretaria de Obras e Agricultura","Prote√ß√£o Animal":"Secretaria de Prote√ß√£o Animal",
    "Sa√∫de":"Secretaria de Sa√∫de","Seguran√ßa P√∫blica":"Secretaria de Seguran√ßa P√∫blica",
    "Seguran√ßa, Sinaliza√ß√£o e Multas":"Secretaria de Seguran√ßa P√∫blica",
    "Trabalho, Emprego e Renda":"Secretaria de Trabalho, Emprego e Renda",
    "Transportes e Servi√ßos P√∫blicos":"Secretaria de Transportes e Servi√ßos P√∫blicos",
    "Transportes, Servi√ßos P√∫blicos e Troca de L√¢mpadas":"Secretaria de Transportes e Servi√ßos P√∫blicos",
    "Urbanismo":"Secretaria de Urbanismo e Habita√ß√£o",
    "Vetores e Zoonoses (Combate √† Dengue, Controle de Pragas, Cria√ß√£o Irregular de Animais e etc.)":"Secretaria de Sa√∫de",
    "Vigil√¢ncia Sanit√°ria":"Secretaria de Sa√∫de",
    "Obras":"Secretaria de Obras e Agricultura","Trabalho":"Secretaria de Trabalho, Emprego e Renda",
    "Seguran√ßa":"Secretaria de Seguran√ßa P√∫blica","Servi√ßos P√∫blicos e Troca de L√¢mpadas":"Secretaria de Transportes e Servi√ßos P√∫blicos",
    "√Årvores":"Secretaria de Meio Ambiente","Controle de Pragas":"Secretaria de Sa√∫de","Cria√ß√£o Irregular de Animais":"Secretaria de Sa√∫de",
    "Adolescente e Idoso":"Secretaria de Assist√™ncia Social e Direitos Humanos","Crian√ßa":"Secretaria de Assist√™ncia Social e Direitos Humanos",
    "Emprego e Renda":"Secretaria de Trabalho, Emprego e Renda","Fiscaliza√ß√£o Urbana":"Secretaria de Urbanismo e Habita√ß√£o",
    "Regulariza√ß√£o e Registro de Im√≥veis":"Secretaria de Urbanismo e Habita√ß√£o","Limpeza Urbana e Bra√ßo de Luz":"Secretaria de Obras e Agricultura",
    "Meio Ambiente (Polui√ß√£o Sonora)":"Secretaria de Meio Ambiente","Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.":"Secretaria de Meio Ambiente",
    "Vetores e Zoonoses (Combate √† Dengue)":"Secretaria de Sa√∫de",
    "Cria√ß√£o Irregular de Animais e etc.)":"Secretaria de Sa√∫de","Licen√ßas e Fiscaliza√ß√µes Ambientais e etc.)":"Secretaria de Meio Ambiente",
    "Meio Ambiente (Polui√ß√£o Sonora":"Secretaria de Meio Ambiente","N√£o se aplica":"Secretaria de Comunica√ß√£o Social e Rela√ß√µes P√∫blicas",
    "Sinaliza√ß√£o e Multas":"Secretaria de Seguran√ßa P√∫blica","Transportes":"Secretaria de Transportes e Servi√ßos P√∫blicos",
    "Vetores e Zoonoses (Combate √† Dengue":"Secretaria de Sa√∫de",
}
MAP_EXACT_ORGAOS = { _norm_tema(k): v for k, v in MAP_TEMA_PARA_ORGAO.items() }

def mapear_orgao_exato(celula_tema):
    orgs = []
    tema_as_str = str(celula_tema) if pd.notna(celula_tema) else ""
    for t in _div_temas(tema_as_str):
        t_norm = _norm_tema(t)
        if t_norm and t_norm in MAP_EXACT_ORGAOS:
            orgs.append(MAP_EXACT_ORGAOS[t_norm])
    return " | ".join(dict.fromkeys(o.strip() for o in orgs if o and str(o).strip())) or None
    
# ============================================= 
# 5) COLETA DE PROTOCOLOS EXISTENTES NA PLANILHA TRATADA - MANTIDO (com ajuste de logging)
# =============================================
_BANNER("5) COLETA DE PROTOCOLOS EXISTENTES NA PLANILHA TRATADA")

try:
    # ---------- CONSTANTES / IDs ----------
    # Defina PLANILHA_TRATADA_ID no topo do arquivo ou altere aqui diretamente:
    PLANILHA_TRATADA_ID = "1SmO5yTD5B6fN_gT-7m1wosP_sbzmtd0agTC-LNCnX9Y"  # <-- coloque aqui o ID CORRETO da planilha tratada fixa

    # ---------- ABRE A PLANILHA TRATADA (√∫nica fonte) ----------
    planilha_tratada_gs = gc.open_by_key(PLANILHA_TRATADA_ID) # Renomeado para evitar conflito com df_tratada
    aba_tratada = planilha_tratada_gs.sheet1
    logging.info(f"Planilha tratada '{PLANILHA_TRATADA_ID}' aberta.")

    df_tratada = pd.DataFrame(aba_tratada.get_all_records())
    df_tratada.columns = [normalizar_nome_coluna(c) for c in df_tratada.columns]

    df_tratada = normalize_protocolo_col(df_tratada, "protocolo")
    protocolos_existentes_set = set(df_tratada["protocolo"].astype(str).tolist())

    # ---------- L√ä A √öLTIMA PLANILHA BRUTA (reutiliza FOLDER_ID_BRUTA e helper) ----------
    # IMPORTANTE: get_latest_spreadsheet_df deve existir (Item 2)
    if 'FOLDER_ID_BRUTA' in globals():
        folder_id = FOLDER_ID_BRUTA
    else:
        folder_id = "1qXj9eGauvOREKVgRPOfKjRlLSKhefXI5"  # fallback se n√£o definido acima

    latest_file_id, latest_file_name, df_bruta = get_latest_spreadsheet_df(folder_id, gc, drive_service)

    # Normaliza colunas e protocolo da bruta
    df_bruta.columns = [normalizar_nome_coluna(c) for c in df_bruta.columns]
    df_bruta = normalize_protocolo_col(df_bruta, "protocolo")

    # Marca novos protocolos (compara√ß√£o com o conjunto da tratada)
    df_bruta["eh_novo"] = ~df_bruta["protocolo"].isin(protocolos_existentes_set)
    novos_protos = df_bruta.loc[df_bruta["eh_novo"], "protocolo"].tolist()

    print(f"üîë Protocolos j√° na planilha tratada: {len(protocolos_existentes_set)}")
    print(f"üÜï Protocolos detectados como novos: {len(novos_protos)}")
    logging.info(f"Protocolos j√° na planilha tratada: {len(protocolos_existentes_set)}")
    logging.info(f"Protocolos detectados como novos: {novos_protos[:50]}")

    # Log dos existentes que n√£o ser√£o enviados
    nao_enviados = df_bruta.loc[~df_bruta["eh_novo"], "protocolo"].tolist()
    print(f"‚ö†Ô∏è Protocolos existentes que n√£o ser√£o enviados (n√£o novos): {len(nao_enviados)}")
    logging.info(f"Protocolos existentes que n√£o ser√£o enviados: {nao_enviados[:50]}")

    # Verifica√ß√£o final
    if df_bruta.empty:
        raise Exception("A planilha bruta mais recente est√° vazia ou n√£o p√¥de ser lida.")

    # Substitui df pelo df_bruta "oficial" para manter compatibilidade posterior
    df = df_bruta.copy()

except Exception as e:
    print(f"‚ö†Ô∏è Erro ao carregar planilhas: {e}")
    logging.warning(f"Erro ao carregar planilhas: {e}")
    df_tratada = pd.DataFrame()
    protocolos_existentes_set = set()
    df = pd.DataFrame()
    df["eh_novo"] = True
    novos_protos = []
    nao_enviados = []

# ======================================================================= #
# ========= IN√çCIO DO BLOCO TEMPOR√ÅRIO DE BACKFILL - INSIRA AQUI ========= #
# ======================================================================= #

# ===================================================================================
# 5.5) BACKFILL TEMPOR√ÅRIO DE 'orgaos' (REMOVER AP√ìS 1¬™ EXECU√á√ÉO)
# ===================================================================================
_BANNER("5.5) BACKFILL TEMPOR√ÅRIO DE 'responsavel' (REMOVER DEPOIS)")
print(" Executando backfill para padronizar TODA a coluna 'responsavel'...")
logging.info("INICIANDO: Backfill tempor√°rio da coluna 'responsavel'.")

try:
    _SUB("Carregando base tratada hist√≥rica para 'responsavel'...")
    df_tratada_hist = pd.DataFrame(aba_tratada.get_all_records())
    df_tratada_hist.columns = [normalizar_nome_coluna(c) for c in df_tratada_hist.columns]
    
    if 'protocolo' not in df_tratada_hist.columns or 'responsavel' not in df_tratada_hist.columns:
        raise ValueError("Colunas 'protocolo' ou 'responsavel' n√£o encontradas na planilha tratada.")

    _SUB("Aplicando a nova l√≥gica de padroniza√ß√£o a todos os dados hist√≥ricos...")
    
    # Cria uma nova coluna com os valores corrigidos, aplicando a MESMA l√≥gica do novo Item 7.6
    df_tratada_hist['responsavel_corrigido'] = df_tratada_hist['responsavel'].astype(str).apply(_clean_whitespace)
    df_tratada_hist['responsavel_corrigido'] = df_tratada_hist['responsavel_corrigido'].str.replace(
        r"^\s*ouvidoria\s+geral\s*$", "Ouvidoria Geral", regex=True, case=False
    )
    df_tratada_hist['responsavel_corrigido'] = df_tratada_hist['responsavel_corrigido'].str.replace(
        r"^\s*ouvidoria\s+setorial\s+de\s+obras\s*$", "Ouvidoria Setorial de Obras", regex=True, case=False
    )
    df_tratada_hist['responsavel_corrigido'] = df_tratada_hist['responsavel_corrigido'].str.replace(
        r"^\s*ouvidoria\s+setorial\s+da\s+sa(u|√∫)de\s*$", "Ouvidoria Setorial da Sa√∫de", regex=True, case=False
    )
    df_tratada_hist['responsavel_corrigido'] = df_tratada_hist['responsavel_corrigido'].str.replace(
        r"^\s*cidadao\s*$", "Cidad√£o", regex=True, case=False
    )
    df_tratada_hist['responsavel_corrigido'] = df_tratada_hist['responsavel_corrigido'].str.replace(
        r"^\s*(Sim|True)\s*$", "Cidad√£o", regex=True, case=False
    )
    df_tratada_hist.loc[df_tratada_hist["responsavel_corrigido"].str.strip() == '', "responsavel_corrigido"] = "N√£o Informado"

    _SUB("Identificando e atualizando as diverg√™ncias...")
    df_para_atualizar = df_tratada_hist[df_tratada_hist['responsavel'] != df_tratada_hist['responsavel_corrigido']]
    
    if not df_para_atualizar.empty:
        print(f" Encontradas {len(df_para_atualizar)} linhas em 'responsavel' para corrigir.")
        TARGET_COL_INDEX = df_tratada_hist.columns.get_loc('responsavel') + 1
        protocolos_na_sheet = aba_tratada.col_values(1)
        protocolo_para_linha = {proto: i + 1 for i, proto in enumerate(protocolos_na_sheet)}
        
        cells_to_update = [
            gspread.Cell(row=protocolo_para_linha[row['protocolo']], col=TARGET_COL_INDEX, value=str(row['responsavel_corrigido']))
            for _, row in df_para_atualizar.iterrows() if row['protocolo'] in protocolo_para_linha
        ]
        
        if cells_to_update:
            aba_tratada.update_cells(cells_to_update, value_input_option='USER_ENTERED')
            print(f"‚úÖ Backfill da coluna 'responsavel' conclu√≠do. {len(cells_to_update)} c√©lulas foram corrigidas.")
    else:
        print("‚úÖ Nenhuma diverg√™ncia hist√≥rica encontrada na coluna 'responsavel'.")

except Exception as e:
    print(f"‚ùå Erro cr√≠tico durante o backfill de 'responsavel': {e}")


# ======================================================================= #
# =================== FIM DO BLOCO TEMPOR√ÅRIO DE BACKFILL ================= #
# ======================================================================= #

# ========================================================
# 6) LIMPEZA B√ÅSICA + RECORTE PARA NOVOS POR PROTOCOLO
# ========================================================
print("üßπ Limpando e identificando novos protocolos...")
df_tratada_protocolos = df_tratada["protocolo"].astype(str).str.strip().tolist()
df["protocolo"] = df["protocolo"].astype(str).str.strip()

df["eh_novo"] = ~df["protocolo"].isin(df_tratada_protocolos)
novos = df[df["eh_novo"] == True]
existentes = df[df["eh_novo"] == False]

print(f"üÜï Novos protocolos: {len(novos)}")
print(f"üîÑ Protocolos existentes: {len(existentes)}")
logging.info(f"Novos protocolos: {len(novos)}, Existentes: {len(existentes)}")

# ========================================================
# 7) TRATAMENTOS E ATUALIZA√á√ÉO DE DADOS (somente NOVOS)
# ========================================================
_BANNER("7) TRATAMENTOS (somente NOVOS)")

# Seleciona apenas os protocolos novos identificados no Item 5
df_novos = df[df["eh_novo"] == True].copy()

if df_novos.empty:
    logging.info("Nenhum protocolo novo para tratamento.")
else:
    logging.info(f"Aplicando tratamentos em {len(df_novos)} protocolos novos. Shape inicial: {df_novos.shape}")

def _tratar_full(df_in: pd.DataFrame) -> pd.DataFrame:
    df_loc = df_in.copy()
    logging.debug(f"Iniciando _tratar_full com DataFrame de shape: {df_loc.shape}")

    # 7.1 Tema/Assunto ‚Äî mant√©m 'n√£o se aplica' ‚Üí 'Ass√©dio'
    try:
        if "tema" in df_loc.columns and "assunto" in df_loc.columns:
            tema_tmp = df_loc["tema"].astype(str).str.strip().str.casefold()
            assunto_tmp = df_loc["assunto"].astype(str).str.strip().str.casefold()
            valores_assunto = ["outro", "outros", "na", "n/a", "n\\a", ""]
            cond_42 = (tema_tmp == "n√£o se aplica") & (assunto_tmp.isin(valores_assunto))
            if int(cond_42.sum()):
                df_loc.loc[cond_42, "assunto"] = "Ass√©dio"
                logging.info(f"Tratamento 7.1 (Assunto) aplicado a {int(cond_42.sum())} linhas.")
            cond_41 = (tema_tmp == "n√£o se aplica")
            if int(cond_41.sum()):
                df_loc.loc[cond_41, "tema"] = "Ass√©dio"
                logging.info(f"Tratamento 7.1 (Tema) aplicado a {int(cond_41.sum())} linhas.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.1 (Tema/Assunto): {e}", exc_info=True)

    # 7.2 Data da conclus√£o ‚Üí texto "DD/MM/AA" ou "N√£o conclu√≠do"
    try:
        if "data_da_conclusao" in df_loc.columns:
            df_loc["data_da_conclusao"] = _conclusao_strict(df_loc["data_da_conclusao"])
            df_loc["data_da_conclusao"] = df_loc["data_da_conclusao"].apply(
                lambda x: x if pd.notna(x) and str(x).strip().lower() not in ["na", "nan", "n/a", ""] else "N√£o conclu√≠do"
            )
            logging.info("Tratamento 7.2 (Data da Conclus√£o) aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.2 (Data da Conclus√£o): {e}", exc_info=True)

    # 7.3 Unidades de sa√∫de (capitaliza e trata ‚Äúsem informa√ß√£o‚Äù)
    try:
        for col in df_loc.columns:
            if "unidade" in col and "saude" in col:
                linhas_alteradas = (df_loc[col].astype(str).str.strip().str.lower() == "sem informa√ß√£o").sum()
                df_loc[col] = (
                    df_loc[col].astype(str).str.strip().str.lower()
                    .replace("sem informa√ß√£o", "N√£o √© uma Unidade de Sa√∫de")
                    .str.capitalize()
                )
                if linhas_alteradas > 0:
                    logging.info(f"Tratamento 7.3 (Unidades de Sa√∫de) aplicado na coluna '{col}' para {linhas_alteradas} linhas.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.3 (Unidades de Sa√∫de): {e}", exc_info=True)

    # 7.4 √ìrg√£os por tema ‚Äî L√ìGICA SIMPLIFICADA E CORRIGIDA
    try:
        # Passo A: Cria a coluna 'orgaos' chamando a fun√ß√£o global
        if "tema" in df_loc.columns:
            df_loc["tema"] = df_loc["tema"].astype(str)
            df_loc["orgaos"] = df_loc["tema"].apply(mapear_orgao_exato)
        else:
            df_loc["orgaos"] = None
        
        # Passo B: Aplica o fallback para valores nulos/vazios
        fallback_value = "Secretaria Municipal de Comunica√ß√£o e Rela√ß√µes P√∫blicas"
        df_loc["orgaos"].fillna(fallback_value, inplace=True)
        df_loc.loc[df_loc["orgaos"].str.strip() == '', "orgaos"] = fallback_value
        
        # Passo C: Executa a sequ√™ncia de limpeza e capitaliza√ß√£o
        df_loc["orgaos"] = df_loc["orgaos"].apply(_clean_whitespace).astype(str)
        df_loc["orgaos"] = df_loc["orgaos"].apply(_to_proper_case_pt)
        logging.info("Tratamento 7.4 (Limpeza e Capitaliza√ß√£o com Acentos) aplicado a 'orgaos'.")

    except Exception as e:
        logging.error(f"Erro no tratamento 7.4 (√ìrg√£os por tema): {e}", exc_info=True)

    # 7.5 Padroniza√ß√£o 'servidor' (dicion√°rio completo)
    try:
        if "servidor" in df_loc.columns:
            dicionario_servidor = {
                "Camila do Lago Marins": "Camila Marins", "Camila Marins": "Camila Marins",
                "Dhayane Cristina Pinho de Almeida": "Dhayane Cristina Pinho de Almeida", "Dhayane Pinho": "Dhayane Cristina Pinho de Almeida",
                "Joana Darc Salles Ferreira": "Joana Darc Salles Ferreira", "Joana Salles": "Joana Darc Salles Ferreira",
                "Lucia Helena Tinoco Pacehco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
                "Lucia  Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "L√∫cia  Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
                "L√∫cia Helena Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helenba Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
                "Rafaella Marques Gomes Santos": "Rafaella Marques Gomes Santos",
                "Roilene Pereira da Silva": "Rosilene Pereira da Silva", "Rosilene Pereira da Silva": "Rosilene Pereira da Silva",
                "Stephanie dos Santos Silva": "Stephanie dos Santos Silva", "Stephanie Santos": "Stephanie dos Santos Silva",
                "St√©phanie Santos": "Stephanie dos Santos Silva", "St√©phaniesantos": "Stephanie dos Santos Silva",
                "Stpehanie Santos": "Stephanie dos Santos Silva",
                "Anne Beatriz da Silva": "Anne Beatriz da Silva Rodrigues", "Bruna Maria ( Coordenadora)": "Cidad√£o",
                "Isabel": "Cidad√£o", "Gabriela da Silva Rozi": "Cidad√£o", "Lana Carolina Mesquita de Andrade": "Cidad√£o",
                "L√≠via Cavalcante": "L√≠via Kathleen Cavalcante Patriota Leite", "L√≠via Kathleen Cavalcante Patriota Leite": "L√≠via Kathleen Cavalcante Patriota Leite",
                "Lucia Helena": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helena Tinoco": "L√∫cia Helena Tinoco Pacheco Varella",
                "Lucia Helena Tinoco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helen Tinoco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
                "Lucia Helan Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella", "Lucia Helena  Tinoco Pacheco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
                "Lucia Helena Tinoco Pachewco Varella": "L√∫cia Helena Tinoco Pacheco Varella",
                "Mery": "Cidad√£o", "Ouvidoria Geral (Adm)": "Cidad√£o", "Rafaella Marques": "Rafaella Marques Gomes Santos",
                "Ronaldo de Oliveira Brand√£o": "Cidad√£o", "S√©phanie Santos": "Stephanie dos Santos Silva",
                "Shirley Santana": "Cidad√£o", "St√©panie Santos": "Stephanie dos Santos Silva",
                "St√©phanie  Santos": "Stephanie dos Santos Silva", "St√©phanie Santos": "Stephanie dos Santos Silva",
                "Stephanie dos Santos": "Stephanie dos Santos Silva", "St√©phanie Santoa": "Stephanie dos Santos Silva",
                "Stephanie Santos": "Stephanie dos Santos Silva", "Stephanie dos Santos": "Stephanie dos Santos Silva",
                "Stephanie Santos": "Stephanie dos Santos Silva", "Thamires Manh√£es": "Cidad√£o"
            }
            _orig = df_loc["servidor"].astype(str).str.strip()
            df_loc["servidor"] = _orig.map(dicionario_servidor).fillna(_orig)
            logging.info("Tratamento 7.5 (Padroniza√ß√£o 'servidor') aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.5 (Padroniza√ß√£o 'servidor'): {e}", exc_info=True)

     # 7.6 Responsavel - L√ìGICA CORRIGIDA E UNIFICADA
    try:
        if "responsavel" in df_loc.columns:
            # Garante que a coluna √© string e limpa espa√ßos extras
            df_loc["responsavel"] = df_loc["responsavel"].astype(str).apply(_clean_whitespace)

            # Aplica padroniza√ß√µes robustas com regex (case-insensitive)
            # Corrige "Ouvidoria Geral" (com ou sem acento, mai√∫sculo ou min√∫sculo)
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*ouvidoria\s+geral\s*$", "Ouvidoria Geral", regex=True, case=False
            )
            # Corrige "Ouvidoria Setorial de Obras"
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*ouvidoria\s+setorial\s+de\s+obras\s*$", "Ouvidoria Setorial de Obras", regex=True, case=False
            )
            # Corrige "Ouvidoria Setorial da Saude" (com ou sem '√∫')
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*ouvidoria\s+setorial\s+da\s+sa(u|√∫)de\s*$", "Ouvidoria Setorial da Sa√∫de", regex=True, case=False
            )
            # Corrige "Cidadao" (com ou sem '√£')
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*cidadao\s*$", "Cidad√£o", regex=True, case=False
            )
            # Corrige "Sim" ou "True"
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*(Sim|True)\s*$", "Cidad√£o", regex=True, case=False
            )
            
            # Trata valores vazios como "N√£o Informado"
            df_loc.loc[df_loc["responsavel"].str.strip() == '', "responsavel"] = "N√£o Informado"

            logging.info("Tratamento 7.6 (Unificado para 'responsavel') aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.6 (Responsavel): {e}", exc_info=True)

    # 7.7 Datas e tipos
    try:
        if "data_da_criacao" in df_loc.columns:
            df_loc["data_da_criacao"] = _to_ddmmaa_text(df_loc["data_da_criacao"]).astype(str)
            logging.info("Tratamento 7.7 (data_da_criacao) aplicado.")
        if "status_demanda" in df_loc.columns:
            df_loc["status_demanda"] = df_loc["status_demanda"].astype(str)
            logging.info("Tratamento 7.7 (status_demanda) tipo aplicado.")
        if "data_da_conclusao" in df_loc.columns:
            df_loc["data_da_conclusao"] = _conclusao_strict(df_loc["data_da_conclusao"])
            logging.info("Tratamento 7.7 (data_da_conclusao) strict aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.7 (Datas e Tipos): {e}", exc_info=True)

    # 7.8 Regra de ouro: se CONCLU√çDA => 'prazo_restante' = 'Demanda Conclu√≠da'
    try:
        if "status_demanda" in df_loc.columns and "prazo_restante" in df_loc.columns:
            mask_conc = df_loc["status_demanda"].map(_is_concluida)
            if mask_conc.any():
                df_loc.loc[mask_conc, "prazo_restante"] = "Demanda Conclu√≠da"
                logging.info(f"Tratamento 7.8 (prazo_restante p/ conclu√≠da) aplicado para {mask_conc.sum()} linhas.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.8 (Prazo Restante): {e}", exc_info=True)

    # 7.9 Padroniza√ß√£o adicional para 'responsavel' (Ouvidorias)
    try:
        if "responsavel" in df_loc.columns:
            df_loc["responsavel"] = df_loc["responsavel"].astype(str)
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*ouvidoria\s+geral\s*$", "Ouvidoria Geral", regex=True, case=False
            )
            df_loc["responsavel"] = df_loc["responsavel"].str.replace(
                r"^\s*ouvidoria\s+setorial\s+de\s+obras\s*$", "Ouvidoria Setorial de Obras", regex=True, case=False
            )
            logging.info("Tratamento 7.9 (Padroniza√ß√£o de Ouvidorias em 'responsavel') aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.9 (Padroniza√ß√£o de Ouvidorias): {e}", exc_info=True)

    # 7.10 Padroniza√ß√£o da coluna 'canal'
    try:
        if "canal" in df_loc.columns:
            df_loc["canal"] = df_loc["canal"].astype(str)
            df_loc["canal"] = df_loc["canal"].str.replace(
                r"^\s*(Colab Gov|Portal Cidad√£o)\s*$", "Aplicativo Colab", regex=True, case=False
            )
            logging.info("Tratamento 7.10 (Padroniza√ß√£o de 'canal') aplicado.")
    except Exception as e:
        logging.error(f"Erro no tratamento 7.10 (Padroniza√ß√£o de 'canal'): {e}", exc_info=True)

    logging.debug(f"Finalizando _tratar_full. Shape final: {df_loc.shape}")
    return df_loc

# Aplica o tratamento aos novos protocolos
try:
    if not df_novos.empty:
        df_novos = _tratar_full(df_novos)
        logging.info(f"Tratamento full aplicado a {len(df_novos)} protocolos novos.")
    else:
        logging.info("df_novos est√° vazio, pulando _tratar_full.")
except Exception as e:
    logging.critical(f"Erro CR√çTICO ao aplicar _tratar_full em df_novos: {e}", exc_info=True)
    # Dependendo da severidade, voc√™ pode querer levantar a exce√ß√£o ou parar o pipeline.
    raise

# QA p√≥s _tratar_full global para df_novos
if not df_novos.empty:
    for col in ['orgaos', 'responsavel', 'status_demanda', 'data_da_conclusao']:
        if col in df_novos.columns:
            empty_count = df_novos[col].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']).sum()
            if empty_count > 0:
                logging.warning(f"QA P√≥s-Tratamento (df_novos): Coluna '{col}' cont√©m {empty_count} valores vazios/inv√°lidos/n√£o informados. Exemplos: {df_novos.loc[df_novos[col].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']), col].unique()[:5].tolist()}")
            
            # Verifica√ß√£o de tipos para garantir que s√£o strings
            if not pd.api.types.is_string_dtype(df_novos[col]):
                logging.error(f"QA P√≥s-Tratamento (df_novos): Coluna '{col}' n√£o √© do tipo string ap√≥s tratamento. Tipo atual: {df_novos[col].dtype}. Convertendo para string.")
                df_novos[col] = df_novos[col].astype(str)
                
# ========================================================
# 8) ATUALIZA√á√ÉO NA PLANILHA TRATADA ‚Äî APENAS NOVOS
# ========================================================
_BANNER("8) ATUALIZA√á√ÉO NA PLANILHA TRATADA ‚Äî APENAS NOVOS")

# ----------------------------------------------------------
# GARANTE QUE df_bruta EXISTE E TEM A COLUNA 'protocolo'
# ----------------------------------------------------------
try:
    if 'df_bruta' not in globals() or df_bruta.empty:
        raise SystemExit("‚ùå df_bruta n√£o est√° definido ou est√° vazio. Carregue a base bruta antes do Item 8.")
    logging.info(f"df_bruta presente e com shape: {df_bruta.shape}")

    df_bruta.columns = [normalizar_nome_coluna(c) for c in df_bruta.columns] # Garante que est√° normalizado
    df_bruta = normalize_protocolo_col(df_bruta, "protocolo") # Garante que protocolo est√° padronizado
    logging.debug("Colunas e protocolos de df_bruta normalizados.")
except Exception as e:
    logging.critical(f"Erro na checagem inicial de df_bruta no Item 8: {e}", exc_info=True)
    raise

# ----------------------------------------------------------
# CARREGA PLANILHA TRATADA E OBT√âM PROTOCOLOS EXISTENTES
# ----------------------------------------------------------

try:
    if 'client' not in globals() or client is None:
        raise SystemExit("‚ùå Cliente gspread n√£o autenticado. Verifique Item 1.")

    PLANILHA_TRATADA_ID = "1SmO5yTD5B6fN_gT-7m1wosP_sbzmtd0agTC-LNCnX9Y"
    planilha_tratada_gs = client.open_by_key(PLANILHA_TRATADA_ID)
    aba_tratada = planilha_tratada_gs.sheet1
    logging.info(f"Planilha tratada '{PLANILHA_TRATADA_ID}' aberta.")
except Exception as e:
    logging.critical(f"Erro ao abrir a planilha tratada ou autenticar no Item 8: {e}", exc_info=True)
    raise

try:
    df_tratada_existente = pd.DataFrame(aba_tratada.get_all_records())
    df_tratada_existente.columns = [normalizar_nome_coluna(c) for c in df_tratada_existente.columns]
    logging.info(f"df_tratada_existente carregado com shape: {df_tratada_existente.shape}")

    protocolos_existentes_set_final = set()
    if "protocolo" in df_tratada_existente.columns:
        df_tratada_existente["protocolo"] = df_tratada_existente["protocolo"].astype(str).str.strip().str.upper()
        protocolos_existentes_set_final = set(df_tratada_existente["protocolo"])
        logging.debug(f"Set de protocolos existentes criado com {len(protocolos_existentes_set_final)} itens.")
    else:
        logging.warning("Coluna 'protocolo' n√£o encontrada em df_tratada_existente. N√£o ser√° poss√≠vel identificar protocolos existentes.")

    cols_alvo_tratada = list(df_tratada_existente.columns)
    if df_tratada_existente.empty:
        if 'df_novos' in globals() and not df_novos.empty:
            cols_alvo_tratada = list(df_novos.columns)
            logging.info("Planilha tratada vazia, usando colunas de df_novos como refer√™ncia para o schema.")
        elif not df_bruta.empty:
            cols_alvo_tratada = list(df_bruta.columns)
            logging.info("Planilha tratada vazia e df_novos vazio, usando colunas de df_bruta como refer√™ncia para o schema.")
        else:
            logging.error("N√£o foi poss√≠vel determinar o schema da planilha tratada. df_tratada_existente, df_novos e df_bruta est√£o vazios.")
            raise ValueError("N√£o foi poss√≠vel determinar o schema da planilha tratada.")
    logging.debug(f"Colunas alvo da planilha tratada: {cols_alvo_tratada}")

except Exception as e:
    logging.critical(f"Erro ao processar df_tratada_existente ou definir schema alvo no Item 8: {e}", exc_info=True)
    raise

# ----------------------------------------------------------
# IDENTIFICA E PREPARA NOVOS PROTOCOLOS PARA ENVIO
# ----------------------------------------------------------

try:
    novos_protocolos_a_enviar = set(df_bruta["protocolo"]) - protocolos_existentes_set_final
    df_send_bruto = df_bruta[df_bruta["protocolo"].isin(novos_protocolos_a_enviar)].copy()

    if df_send_bruto.empty:
        logging.info("Nenhum protocolo novo detectado para envio. df_send ser√° um DataFrame vazio.")
        print("üßπ Nenhum protocolo novo para enviar.")
        df_send = pd.DataFrame(columns=cols_alvo_tratada) # Define df_send vazio com colunas corretas
    else:
        logging.info(f"Detectados {len(df_send_bruto)} protocolos novos para processar e enviar. Shape inicial: {df_send_bruto.shape}")
        print(f"üßπ Novos protocolos a enviar: {len(df_send_bruto)}")

        # APLICA TODOS OS TRATAMENTOS DE _tratar_full AQUI!
        df_send = _tratar_full(df_send_bruto.copy())
        logging.info(f"Fun√ß√£o _tratar_full aplicada a df_send_bruto. Shape ap√≥s tratamento: {df_send.shape}")

        # Remove colunas auxiliares que n√£o devem ser escritas no Google Sheets
        cols_to_drop = []
        if "eh_novo" in df_send.columns:
            cols_to_drop.append("eh_novo")
        # Adicione aqui outras colunas auxiliares
        # if "alguma_coluna_temp" in df_send.columns: cols_to_drop.append("alguma_coluna_temp")

        if cols_to_drop:
            df_send = df_send.drop(columns=cols_to_drop)
            logging.info(f"Colunas auxiliares removidas de df_send: {cols_to_drop}. Novo shape: {df_send.shape}")

        # Garante que o df_send tem as colunas corretas e na ordem certa
        df_send_final = df_send.reindex(columns=cols_alvo_tratada, fill_value="")
        logging.info(f"df_send reindexado para alinhar com colunas alvo. Shape final: {df_send_final.shape}")

        # QA: Verifica se alguma coluna do df_send_final cont√©m valores inesperados antes do envio
        for col_qa in ['orgaos', 'responsavel', 'status_demanda', 'data_da_conclusao']:
            if col_qa in df_send_final.columns:
                unexpected_values = df_send_final[col_qa].astype(str).str.contains(r'(?i)^(sim|nao|true|false|\?{2,}|nan|none)$')
                if unexpected_values.any():
                    logging.error(f"QA Pr√©-Envio (df_send): Coluna '{col_qa}' cont√©m valores inesperados em {unexpected_values.sum()} linhas. Exemplos: {df_send_final.loc[unexpected_values, col_qa].unique()[:5].tolist()}",
                                  extra={'data': df_send_final.loc[unexpected_values, ['protocolo', col_qa]].to_dict(orient='records')[:5]})
                    # Considerar um raise SystemExit aqui se a qualidade do dado for cr√≠tica

        # Garante que todas as colunas sejam strings para evitar problemas de tipo no GSpread
        for col in df_send_final.columns:
            df_send_final[col] = df_send_final[col].astype(str)
        logging.info("Todas as colunas de df_send_final convertidas para string.")

        df_send = df_send_final.copy() # Atribui o DataFrame final preparado para df_send

except Exception as e:
    logging.critical(f"Erro na prepara√ß√£o final de df_send no Item 8: {e}", exc_info=True)
    raise

# ----------------------------------------------------------
# TRATAMENTO CR√çTICO ‚Äî DATA DA CONCLUS√ÉO (AP√ìS _tratar_full)
# e PADRONIZA OUTRAS DATAS
#
# Com a aplica√ß√£o de _tratar_full acima, estas fun√ß√µes devem ser menos necess√°rias.
# Elas s√£o mantidas como um √∫ltimo ajuste de formato para DD/MM/YYYY se _tratar_full
# retornar DD/MM/YY e o GSheet esperar o ano com 4 d√≠gitos.
# ----------------------------------------------------------
def tratar_data_conclusao_item8(x):
    if pd.isna(x) or str(x).strip().lower() in ["", "nan", "na", "n/a", "none", "n√£o conclu√≠do"]:
        return "N√£o conclu√≠do"
    try:
        dt = pd.to_datetime(x, errors="coerce", dayfirst=True)
        if pd.notna(dt):
            return dt.strftime("%d/%m/%Y")
        else:
            return "N√£o conclu√≠do"
    except Exception:
        return "N√£o conclu√≠do"

if not df_send.empty and "data_da_conclusao" in df_send.columns:
    df_send["data_da_conclusao"] = df_send["data_da_conclusao"].apply(tratar_data_conclusao_item8)
    logging.debug("Re-aplicado tratamento de 'data_da_conclusao' para garantir formato DD/MM/YYYY.")

def tratar_data_generica_item8_final(x):
    """
    Tratamento final robusto para garantir o formato DD/MM/AAAA para o Sheets,
    lidando com valores que j√° s√£o strings ou formatos de 2 d√≠gitos.
    """
    s = str(x).strip()
    if s.lower() in ["", "nan", "na", "n/a", "none"]:
        return ""

    # Verifica se j√° est√° no formato DD/MM/AAAA. Se sim, mant√©m como string.
    if re.fullmatch(r"\d{2}/\d{2}/\d{4}", s):
        return s
    
    # 1. Tenta o parsing flex√≠vel (mais comum)
    try:
        dt = pd.to_datetime(s, errors="coerce", dayfirst=True)
        if pd.notna(dt):
            return dt.strftime("%d/%m/%Y")
        
        # 2. Se falhou, tenta explicitamente formatos de 2 d√≠gitos que podem ter escapado (DD/MM/AA)
        if re.fullmatch(r"\d{2}/\d{2}/\d{2}", s):
            dt_y = pd.to_datetime(s, format="%d/%m/%y", errors="coerce")
            if pd.notna(dt_y):
                return dt_y.strftime("%d/%m/%Y")
        
        # 3. Tenta formatos ISO (YYYY-MM-DD)
        if re.match(r"^\d{4}-\d{2}-\d{2}", s):
            dt_iso = pd.to_datetime(s, errors="coerce", dayfirst=False)
            if pd.notna(dt_iso):
                return dt_iso.strftime("%d/%m/%Y")
            
        return "" # Se tudo falhou
            
    except Exception:
        return "" # Em caso de erro de parsing

for col in ["data_da_criacao"]:
    if not df_send.empty and col in df_send.columns:
        # Aplica a fun√ß√£o de tratamento final (com ano de 4 d√≠gitos)
        df_send[col] = df_send[col].apply(tratar_data_generica_item8_final)
        
        # GARANTE que a coluna FINAL √© string (DD/MM/AAAA) para envio sem erro de tipo.
        df_send[col] = df_send[col].astype(str)
        logging.debug(f"Re-aplicado tratamento FINAL de '{col}' para garantir formato DD/MM/AAAA (String).")


# ----------------------------------------------------------
# CHECAGEM DE SANIDADE ‚Äî UNIDADE_CADASTRO (em df_send j√° tratado)
# ----------------------------------------------------------
if not df_send.empty and "unidade_cadastro" in df_send.columns:
    nulos_uc = int(df_send["unidade_cadastro"].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']).sum())
    print(f"üß™ Checagem (NOVOS - PRONTOS PARA ENVIO): unidade_cadastro presente | vazios={nulos_uc}")
    logging.info(f"Checagem (NOVOS - PRONTOS PARA ENVIO): unidade_cadastro presente | vazios={nulos_uc}")
    if nulos_uc > 0:
        logging.warning(f"QA Pr√©-Envio: 'unidade_cadastro' cont√©m {nulos_uc} valores vazios/inv√°lidos em df_send. Exemplos: {df_send.loc[df_send['unidade_cadastro'].astype(str).str.strip().isin(['', 'nan', 'none', 'n/a', 'n√£o informado']), 'unidade_cadastro'].unique()[:5].tolist()}")
else:
    print("‚ö†Ô∏è Aviso: unidade_cadastro n√£o est√° em df_send ou df_send est√° vazio.")
    logging.warning("unidade_cadastro n√£o est√° em df_send ou df_send est√° vazio. Verifique a consist√™ncia do schema.")

# ----------------------------------------------------------
# ENVIO EM LOTES
# ----------------------------------------------------------
if df_send.empty:
    logging.info("Nenhum protocolo para enviar, pulando envio em lotes.")
    print("üì¶ Nenhum protocolo para enviar.")
else:
    lote = 500
    total_lotes = (len(df_send) + lote - 1) // lote
    print(f"üì¶ Envio ‚Äî APENAS NOVOS (FINAL): {len(df_send)} linhas | {total_lotes} lotes")
    logging.info(f"Envio ‚Äî APENAS NOVOS (FINAL): {len(df_send)} linhas | {total_lotes} lotes")

    existing_values = aba_tratada.get_all_values()
    sheet_is_empty = len(existing_values) == 0

    for i in range(0, len(df_send), lote):
        chunk = df_send.iloc[i:i+lote].copy()
        rows = chunk.values.tolist()
        first_idx = i + 1
        last_idx = min(i + lote, len(df_send))
        protos_preview = list(chunk.get("protocolo", []))[:3]
        logging.debug(f"Processando lote {first_idx}-{last_idx}. Pr√©via protocolos: {protos_preview}")
        print(f"   ‚Ä¢ Enviando {first_idx}-{last_idx} (pr√©via protocolos: {protos_preview})")

        try:
            if sheet_is_empty:
                header = chunk.columns.tolist()
                aba_tratada.append_rows([header] + rows)
                logging.info(f"Lote {first_idx}-{last_idx} enviado com cabe√ßalho.")
                sheet_is_empty = False
            else:
                aba_tratada.append_rows(rows)
                logging.info(f"Lote {first_idx}-{last_idx} enviado (sem cabe√ßalho).")
        except Exception as e:
            logging.exception(f"Erro CR√çTICO ao enviar lote {first_idx}-{last_idx}: {e}")
            print(f"‚ùå Erro ao enviar lote {first_idx}-{last_idx}: {e}")
            failed = chunk[["protocolo"]].copy()
            timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            failed.to_csv(f"failed_append_{first_idx}_{last_idx}_{timestamp}.csv", index=False, encoding="utf-8-sig")
            # Dependendo da severidade, voc√™ pode querer parar o pipeline aqui.

print("‚úÖ Atualiza√ß√£o da planilha tratada conclu√≠da com sucesso.")
logging.info("‚úÖ Atualiza√ß√£o da planilha tratada conclu√≠da com sucesso.")

# ========================================================
# 9) PATCH / ATUALIZA√á√ÉO DE STATUS E DELTA HIST√ìRICO (CORRIGIDO)
# ========================================================
_BANNER("9) PATCH / ATUALIZA√á√ÉO DE STATUS E DELTA HIST√ìRICO (CORRIGIDO)")

import time

# --------------------------------------------------------
# üîß Helper principal de padroniza√ß√£o de status e datas
# --------------------------------------------------------
def _prepare_status(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    # 1Ô∏è‚É£ Padroniza status_demanda
    if "status_demanda" in df.columns:
        df["status_demanda"] = df["status_demanda"].apply(
            lambda v: "CONCLU√çDA" if _is_concluida(v)
            else "EM ANDAMENTO" if (v and str(v).strip() != "")
            else v
        )

    # 2Ô∏è‚É£ Padroniza prazo_restante
    if "prazo_restante" in df.columns:
        df["prazo_restante"] = df["prazo_restante"].apply(_canon_prazo_restante)

    # 3Ô∏è‚É£ Padroniza data_da_conclusao ‚Äî tratamento definitivo
    if "data_da_conclusao" in df.columns:
        def _tratar_data_conclusao(x):
            """Converte datas v√°lidas e substitui inv√°lidas/vazias por 'N√£o conclu√≠do'"""
            if pd.isna(x) or str(x).strip().lower() in ["", "nan", "na", "n/a", "none"]:
                return "N√£o conclu√≠do"
            try:
                dt = pd.to_datetime(x, errors="coerce")
                if pd.isna(dt):
                    return "N√£o conclu√≠do"
                return dt.strftime("%d/%m/%Y")
            except Exception:
                return "N√£o conclu√≠do"

        df["data_da_conclusao"] = df["data_da_conclusao"].apply(_tratar_data_conclusao)

    # 4Ô∏è‚É£ Limpeza de "N√£o h√° dados"
    for col in ["tempo_de_resolucao_em_dias"]:
        if col in df.columns:
            df[col] = df[col].replace("N√£o h√° dados", "")

    return df


# --------------------------------------------------------
# Fallback de envio de lotes (log)
# --------------------------------------------------------
def _post_lotes(df: pd.DataFrame, msg: str, cols: list):
    logging.warning(f"Fallback acionado ({msg}) para {len(df)} linhas. Colunas: {cols}")

# Antes de criar/usar os deltas, aplique o prepare em todo df (ou pelo menos nos deltas)
df = _prepare_status(df)  # garante que coluna principal esteja padronizada

# Em seguida crie os deltas com base no df j√° padronizado:
delta_status = df[df["status_demanda"] != df.get("status_demanda_OLD", df["status_demanda"])]
delta_conc   = df[df["data_da_conclusao"] != df.get("data_da_conclusao_OLD", df["data_da_conclusao"])]
delta_tempo  = df[df["tempo_de_resolucao_em_dias"] != df.get("tempo_de_resolucao_em_dias_OLD", df["tempo_de_resolucao_em_dias"])]

# --------------------------------------------------------
# PATCH principal ‚Äî atualiza√ß√£o de c√©lulas no Google Sheets
# --------------------------------------------------------
def _patch_grouped_force(df: pd.DataFrame, key_col: str, value_col: str, sheet=None):
    if df.empty:
        logging.info("Delta vazio. Nada a atualizar.")
        return

    batch_size = 50

    # Aplica padroniza√ß√µes completas
    df = _prepare_status(df)
    df[key_col] = df[key_col].astype(str).str.strip()
    df[value_col] = df[value_col].astype(str).str.strip()

    # üîí Corrige 'data_da_conclusao' p√≥s-stringifica√ß√£o
    if value_col == "data_da_conclusao":
        df[value_col] = df[value_col].apply(
            lambda x: "N√£o conclu√≠do" if str(x).strip().lower() in ["", "nan", "na", "n/a", "none", "nat"] else x
        )

    if sheet is None:
        _post_lotes(df, f"{value_col} (fallback)", [key_col, value_col])
        return

    # Tenta localizar colunas no Sheets
    try:
        key_list = sheet.col_values(1)
    except Exception as e:
        logging.error(f"Erro ao obter protocolos da planilha: {e}")
        _post_lotes(df, f"{value_col} (fallback)", [key_col, value_col])
        return

    try:
        col_idx = sheet.find(value_col).col
    except Exception:
        logging.warning(f"Coluna '{value_col}' n√£o encontrada. Usando coluna 2 como fallback.")
        col_idx = 2

    # --- PREPARA LISTA DE ATUALIZA√á√ÉO ---
    to_update = []
    for _, row in df.iterrows():
        key = row[key_col]
        value = row.get(f"{value_col}_trat", row[value_col])
        if key in key_list:
            row_idx = key_list.index(key) + 1
            to_update.append((row_idx, col_idx, value))
        else:
            logging.warning(f"Protocolo '{key}' n√£o encontrado na planilha.")

    # --- ATUALIZA√á√ÉO EM BATCH ---
    total_updated = 0
    for i in range(0, len(to_update), batch_size):
        batch = to_update[i:i + batch_size]
        if not batch:
            continue

        cleaned_batch = []
        for r in batch:
            value = r[2]
            if pd.isna(value) or str(value).strip() == "":
                value = "N√£o conclu√≠do" if value_col == "data_da_conclusao" else ""
            value = str(value).strip()
            cleaned_batch.append((r[0], r[1], value))

        range_rows = [r[0] for r in cleaned_batch]
        values = [[r[2]] for r in cleaned_batch]

        try:
            import gspread.utils
            start_row = min(range_rows)
            end_row = max(range_rows)
            range_str = (
                f"{gspread.utils.rowcol_to_a1(start_row, col_idx)}:"
                f"{gspread.utils.rowcol_to_a1(end_row, col_idx)}"
            )
            sheet.update(range_str, values)
            total_updated += len(cleaned_batch)
        except Exception as e:
            logging.error(f"Erro no batch update: {e}")
            _post_lotes(
                pd.DataFrame([(r[0], r[2]) for r in cleaned_batch], columns=[key_col, value_col]),
                f"{value_col} (fallback)",
                [key_col, value_col],
            )

    logging.info(f"‚úÖ Status atualizado em batch: {total_updated}/{len(df)} linhas.")
    if total_updated == 0:
        logging.info("Nenhuma linha atualizada diretamente. Enviando via _post_lotes como fallback.")
        _post_lotes(df, f"{value_col} (fallback)", [key_col, value_col])


# ========================================================
# 9.1) Delta TEMPO_RESOLUCAO local (0 ‚Üí 1)
# ========================================================
if "tempo_de_resolucao_em_dias" in df.columns:
    s_local = df["tempo_de_resolucao_em_dias"].astype("object")

    def _fix_zero_keep_text(v):
        if pd.isna(v):
            return v
        sv = str(v).strip()
        try:
            num = pd.to_numeric(sv, errors="coerce")
            if pd.notna(num) and float(num) == 0.0:
                return "1"
        except Exception:
            pass
        return sv

    df["tempo_de_resolucao_em_dias"] = s_local.map(_fix_zero_keep_text)

# ========================================================
# 10) DELTAS HIST√ìRICOS (status_demanda, data_da_conclusao, tempo_de_resolucao_em_dias)
# ========================================================

_BANNER("10) DELTAS HIST√ìRICOS (ajustado para novos protocolos)")

# --- Alinha colunas do df_send ao schema da tratada ---
try:
    cols_tratada = list(df_tratada.columns)
    # df_send pode n√£o existir (caso nenhum novo); garante vari√°vel
    if 'df_send' not in globals():
        df_send = pd.DataFrame(columns=cols_tratada)
    df_send_aligned = df_send.reindex(columns=cols_tratada, fill_value="")
    df_full = pd.concat([df_tratada, df_send_aligned], ignore_index=True, sort=False)
except Exception as e:
    logging.warning(f"Falha ao concatenar bases tratada + novos: {e}")
    # fallback simples: tenta usar df_send como fonte
    df_full = df_send.copy()

# --- Garante exist√™ncia das colunas *_OLD para compara√ß√µes de hist√≥rico ---
for col in ["status_demanda", "data_da_conclusao", "tempo_de_resolucao_em_dias"]:
    old_col = f"{col}_OLD"
    if old_col not in df_full.columns:
        # copia o valor atual para coluna OLD (se n√£o existir), normalizando nulos
        df_full[old_col] = df_full.get(col, "").fillna("")

# --- Fun√ß√£o delta robusta (com fillna e casting a str) ---
def _delta_df(df_full_local: pd.DataFrame, col: str) -> pd.DataFrame:
    old_col = f"{col}_OLD"
    if old_col in df_full_local.columns:
        left = df_full_local.get(col, "").fillna("").astype(str)
        right = df_full_local.get(old_col, "").fillna("").astype(str)
        return df_full_local[left != right].copy()
    else:
        # se n√£o h√° coluna OLD, considera apenas os rec√©m marcados como 'eh_novo'
        return df_full_local[df_full_local.get("eh_novo", False) == True].copy()

# --- Calcula deltas espec√≠ficos (apenas uma vez e sem sobrescritas) ---
delta_status = _delta_df(df_full, "status_demanda")
delta_conc   = _delta_df(df_full, "data_da_conclusao")
delta_tempo  = _delta_df(df_full, "tempo_de_resolucao_em_dias")

# --- Logs e verifica√ß√µes ---
logging.info(f"Delta STATUS: {len(delta_status)} linhas alteradas/novas")
logging.info(f"Delta DATA_CONCLUSAO: {len(delta_conc)} linhas alteradas/novas")
logging.info(f"Delta TEMPO_DE_RESOLUCAO: {len(delta_tempo)} linhas alteradas/novas")

print(f"üìä Deltas calculados com sucesso:")
print(f"   ‚Ä¢ STATUS: {len(delta_status)}")
print(f"   ‚Ä¢ DATA_CONCLUSAO: {len(delta_conc)}")
print(f"   ‚Ä¢ TEMPO_DE_RESOLUCAO: {len(delta_tempo)}")

# ========================================================
# 11) QA & SUM√ÅRIO FINAL
# ========================================================
_BANNER("11) QA & SUM√ÅRIO FINAL")

# --- Protocolos novos / existentes ---
if "eh_novo" in df.columns:
    novos_protos = df.loc[df["eh_novo"] == True, "protocolo"].astype(str).str.strip()
    print(f"üîπ Protocolos novos: {len(novos_protos)}")
    logging.info(f"Protocolos novos: {len(novos_protos)}")
else:
    novos_protos = pd.Series([], dtype=str)
    logging.info("Coluna 'eh_novo' ausente ‚Äî nenhum protocolo novo identificado.")

# --- QA de colunas cr√≠ticas ---
qa_cols = ["status_demanda", "data_da_conclusao", "tempo_de_resolucao_em_dias"]
for col in qa_cols:
    if col in df.columns:
        nulos = df[col].isna().sum()
        print(f"üîπ {col} vazio: {nulos} linhas")
        logging.info(f"QA: {col} vazio: {nulos} linhas")
    else:
        logging.info(f"Coluna '{col}' ausente.")

# --- Sum√°rio final ---
print("‚úÖ QA conclu√≠do. Sum√°rio final:")
logging.info("QA conclu√≠do. Sum√°rio final:")
print(f"  ‚Ä¢ Protocolos totais na planilha: {len(df)}")
print(f"  ‚Ä¢ Protocolos novos identificados: {len(novos_protos)}")
for col in qa_cols:
    nulos = df[col].isna().sum() if col in df.columns else 0
    print(f"  ‚Ä¢ {col} vazio: {nulos}")

# ========================================================
# 12) FINALIZA√á√ÉO
# ========================================================

# --- Sanity checks finais (colocar antes do _BANNER("12) PIPELINE FINALIZADO")) ---
try:
    bruta_rows = len(df_bruta) if 'df_bruta' in globals() and isinstance(df_bruta, pd.DataFrame) else 0
    bruta_cols = list(df_bruta.columns)[:20] if 'df_bruta' in globals() and isinstance(df_bruta, pd.DataFrame) and not df_bruta.empty else []
except Exception:
    bruta_rows, bruta_cols = 0, []

try:
    tratada_rows = len(df_tratada) if 'df_tratada' in globals() and isinstance(df_tratada, pd.DataFrame) else 0
    tratada_cols = list(df_tratada.columns)[:20] if 'df_tratada' in globals() and isinstance(df_tratada, pd.DataFrame) and not df_tratada.empty else []
except Exception:
    tratada_rows, tratada_cols = 0, []

novos_cnt = int(df_bruta['eh_novo'].sum()) if 'df_bruta' in globals() and 'eh_novo' in df_bruta.columns else 0
df_send_cnt = len(df_send) if 'df_send' in globals() and isinstance(df_send, pd.DataFrame) else 0

logging.info(f"Sanity: df_bruta rows={bruta_rows} cols={bruta_cols}")
logging.info(f"Sanity: df_tratada rows={tratada_rows} cols={tratada_cols}")
logging.info(f"Sanity: novos detectados={novos_cnt} | df_send (preparados para envio)={df_send_cnt}")
print(f"Sanity checks ‚Äî bruta:{bruta_rows} rows, tratada:{tratada_rows} rows, novos:{novos_cnt}, to_send:{df_send_cnt}")

# opcional: numero real de linhas na sheet (pode ser custoso em tempo)
try:
    if 'aba_tratada' in globals():
        total_sheet_rows = len(aba_tratada.get_all_values())
        logging.info(f"Sanity: aba_tratada (sheet) rows={total_sheet_rows}")
except Exception as e:
    logging.warning(f"N√£o foi poss√≠vel obter aba_tratada.get_all_values(): {e}")

_BANNER("12) PIPELINE FINALIZADO")

print("üéØ Pipeline executado com sucesso!")
logging.info("Pipeline executado com sucesso")

print("Fluxo: GoogleSheets (bruto) ‚Üí Python (tratamento) ‚Üí GoogleSheets (tratado) ‚Üí LookerStudio")

# --- Resumo de atualiza√ß√µes completas ---
for col in qa_cols:
    if col in df.columns:
        atualizadas = len(df[df[col].notna()])
        print(f"  ‚Ä¢ {col} atualizadas: {atualizadas} linhas")
        logging.info(f"{col} atualizadas: {atualizadas} linhas")
